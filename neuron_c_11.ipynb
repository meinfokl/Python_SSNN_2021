{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.7760975360870361, Accuracy: 22.155555725097656, Test Accuracy: 31.663326263427734\n",
      "Epoch 2, Loss: 1.7030636072158813, Accuracy: 25.555557250976562, Test Accuracy: 22.04408836364746\n",
      "Epoch 3, Loss: 1.6316509246826172, Accuracy: 24.600000381469727, Test Accuracy: 25.450902938842773\n",
      "Epoch 4, Loss: 1.5824707746505737, Accuracy: 29.044443130493164, Test Accuracy: 32.66532897949219\n",
      "Epoch 5, Loss: 1.567099928855896, Accuracy: 31.422222137451172, Test Accuracy: 35.27054214477539\n",
      "Epoch 6, Loss: 1.5060139894485474, Accuracy: 38.31111145019531, Test Accuracy: 39.47895812988281\n",
      "Epoch 7, Loss: 1.451520562171936, Accuracy: 44.155555725097656, Test Accuracy: 46.69338607788086\n",
      "Epoch 8, Loss: 1.399530053138733, Accuracy: 48.42222213745117, Test Accuracy: 49.89979934692383\n",
      "Epoch 9, Loss: 1.3389431238174438, Accuracy: 52.666664123535156, Test Accuracy: 55.31061935424805\n",
      "Epoch 10, Loss: 1.2385964393615723, Accuracy: 61.04444122314453, Test Accuracy: 69.7394790649414\n",
      "Epoch 11, Loss: 1.1240899562835693, Accuracy: 71.15555572509766, Test Accuracy: 79.759521484375\n",
      "Epoch 12, Loss: 1.0393236875534058, Accuracy: 77.66666412353516, Test Accuracy: 81.96392822265625\n",
      "Epoch 13, Loss: 0.9774050712585449, Accuracy: 79.66667175292969, Test Accuracy: 82.1643295288086\n",
      "Epoch 14, Loss: 0.926770806312561, Accuracy: 80.66666412353516, Test Accuracy: 82.1643295288086\n",
      "Epoch 15, Loss: 0.9125248193740845, Accuracy: 77.4000015258789, Test Accuracy: 82.96593475341797\n",
      "Epoch 16, Loss: 0.8593587279319763, Accuracy: 79.88888549804688, Test Accuracy: 83.36673736572266\n",
      "Epoch 17, Loss: 0.8181995153427124, Accuracy: 81.5999984741211, Test Accuracy: 83.96793365478516\n",
      "Epoch 18, Loss: 0.7784191966056824, Accuracy: 82.9111099243164, Test Accuracy: 84.56913757324219\n",
      "Epoch 19, Loss: 0.7387898564338684, Accuracy: 83.9111099243164, Test Accuracy: 86.77354431152344\n",
      "Epoch 20, Loss: 0.7052604556083679, Accuracy: 85.0888900756836, Test Accuracy: 87.77555084228516\n",
      "Epoch 21, Loss: 0.6749271154403687, Accuracy: 86.0, Test Accuracy: 88.57715606689453\n",
      "Epoch 22, Loss: 0.6473038792610168, Accuracy: 86.73333740234375, Test Accuracy: 89.17835998535156\n",
      "Epoch 23, Loss: 0.6219884753227234, Accuracy: 87.68888854980469, Test Accuracy: 89.37875366210938\n",
      "Epoch 24, Loss: 0.5986257195472717, Accuracy: 88.82221984863281, Test Accuracy: 89.7795639038086\n",
      "Epoch 25, Loss: 0.5769395232200623, Accuracy: 90.15555572509766, Test Accuracy: 90.78156280517578\n",
      "Epoch 26, Loss: 0.5566950440406799, Accuracy: 91.35555267333984, Test Accuracy: 92.58516693115234\n",
      "Epoch 27, Loss: 0.537680447101593, Accuracy: 92.26667022705078, Test Accuracy: 93.98797607421875\n",
      "Epoch 28, Loss: 0.5197148323059082, Accuracy: 92.68888854980469, Test Accuracy: 94.98998260498047\n",
      "Epoch 29, Loss: 0.50264573097229, Accuracy: 93.15555572509766, Test Accuracy: 95.59117889404297\n",
      "Epoch 30, Loss: 0.48633649945259094, Accuracy: 93.4000015258789, Test Accuracy: 95.59117889404297\n",
      "Epoch 31, Loss: 0.4706490635871887, Accuracy: 93.71111297607422, Test Accuracy: 95.59117889404297\n",
      "Epoch 32, Loss: 0.45543578267097473, Accuracy: 93.9111099243164, Test Accuracy: 95.99198150634766\n",
      "Epoch 33, Loss: 0.4401857852935791, Accuracy: 94.1111068725586, Test Accuracy: 95.99198150634766\n",
      "Epoch 34, Loss: 0.4229182004928589, Accuracy: 94.35555267333984, Test Accuracy: 95.59117889404297\n",
      "Epoch 35, Loss: 0.4071677625179291, Accuracy: 94.57777404785156, Test Accuracy: 95.79158782958984\n",
      "Epoch 36, Loss: 0.391880065202713, Accuracy: 94.73333740234375, Test Accuracy: 95.79158782958984\n",
      "Epoch 37, Loss: 0.3779759109020233, Accuracy: 94.86666870117188, Test Accuracy: 95.99198150634766\n",
      "Epoch 38, Loss: 0.36498427391052246, Accuracy: 94.95555877685547, Test Accuracy: 95.99198150634766\n",
      "Epoch 39, Loss: 0.35277512669563293, Accuracy: 95.04444885253906, Test Accuracy: 95.99198150634766\n",
      "Epoch 40, Loss: 0.3412490785121918, Accuracy: 95.0888900756836, Test Accuracy: 96.1923828125\n",
      "Epoch 41, Loss: 0.3303290009498596, Accuracy: 95.13333129882812, Test Accuracy: 96.39278411865234\n",
      "Epoch 42, Loss: 0.3199522793292999, Accuracy: 95.24444580078125, Test Accuracy: 96.99398803710938\n",
      "Epoch 43, Loss: 0.31006547808647156, Accuracy: 95.37777709960938, Test Accuracy: 97.5951919555664\n",
      "Epoch 44, Loss: 0.3006226718425751, Accuracy: 95.66666412353516, Test Accuracy: 97.5951919555664\n",
      "Epoch 45, Loss: 0.2915812134742737, Accuracy: 95.80000305175781, Test Accuracy: 97.79559326171875\n",
      "Epoch 46, Loss: 0.2828984558582306, Accuracy: 96.08888244628906, Test Accuracy: 97.79559326171875\n",
      "Epoch 47, Loss: 0.2745267450809479, Accuracy: 96.20000457763672, Test Accuracy: 97.79559326171875\n",
      "Epoch 48, Loss: 0.2664066255092621, Accuracy: 96.4000015258789, Test Accuracy: 97.79559326171875\n",
      "Epoch 49, Loss: 0.258466899394989, Accuracy: 96.64443969726562, Test Accuracy: 97.79559326171875\n",
      "Epoch 50, Loss: 0.2506627142429352, Accuracy: 96.82221984863281, Test Accuracy: 97.79559326171875\n",
      "Epoch 51, Loss: 0.24302367866039276, Accuracy: 96.95555877685547, Test Accuracy: 97.79559326171875\n",
      "Epoch 52, Loss: 0.23559607565402985, Accuracy: 97.19999694824219, Test Accuracy: 97.79559326171875\n",
      "Epoch 53, Loss: 0.22839009761810303, Accuracy: 97.31111145019531, Test Accuracy: 97.79559326171875\n",
      "Epoch 54, Loss: 0.2214048057794571, Accuracy: 97.39999389648438, Test Accuracy: 97.99598693847656\n",
      "Epoch 55, Loss: 0.21463848650455475, Accuracy: 97.48888397216797, Test Accuracy: 97.99598693847656\n",
      "Epoch 56, Loss: 0.2080887258052826, Accuracy: 97.5999984741211, Test Accuracy: 97.99598693847656\n",
      "Epoch 57, Loss: 0.20175135135650635, Accuracy: 97.97777557373047, Test Accuracy: 97.99598693847656\n",
      "Epoch 58, Loss: 0.1956215500831604, Accuracy: 98.4000015258789, Test Accuracy: 97.99598693847656\n",
      "Epoch 59, Loss: 0.18969371914863586, Accuracy: 98.977783203125, Test Accuracy: 98.5971908569336\n",
      "Epoch 60, Loss: 0.18396250903606415, Accuracy: 99.53333282470703, Test Accuracy: 99.19839477539062\n",
      "Epoch 61, Loss: 0.17842258512973785, Accuracy: 99.68888854980469, Test Accuracy: 99.59919738769531\n",
      "Epoch 62, Loss: 0.1730690896511078, Accuracy: 99.75555419921875, Test Accuracy: 99.59919738769531\n",
      "Epoch 63, Loss: 0.16789695620536804, Accuracy: 99.80000305175781, Test Accuracy: 99.59919738769531\n",
      "Epoch 64, Loss: 0.16290125250816345, Accuracy: 99.82221984863281, Test Accuracy: 99.59919738769531\n",
      "Epoch 65, Loss: 0.15807649493217468, Accuracy: 99.86666107177734, Test Accuracy: 99.59919738769531\n",
      "Epoch 66, Loss: 0.15341678261756897, Accuracy: 99.86666107177734, Test Accuracy: 99.59919738769531\n",
      "Epoch 67, Loss: 0.14891590178012848, Accuracy: 99.8888931274414, Test Accuracy: 99.59919738769531\n",
      "Epoch 68, Loss: 0.144567608833313, Accuracy: 99.9111099243164, Test Accuracy: 99.59919738769531\n",
      "Epoch 69, Loss: 0.14036552608013153, Accuracy: 99.9111099243164, Test Accuracy: 99.59919738769531\n",
      "Epoch 70, Loss: 0.13630357384681702, Accuracy: 99.9111099243164, Test Accuracy: 99.59919738769531\n",
      "Epoch 71, Loss: 0.1323757767677307, Accuracy: 99.9111099243164, Test Accuracy: 99.59919738769531\n",
      "Epoch 72, Loss: 0.12857644259929657, Accuracy: 99.9111099243164, Test Accuracy: 99.59919738769531\n",
      "Epoch 73, Loss: 0.12490025162696838, Accuracy: 99.9111099243164, Test Accuracy: 99.59919738769531\n",
      "Epoch 74, Loss: 0.12134210765361786, Accuracy: 99.93333435058594, Test Accuracy: 99.59919738769531\n",
      "Epoch 75, Loss: 0.11789724975824356, Accuracy: 99.93333435058594, Test Accuracy: 99.59919738769531\n",
      "Epoch 76, Loss: 0.11456098407506943, Accuracy: 99.93333435058594, Test Accuracy: 99.59919738769531\n",
      "Epoch 77, Loss: 0.11132916808128357, Accuracy: 99.93333435058594, Test Accuracy: 99.59919738769531\n",
      "Epoch 78, Loss: 0.1081976667046547, Accuracy: 99.93333435058594, Test Accuracy: 99.59919738769531\n",
      "Epoch 79, Loss: 0.105162613093853, Accuracy: 99.95555114746094, Test Accuracy: 99.59919738769531\n",
      "Epoch 80, Loss: 0.10165636986494064, Accuracy: 99.95555114746094, Test Accuracy: 99.59919738769531\n",
      "Epoch 81, Loss: 0.09716717153787613, Accuracy: 99.95555114746094, Test Accuracy: 99.59919738769531\n",
      "Epoch 82, Loss: 0.09356419742107391, Accuracy: 99.95555114746094, Test Accuracy: 99.59919738769531\n",
      "Epoch 83, Loss: 0.09031697362661362, Accuracy: 99.977783203125, Test Accuracy: 99.59919738769531\n",
      "Epoch 84, Loss: 0.08730283379554749, Accuracy: 99.977783203125, Test Accuracy: 99.59919738769531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85, Loss: 0.08448157459497452, Accuracy: 99.977783203125, Test Accuracy: 99.59919738769531\n",
      "Epoch 86, Loss: 0.08182390034198761, Accuracy: 99.977783203125, Test Accuracy: 99.59919738769531\n",
      "Epoch 87, Loss: 0.07930085062980652, Accuracy: 99.977783203125, Test Accuracy: 99.59919738769531\n",
      "Epoch 88, Loss: 0.0768909752368927, Accuracy: 99.977783203125, Test Accuracy: 99.59919738769531\n",
      "Epoch 89, Loss: 0.0745808407664299, Accuracy: 99.977783203125, Test Accuracy: 99.59919738769531\n",
      "Epoch 90, Loss: 0.07236150652170181, Accuracy: 99.977783203125, Test Accuracy: 99.59919738769531\n",
      "Epoch 91, Loss: 0.07022623717784882, Accuracy: 99.977783203125, Test Accuracy: 99.79959869384766\n",
      "Epoch 92, Loss: 0.06816928833723068, Accuracy: 99.977783203125, Test Accuracy: 99.79959869384766\n",
      "Epoch 93, Loss: 0.06618568301200867, Accuracy: 99.977783203125, Test Accuracy: 99.79959869384766\n",
      "Epoch 94, Loss: 0.06427090615034103, Accuracy: 99.977783203125, Test Accuracy: 99.79959869384766\n",
      "Epoch 95, Loss: 0.06242108717560768, Accuracy: 99.977783203125, Test Accuracy: 99.79959869384766\n",
      "Epoch 96, Loss: 0.06063276529312134, Accuracy: 99.977783203125, Test Accuracy: 99.79959869384766\n",
      "Epoch 97, Loss: 0.05890294164419174, Accuracy: 99.977783203125, Test Accuracy: 99.79959869384766\n",
      "Epoch 98, Loss: 0.05722905695438385, Accuracy: 99.977783203125, Test Accuracy: 99.79959869384766\n",
      "Epoch 99, Loss: 0.055608730763196945, Accuracy: 99.977783203125, Test Accuracy: 99.79959869384766\n",
      "Epoch 100, Loss: 0.05403989553451538, Accuracy: 99.977783203125, Test Accuracy: 99.79959869384766\n",
      "Epoch 101, Loss: 0.05252053588628769, Accuracy: 99.977783203125, Test Accuracy: 99.79959869384766\n",
      "Epoch 102, Loss: 0.0510488897562027, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 103, Loss: 0.049623265862464905, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 104, Loss: 0.048242054879665375, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 105, Loss: 0.046903692185878754, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 106, Loss: 0.04560678452253342, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 107, Loss: 0.04434989020228386, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 108, Loss: 0.04313170537352562, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 109, Loss: 0.04195091500878334, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 110, Loss: 0.04080629348754883, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 111, Loss: 0.039696648716926575, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 112, Loss: 0.03862084075808525, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 113, Loss: 0.037577759474515915, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 114, Loss: 0.0365663506090641, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 115, Loss: 0.03558557480573654, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 116, Loss: 0.034634437412023544, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 117, Loss: 0.03371195122599602, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 118, Loss: 0.032817211002111435, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 119, Loss: 0.031949326395988464, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 120, Loss: 0.031107405200600624, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 121, Loss: 0.030290624126791954, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 122, Loss: 0.029498174786567688, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 123, Loss: 0.0287292692810297, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 124, Loss: 0.02798313833773136, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 125, Loss: 0.027259068563580513, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 126, Loss: 0.026556361466646194, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 127, Loss: 0.025874294340610504, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 128, Loss: 0.025212232023477554, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 129, Loss: 0.024569537490606308, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 130, Loss: 0.02394557185471058, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 131, Loss: 0.023339765146374702, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 132, Loss: 0.022751525044441223, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 133, Loss: 0.02218029834330082, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 134, Loss: 0.02162555232644081, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 135, Loss: 0.021086741238832474, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 136, Loss: 0.020563384518027306, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 137, Loss: 0.02005499228835106, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 138, Loss: 0.01956108771264553, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 139, Loss: 0.01908121630549431, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 140, Loss: 0.018614942207932472, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 141, Loss: 0.0181618370115757, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 142, Loss: 0.01772148348391056, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 143, Loss: 0.01729349046945572, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 144, Loss: 0.01687745377421379, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 145, Loss: 0.016473038122057915, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 146, Loss: 0.01607985608279705, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 147, Loss: 0.015697570517659187, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 148, Loss: 0.015325836837291718, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 149, Loss: 0.01496433187276125, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 150, Loss: 0.014612745493650436, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 151, Loss: 0.014270767569541931, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 152, Loss: 0.013938109390437603, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 153, Loss: 0.013614475727081299, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 154, Loss: 0.013299600221216679, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 155, Loss: 0.012993218377232552, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 156, Loss: 0.01269506011158228, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 157, Loss: 0.012404890730977058, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 158, Loss: 0.01212246809154749, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 159, Loss: 0.011847545392811298, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 160, Loss: 0.011579902842640877, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 161, Loss: 0.011319325305521488, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 162, Loss: 0.011065601371228695, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 163, Loss: 0.010818520560860634, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 164, Loss: 0.01057788822799921, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 165, Loss: 0.010343519039452076, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 166, Loss: 0.010115224868059158, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 167, Loss: 0.009892814792692661, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 168, Loss: 0.009676122106611729, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 169, Loss: 0.009464978240430355, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 170, Loss: 0.009259220212697983, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 171, Loss: 0.009058686904609203, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 172, Loss: 0.008863221853971481, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 173, Loss: 0.00867268443107605, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 174, Loss: 0.008486930280923843, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 175, Loss: 0.008305806666612625, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 176, Loss: 0.008129190653562546, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 177, Loss: 0.007956936955451965, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 178, Loss: 0.007788912393152714, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 179, Loss: 0.007625006604939699, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 180, Loss: 0.0074650878086686134, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 181, Loss: 0.0073090242221951485, Accuracy: 100.0, Test Accuracy: 100.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182, Loss: 0.007156709209084511, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 183, Loss: 0.0070080142468214035, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 184, Loss: 0.006862829905003309, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 185, Loss: 0.006721040233969688, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 186, Loss: 0.006582535803318024, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 187, Loss: 0.006447202526032925, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 188, Loss: 0.006314937490969896, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 189, Loss: 0.006185657344758511, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 190, Loss: 0.0060592759400606155, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 191, Loss: 0.0059357257559895515, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 192, Loss: 0.005814948584884405, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 193, Loss: 0.005696908105164766, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 194, Loss: 0.005581566598266363, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 195, Loss: 0.0054688905365765095, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 196, Loss: 0.005358842667192221, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 197, Loss: 0.005251375958323479, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 198, Loss: 0.005146436858922243, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 199, Loss: 0.005043962504714727, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 200, Loss: 0.004943886771798134, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 201, Loss: 0.004846128635108471, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 202, Loss: 0.004750621039420366, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 203, Loss: 0.004657283425331116, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 204, Loss: 0.004566038493067026, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 205, Loss: 0.004476823378354311, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 206, Loss: 0.00438954820856452, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 207, Loss: 0.004304162692278624, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 208, Loss: 0.004220574628561735, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 209, Loss: 0.004138735122978687, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 210, Loss: 0.004058570135384798, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 211, Loss: 0.003980021458119154, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 212, Loss: 0.00390302250161767, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 213, Loss: 0.003827520180493593, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 214, Loss: 0.00375345884822309, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 215, Loss: 0.0036807795986533165, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 216, Loss: 0.003609430743381381, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 217, Loss: 0.0035393652506172657, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 218, Loss: 0.003470535622909665, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 219, Loss: 0.00340290367603302, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 220, Loss: 0.0033364316914230585, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 221, Loss: 0.0032710861414670944, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 222, Loss: 0.0032068435102701187, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 223, Loss: 0.003143675159662962, Accuracy: 100.0, Test Accuracy: 100.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-a453bf982124>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdatas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-a453bf982124>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(datas, labels)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;31m#@tf.function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistributed_training_utils\u001b[0m  \u001b[1;31m# pylint:disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     return distributed_training_utils.call_replica_local_fn(\n\u001b[1;32m--> 194\u001b[1;33m         replica_local_fn, *args, **kwargs)\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\distribute\\distributed_training_utils.py\u001b[0m in \u001b[0;36mcall_replica_local_fn\u001b[1;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextended\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mreplica_local_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreplica_local_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m       \u001b[1;34m\"\"\"Updates the state of the metric in a replica-local context.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m       \u001b[0mupdate_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[0mresult_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[1;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_context_for_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m       \u001b[0mupdate_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mupdate_op\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# update_op will be None in eager execution.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m       \u001b[0mmetric_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mupdate_state\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m    580\u001b[0m         y_pred, y_true)\n\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m     \u001b[0mmatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    583\u001b[0m     return super(MeanMetricWrapper, self).update_state(\n\u001b[0;32m    584\u001b[0m         matches, sample_weight=sample_weight)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mcategorical_accuracy\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m   2767\u001b[0m       math_ops.equal(\n\u001b[0;32m   2768\u001b[0m           math_ops.argmax(y_true, axis=-1), math_ops.argmax(y_pred, axis=-1)),\n\u001b[1;32m-> 2769\u001b[1;33m       K.floatx())\n\u001b[0m\u001b[0;32m   2770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2771\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[1;34m(x, dtype, name)\u001b[0m\n\u001b[0;32m    702\u001b[0m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"x\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Casting complex to real discards imaginary part.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[1;34m(x, DstT, Truncate, name)\u001b[0m\n\u001b[0;32m   2196\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cast\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2197\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DstT\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDstT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Truncate\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2198\u001b[1;33m         Truncate)\n\u001b[0m\u001b[0;32m   2199\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2200\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# hybrid model L_S \n",
    "\n",
    "# P 05\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.activations import relu\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LSLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,  num_outputs_s, num_outputs_r, num_outputs_l, activation=sigmoid, wstd = 0.3, bstd = 0.5):\n",
    "        super(LSLayer, self).__init__()\n",
    "        self.num_outputs_l = num_outputs_l\n",
    "        self.num_outputs_s = num_outputs_s \n",
    "        self.num_outputs_r = num_outputs_r\n",
    "        self.num_outputs = num_outputs_l + num_outputs_s + num_outputs_r\n",
    "        self.activation = activation\n",
    "        self.wstd = wstd\n",
    "        self.bstd = bstd\n",
    "        \n",
    "    def build(self, input_shape):  \n",
    "        self.num_inputs = input_shape[-1]\n",
    "        self.kernel = self.add_weight(\"kernel\",\n",
    "                                      shape=(int(input_shape[-1]),\n",
    "                                             self.num_outputs), \n",
    "                                      initializer=tf.keras.initializers.RandomNormal(stddev=self.wstd),\n",
    "                                     trainable=True)\n",
    "\n",
    "        self.bias = self.add_weight(\"bias\",\n",
    "                                      shape=[self.num_outputs],\n",
    "                                    initializer=tf.keras.initializers.RandomNormal(stddev=self.bstd),\n",
    "                                   trainable=True)\n",
    "        \n",
    "    \n",
    "    # F2 method LS layer\n",
    "    def call(self, input):\n",
    "        \n",
    "        isp = input.shape\n",
    "        In1 = tf.transpose(input)\n",
    "        kernel_S, kernel_L  = tf.split(self.kernel,[ self.num_outputs_s + self.num_outputs_r, self.num_outputs_l ], axis = 1 )\n",
    "        bias_S, bias_L  = tf.split(self.bias,[ self.num_outputs_s +  self.num_outputs_r, self.num_outputs_l ], axis = 0 )\n",
    "        \n",
    "        # case spherical\n",
    "        \n",
    "        s_shape  = self.num_outputs_s + self.num_outputs_r\n",
    "        In2 = tf.stack([In1] * s_shape)\n",
    "        InD = tf.transpose(In2)\n",
    "        WD = tf.stack([kernel_S] * isp[0])\n",
    "        ddd = WD - InD\n",
    "        dd0 = tf.math.multiply(ddd, ddd)\n",
    "        dd1 = tf.math.reduce_sum(dd0, axis =1)\n",
    "        dd2 = tf.cast(dd1,tf.double)\n",
    "        dd3 = tf.sqrt(dd2)\n",
    "        d_r = tf.cast(dd3,tf.float32)\n",
    "        d_R = tf.abs(bias_S)\n",
    "        d_rR = tf.math.divide_no_nan(d_r,d_R)\n",
    "        d_x0 = tf.ones(d_rR.shape) - d_rR\n",
    "        result_S = tf.math.scalar_mul(6,d_x0)\n",
    "        result_S = sigmoid(result_S)\n",
    "        \n",
    "        # case linear\n",
    "\n",
    "        d_1 = tf.stack([bias_L] * isp[0])\n",
    "        result_L = tf.matmul(input, kernel_L) + d_1 \n",
    "        result_L = relu(result_L)\n",
    "\n",
    "        #case empty, merge\n",
    "        \n",
    "        if self.num_outputs_r > 0:\n",
    "            r_S, _ = tf.split (result_S,[self.num_outputs_s, self.num_outputs_r],axis=1 )\n",
    "            r_1 = np.zeros((result_S.shape[0],self.num_outputs_r))\n",
    "            result_R = tf.cast(tf.constant(r_1),tf.float32)\n",
    "            result = tf.concat([r_S, result_R, result_L],axis=1)            \n",
    "        else:\n",
    "            result = tf.concat([result_S, result_L],axis=1)        \n",
    "        \n",
    "        return result\n",
    "    \n",
    "\n",
    "class NN_Model(Model):\n",
    "    \n",
    "    def __init__(self,c,hs,hr,hl):\n",
    "        super(NN_Model, self).__init__()\n",
    "        self.d1 = LSLayer(hs,hr,hl)\n",
    "        self.d2 = Dense(c)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        #print (\"call benn:\",x, tf.math.reduce_sum(x))\n",
    "        return self.d2(x)\n",
    "        \n",
    "\n",
    "\n",
    "#@tf.function\n",
    "def train_step(datas, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(datas, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "#@tf.function\n",
    "def test_step(datas, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    \n",
    "    predictions = model(datas, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "    \n",
    "    \n",
    "def update_NN_model (model, train_ds):\n",
    "\n",
    "    rdb = 0\n",
    "    odb = 0\n",
    "    N = min(4, model.d1.num_outputs_r)   # number of new SSN nodes\n",
    "    \n",
    "    if N <= 0:\n",
    "        return\n",
    "    \n",
    "    baditems = []\n",
    "    for datas, labels in train_ds:\n",
    "        predictions = model(datas, training=False)\n",
    "        for i in range(datas.shape[0]):\n",
    "            #print (datas.numpy()[i], predictions.numpy()[i], np.argmax(predictions.numpy()[i]), labels.numpy()[i],np.argmax(labels.numpy()[i]))\n",
    "            if np.argmax(predictions.numpy()[i]) != np.argmax(labels.numpy()[i]):\n",
    "                rdb = rdb + 1\n",
    "                baditems.append(datas.numpy()[i])\n",
    "            odb = odb + 1        \n",
    "    #print (\"pontossag:\",(odb-rdb)/odb, len(baditems))\n",
    "    inds = random.sample(range(len(baditems)), N)\n",
    "\n",
    "    neww = np.zeros((model.d1.num_inputs,N))\n",
    "    for i in range(N):\n",
    "        for j in range(model.d1.num_inputs):\n",
    "            neww[j,i] = baditems[inds[i]][j]\n",
    "\n",
    "    newb = np.zeros((N))\n",
    "    for i in range(N):\n",
    "        newb[i] = random.random()*model.d1.bstd\n",
    " \n",
    "    xu = model.d1.get_weights()\n",
    "    \n",
    "    for j in range(N):\n",
    "        for i in range(xu[0].shape[0]):\n",
    "            xu[0][i][model.d1.num_outputs_s + j] = neww[i,j]\n",
    "        xu[1][j] = newb[j]\n",
    "            \n",
    "    model.d1.set_weights(xu )\n",
    "    \n",
    "    model.d1.num_outputs_s = model.d1.num_outputs_s + N\n",
    "    model.d1.num_outputs_r = model.d1.num_outputs_r - N\n",
    "\n",
    "C= 6\n",
    "L= 50\n",
    "N= 5000\n",
    "M= 6\n",
    "HS = 25\n",
    "HR = 10\n",
    "HL = 0\n",
    "EPOCHS = 300\n",
    "\n",
    "# Create an instance of the model\n",
    "model = NN_Model(C,HS,HR,HL)\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "#loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "(x_train,y_train,x_test,y_test) = gen_data_array(C, L, N, M)\n",
    "#print (x_train[:2])\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).batch(32)\n",
    "#print (train_ds)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for epoch in range(EPOCHS):\n",
    "  # Reset the metrics at the start of the next epoch\n",
    "\n",
    "    if epoch > 1:\n",
    "        xu2 = model.d1.get_weights()\n",
    "        #print (\"WW\",epoch, \"WW\", xu2[0])\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    for datas, labels in train_ds:\n",
    "        train_step(datas, labels)\n",
    "        \n",
    "                \n",
    "    for test_datas, test_labels in test_ds:\n",
    "        #print (\"test_data_shape\", test_datas.shape)\n",
    "        predictions = model(test_datas, training=False)\n",
    "        #print (\"ttttttttttttttttttt\")\n",
    "        #for i in range(test_datas.shape[0]):\n",
    "        #    print (predictions.numpy()[i], test_labels.numpy()[i])\n",
    "        test_step(test_datas, test_labels)\n",
    "        \n",
    "    if epoch % 5 == 3:\n",
    "        update_NN_model (model, train_ds)\n",
    "        \n",
    "        \n",
    "    X.append(epoch)\n",
    "    Y.append(test_accuracy.result() * 100)\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "        f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "        f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "      )    \n",
    "\n",
    "\n",
    "    \n",
    "plt.plot(X, Y,label=\"Accuracy curve\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.2 6.696599468718765\n"
     ]
    }
   ],
   "source": [
    "import statistics \n",
    "\n",
    "x = [82,97,81,75,86,85,76,86,92,82]\n",
    "print (sum(x)/len(x), statistics.stdev(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "def gen_cluster_data_list(Cv, Lv, Nv, Mv):\n",
    "    Tr = []\n",
    "    Ts = []\n",
    "    C = Cv  # number of categories\n",
    "    L = Lv   # number of centers\n",
    "    N = Nv  # number of elements\n",
    "    M = Mv  # number of dimensions\n",
    "    X, y = make_blobs(n_samples=N, centers=L, n_features=M,cluster_std=.5, random_state=11)\n",
    "    cmap = []\n",
    "    for _ in range(L):\n",
    "        cmap.append(random.randint(0,C-1))\n",
    "    cols = []\n",
    "    for i in range(N):\n",
    "        cols.append(cmap[y[i]])\n",
    "\n",
    "    for i in range(int(0.9*N)):\n",
    "        row = [X[i,j] for j in range(M)]\n",
    "        row.append(cols[i])\n",
    "        Tr.append(row)\n",
    "    \n",
    "    for i in range(int(0.9*N)+1,N):\n",
    "        row = [X[i,j] for j in range(M)]\n",
    "        row.append(cols[i])\n",
    "        Ts.append(row)\n",
    "        \n",
    "    return (Tr, Ts)\n",
    "\n",
    "def normalize (train):\n",
    "    mx = []\n",
    "    mn = []\n",
    "    for i in range(len(train[0])-1):\n",
    "        mx.append(max([x[i] for x in train ]))\n",
    "        mn.append(min([x[i] for x in train ]))\n",
    "    for row in train:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i] - mn[i]) / (mx[i] - mn[i]) \n",
    "    return train\n",
    "\n",
    "\n",
    "def gen_data_array(Cv, Lv, Nv, Mv):\n",
    "    C = Cv  # number of categories\n",
    "    L = Lv   # number of centers\n",
    "    N = Nv  # number of elements\n",
    "    M = Mv  # number of dimensions\n",
    "    (T1,T2)  = gen_cluster_data_list(C, L, N, M)\n",
    "\n",
    "    T = normalize(T1)\n",
    "    N = len(T)\n",
    "    x2_train = np.zeros((N,M),dtype='float32')\n",
    "    y2_train = np.zeros((N,C))\n",
    "    for i in range(N):\n",
    "        row = T[i]\n",
    "        for j in range(M):\n",
    "            x2_train[i,j] = row[j]\n",
    "        y2_train[i,row[-1]] = 1\n",
    "\n",
    "    Ts = normalize(T2)\n",
    "    Ns = len(Ts)\n",
    "    x2_test = np.zeros((Ns,M),dtype='float32')\n",
    "    y2_test = np.zeros((Ns,C))\n",
    "    for i in range(Ns):\n",
    "        row = Ts[i]\n",
    "        for j in range(M):\n",
    "            x2_test[i,j] = row[j]\n",
    "        y2_test[i, row[-1]] = 1\n",
    "        \n",
    "    return (x2_train,y2_train, x2_test, y2_test)\n",
    "\n",
    "def gen_data_array_s(Cv, Lv, Nv, Mv):\n",
    "    C = Cv  # number of categories\n",
    "    L = Lv   # number of centers\n",
    "    N = Nv  # number of elements\n",
    "    M = Mv  # number of dimensions\n",
    "    (T1,T2)  = gen_cluster_data_list(C, L, N, M)\n",
    "\n",
    "    T = normalize(T1)\n",
    "    N = len(T)\n",
    "    x2_train = np.zeros((N,M),dtype='float32')\n",
    "    y2_train = np.zeros((N,1))\n",
    "    for i in range(N):\n",
    "        row = T[i]\n",
    "        for j in range(M):\n",
    "            x2_train[i,j] = row[j]\n",
    "        y2_train[i] = row[-1]\n",
    "\n",
    "    Ts = normalize(T2)\n",
    "    Ns = len(Ts)\n",
    "    x2_test = np.zeros((Ns,M),dtype='float32')\n",
    "    y2_test = np.zeros((Ns,1))\n",
    "    for i in range(Ns):\n",
    "        row = Ts[i]\n",
    "        for j in range(M):\n",
    "            x2_test[i,j] = row[j]\n",
    "        y2_test[i] = row[-1]\n",
    "        \n",
    "    return (x2_train,y2_train, x2_test, y2_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
