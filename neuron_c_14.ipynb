{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N= 623\n",
      "k= 0 ------------------------------\n",
      "XX 0.73913044 0.67105263\n",
      "update layer\n",
      "XX 0.7853692 0.7631579\n",
      "XX 0.7893248 0.7631579\n",
      "accuracy: 0.7631579\n",
      "k= 1 ------------------------------\n",
      "update layer\n",
      "accuracy: 0.7631579\n",
      "k= 2 ------------------------------\n",
      "update layer\n",
      "accuracy: 0.7631579\n",
      "k= 3 ------------------------------\n",
      "update layer\n",
      "accuracy: 0.7631579\n",
      "k= 4 ------------------------------\n",
      "update layer\n",
      "accuracy: 0.7631579\n",
      "k= 5 ------------------------------\n",
      "accuracy: 0.7631579\n",
      "k= 6 ------------------------------\n",
      "accuracy: 0.7631579\n",
      "k= 7 ------------------------------\n",
      "XX 0.79234976 0.80263156\n",
      "accuracy: 0.80263156\n",
      "k= 8 ------------------------------\n",
      "XX 0.8509317 0.7894737\n",
      "XX 0.86002123 0.7894737\n",
      "accuracy: 0.7894737\n",
      "k= 9 ------------------------------\n",
      "accuracy: 0.7894737\n"
     ]
    }
   ],
   "source": [
    "# hybrid model DLS\n",
    "\n",
    "# P 05\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.activations import relu\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "class LSLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,  num_outputs_s, num_outputs_r, num_outputs_l, activation=sigmoid, wstd = 0.3, bstd = 0.5):\n",
    "        super(LSLayer, self).__init__()\n",
    "        self.num_outputs_l = num_outputs_l\n",
    "        self.num_outputs_s = num_outputs_s \n",
    "        self.num_outputs_r = num_outputs_r\n",
    "        self.num_outputs = num_outputs_l + num_outputs_s + num_outputs_r\n",
    "        self.activation = activation\n",
    "        self.wstd = wstd\n",
    "        self.bstd = bstd\n",
    "        \n",
    "    def build(self, input_shape):  \n",
    "        self.num_inputs = input_shape[-1]\n",
    "        self.kernel = self.add_weight(\"kernel\",\n",
    "                                      shape=(int(input_shape[-1]),\n",
    "                                             self.num_outputs), \n",
    "                                      initializer=tf.keras.initializers.RandomNormal(stddev=self.wstd),\n",
    "                                     trainable=True)\n",
    "\n",
    "        self.bias = self.add_weight(\"bias\",\n",
    "                                      shape=[self.num_outputs],\n",
    "                                    initializer=tf.keras.initializers.RandomNormal(stddev=self.bstd),\n",
    "                                   trainable=True)\n",
    "        \n",
    "        params = self.get_weights()\n",
    "    \n",
    "        for j in range(self.num_outputs):\n",
    "            r = 0\n",
    "            for i in range(self.num_inputs):\n",
    "                params[0][i][j] = params[0][i][j]*random.random()*3\n",
    "                r += (params[0][i][j]-0.5)**2\n",
    "            r = math.sqrt(r)\n",
    "            params[1][j] = r\n",
    "\n",
    "        self.set_weights(params )\n",
    "        \n",
    "    \n",
    "    # F2 method LS layer\n",
    "    def call(self, input):\n",
    "        \n",
    "        isp = input.shape\n",
    "        In1 = tf.transpose(input)\n",
    "        kernel_S, kernel_L  = tf.split(self.kernel,[ self.num_outputs_s + self.num_outputs_r, self.num_outputs_l ], axis = 1 )\n",
    "        bias_S, bias_L  = tf.split(self.bias,[ self.num_outputs_s +  self.num_outputs_r, self.num_outputs_l ], axis = 0 )\n",
    "        \n",
    "        # case spherical\n",
    "        \n",
    "        s_shape  = self.num_outputs_s + self.num_outputs_r\n",
    "        In2 = tf.stack([In1] * s_shape)\n",
    "        InD = tf.transpose(In2)\n",
    "        WD = tf.stack([kernel_S] * isp[0])\n",
    "        ddd = WD - InD\n",
    "        dd0 = tf.math.multiply(ddd, ddd)\n",
    "        dd1 = tf.math.reduce_sum(dd0, axis =1)\n",
    "        dd2 = tf.cast(dd1,tf.double)\n",
    "        dd3 = tf.sqrt(dd2)\n",
    "        d_r = tf.cast(dd3,tf.float32)\n",
    "        d_R = tf.abs(bias_S)\n",
    "        d_rR = tf.math.divide_no_nan(d_r,d_R)\n",
    "        d_x0 = tf.ones(d_rR.shape) - d_rR\n",
    "        result_S = tf.math.scalar_mul(6,d_x0)\n",
    "        result_S = sigmoid(result_S)\n",
    "        \n",
    "        # case linear\n",
    "\n",
    "        d_1 = tf.stack([bias_L] * isp[0])\n",
    "        result_L = tf.matmul(input, kernel_L) + d_1 \n",
    "        result_L = relu(result_L)\n",
    "\n",
    "        #case empty, merge\n",
    "        \n",
    "        '''\n",
    "        #print (self.num_outputs_r)\n",
    "        if self.num_outputs_r > 0:\n",
    "            r_S, _ = tf.split (result_S,[self.num_outputs_s, self.num_outputs_r],axis=1 )\n",
    "            r_1 = np.zeros((result_S.shape[0],self.num_outputs_r))\n",
    "            result_R = tf.cast(tf.constant(r_1),tf.float32)\n",
    "            result = tf.concat([r_S, result_R, result_L],axis=1)            \n",
    "            #print (self.num_outputs_s, self.num_outputs_r)\n",
    "            #print (\"result_S\", result_S)\n",
    "            #print (\"result_L\", result_L)\n",
    "            #print (\"result\", result)\n",
    "        else:\n",
    "            result = tf.concat([result_S, result_L],axis=1)        \n",
    "        '''\n",
    "        \n",
    "        result = tf.concat([result_S, result_L],axis=1)        \n",
    "        \n",
    "        return result\n",
    "    \n",
    "\n",
    "class NN_Model(Model):\n",
    "    \n",
    "    def __init__(self,c,hs,hr,hl):\n",
    "        super(NN_Model, self).__init__()\n",
    "        self.d1 = LSLayer(hs,hr,hl)\n",
    "        self.d2 = Dense(c)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        #print (\"call benn:\",x, tf.math.reduce_sum(x))\n",
    "        return self.d2(x)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "#@tf.function\n",
    "def train_step(datas, labels,modelk,loss_objectk,optimizerk,train_lossk,train_accuracyk):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = modelk(datas, training=True)\n",
    "        loss = loss_objectk(labels, predictions)\n",
    "    gradients = tape.gradient(loss, modelk.trainable_variables)\n",
    "    optimizerk.apply_gradients(zip(gradients, modelk.trainable_variables))\n",
    "\n",
    "    train_lossk(loss)\n",
    "    train_accuracyk(labels, predictions)\n",
    "\n",
    "#@tf.function\n",
    "def test_step(datas, labels,modelk,loss_objectk,test_lossk,test_accuracyk):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    \n",
    "    predictions = modelk(datas, training=False)\n",
    "    t_loss = loss_objectk(labels, predictions)\n",
    "\n",
    "    test_lossk(t_loss)\n",
    "    test_accuracyk(labels, predictions)\n",
    "    \n",
    "\n",
    "def analyse_NN (model, train_ds,optimizer):\n",
    "    \n",
    "    #print (\"===============================================\")\n",
    "    cnt = 0\n",
    "    xu = model.d2.get_weights()\n",
    "    C = xu[0].shape[1]\n",
    "    M = xu[0].shape[0]\n",
    "    Nfitness = [0 for _ in range(M)]\n",
    "    for datas, labels in train_ds:\n",
    "        dA = datas.numpy()\n",
    "        dL = labels.numpy()\n",
    "        for i in range(dA.shape[0]):\n",
    "            cnt += 1\n",
    "            #print (\"sample \", cnt, \"..................\")\n",
    "            tA1 = tf.constant(dA[i])\n",
    "            tA = tf.reshape(tA1,[1,dA.shape[1]])\n",
    "            #predictions = model(tA, training=False)\n",
    "            to1 = model.d1 (tA)\n",
    "            to2 = model.d2 (to1)\n",
    "            #print (\"output:\",to2.numpy()[0],\"labels\", dL[i])\n",
    "            #print (xu[0].shape)\n",
    "            for j in range(M):\n",
    "                vl = []\n",
    "                for c in range(C):\n",
    "                    xw = xu[0][:,c]\n",
    "                    xo1 = to1.numpy()[0]\n",
    "                    val = xo1[j]*xw[j] +  tf.constant(xu[1][c]).numpy() / M \n",
    "                    vl.append(val)\n",
    "                if vl.index(max(vl)) == list(dL[i]).index(max(dL[i])):\n",
    "                    val = 1\n",
    "                else:\n",
    "                    val = 0            \n",
    "                Nfitness[j] += val\n",
    "    \n",
    "    \n",
    "    \n",
    "    #for j in range(M):\n",
    "    #    mode = \"S\"\n",
    "    #    if j >= model.d1.num_outputs_s + model.d1.num_outputs_r:\n",
    "    #        mode = \"L\"\n",
    "    #    print (j, mode, 'val:', Nfitness[j])                \n",
    "    print ('                   ', sum(Nfitness),min(Nfitness),max(Nfitness))                               \n",
    "            \n",
    "    #print (\"===============================================\")\n",
    "\n",
    "    \n",
    "def update_NN_model (model, train_ds,optimizer):\n",
    "\n",
    "    rdb = 0\n",
    "    odb = 0\n",
    "    #N = min(5, model.d1.num_outputs_r)   # number of new SSN nodes\n",
    "    N = model.d1.num_outputs_r   # number of new SSN nodes\n",
    "    \n",
    "    if N <= 0:\n",
    "        return\n",
    "    \n",
    "    # k-means\n",
    "    \n",
    "    baditems = []\n",
    "    for datas, labels in train_ds:\n",
    "        predictions = model(datas, training=False)\n",
    "        for i in range(datas.shape[0]):\n",
    "            #print (datas.numpy()[i], predictions.numpy()[i], np.argmax(predictions.numpy()[i]), labels.numpy()[i],np.argmax(labels.numpy()[i]))\n",
    "            if np.argmax(predictions.numpy()[i]) != np.argmax(labels.numpy()[i]):\n",
    "                rdb = rdb + 1\n",
    "                baditems.append(datas.numpy()[i])\n",
    "            odb = odb + 1        \n",
    "    #print (\"pontossag:\",(odb-rdb)/odb, len(baditems))\n",
    "    N = min(N, len(baditems))\n",
    "    if N == 0:\n",
    "        return\n",
    "    inds = random.sample(range(len(baditems)), N)\n",
    "    \n",
    "    print (\"update layer\")\n",
    "    #print (baditems)\n",
    "    centers = KMeans(n_clusters=N).fit(baditems).cluster_centers_\n",
    "    #print (\"centers:\")\n",
    "    #print (centers)\n",
    "    neww = np.zeros((model.d1.num_inputs,N))\n",
    "    for i in range(N):\n",
    "        for j in range(model.d1.num_inputs):\n",
    "            neww[j,i] = centers[i][j]\n",
    "    #print (\"neww\")\n",
    "    #print (neww)\n",
    "            \n",
    "    xu = model.d1.get_weights()\n",
    "        \n",
    "    idx = random.sample(range(model.d1.num_outputs_s + model.d1.num_outputs_r),N)\n",
    "    for j1 in range(N):\n",
    "        j = idx[j1]\n",
    "        for i in range(xu[0].shape[0]):\n",
    "            xu[0][i][j] = neww[i,j1]\n",
    "           \n",
    "    for j1 in range(N):\n",
    "        j = idx[j1]\n",
    "        r = 0\n",
    "        for i in range(xu[0].shape[0]):\n",
    "            r += (xu[0][i][j]-0.5)**2\n",
    "        r = math.sqrt(r)\n",
    "        xu[1][j] = r\n",
    "            \n",
    "            \n",
    "    model.d1.set_weights(xu )\n",
    "    \n",
    "    #model.d1.num_outputs_s = model.d1.num_outputs_s + N\n",
    "    #model.d1.num_outputs_r = model.d1.num_outputs_r - N\n",
    "\n",
    "    #optimizer = tf.keras.optimizers.Adam\n",
    "    #for var in optimizer.variables():\n",
    "    #    var.assign(tf.zeros_like(var))\n",
    "    \n",
    "\n",
    "K = 10\n",
    "\n",
    "# (x_train,y_train,x_test,y_test) = gen_data_array_cv(K)\n",
    "\n",
    "M = x_train[0].shape[1]\n",
    "C = y_train[0].shape[1]\n",
    "\n",
    "EPOCHS = 80\n",
    "Eupd = [20]\n",
    "\n",
    "H = M * 5\n",
    "HS = M\n",
    "HR = M\n",
    "HL = 5*M\n",
    "B = 32\n",
    "\n",
    "model = []\n",
    "loss_object =  []\n",
    "optimizer = []\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "val_loss = []\n",
    "val_accuracy = []\n",
    "train_ds = []\n",
    "val_ds = []\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(B)\n",
    "\n",
    "\n",
    "\n",
    "best_v = 0\n",
    "best_t = 0\n",
    "\n",
    "print (\"N=\", x_train[0].shape[0])\n",
    "\n",
    "for k in range(K):\n",
    "    print (\"k=\",k, \"------------------------------\")\n",
    "    # Create an instance of the model\n",
    "    model.append( NN_Model(C,HS,HR,HL))\n",
    "    #model.append( NN_Model(H,C,0,0))\n",
    "\n",
    "    loss_object.append(tf.keras.losses.CategoricalCrossentropy(from_logits=True))\n",
    "    #loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "    #optimizer.append(tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07,))\n",
    "    optimizer.append(tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07,))\n",
    "    train_loss.append(tf.keras.metrics.Mean(name='train_loss'))\n",
    "    train_accuracy.append(tf.keras.metrics.CategoricalAccuracy(name='train_accuracy'))\n",
    "\n",
    "    test_loss.append(tf.keras.metrics.Mean(name='test_loss'))\n",
    "    test_accuracy.append(tf.keras.metrics.CategoricalAccuracy(name='test_accuracy'))\n",
    "    \n",
    "    val_loss.append(tf.keras.metrics.Mean(name='val_loss'))\n",
    "    val_accuracy.append(tf.keras.metrics.CategoricalAccuracy(name='val_accuracy'))\n",
    "    \n",
    "\n",
    "    #print (x_train[:2])\n",
    "    train_ds.append( tf.data.Dataset.from_tensor_slices(\n",
    "        (x_train[k], y_train[k])).batch(B))\n",
    "    val_ds.append( tf.data.Dataset.from_tensor_slices(\n",
    "        (x_val[k], y_val[k])).batch(B))\n",
    "    #print (train_ds)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for epoch in range(EPOCHS):\n",
    "      # Reset the metrics at the start of the next epoch\n",
    "\n",
    "        train_loss[k].reset_states()\n",
    "        train_accuracy[k].reset_states()\n",
    "        test_loss[k].reset_states()\n",
    "        test_accuracy[k].reset_states()\n",
    "\n",
    "        for datas, labels in train_ds[k]:\n",
    "            train_step(datas, labels,model[k],loss_object[k],optimizer[k],train_loss[k],train_accuracy[k])\n",
    "\n",
    "\n",
    "        for test_datas, test_labels in test_ds:\n",
    "            #tpredictions = model[k](test_datas, training=False)\n",
    "            test_step(test_datas, test_labels,model[k],loss_object[k],test_loss[k],test_accuracy[k])\n",
    "\n",
    "        for val_datas, val_labels in val_ds[k]:\n",
    "            #vpredictions = model[k](val_datas, training=False)\n",
    "            test_step(val_datas, val_labels,model[k],loss_object[k],val_loss[k],val_accuracy[k])\n",
    "            \n",
    "            \n",
    "        #if epoch % 2 == 1:\n",
    "        #    print (\"Anal epoch\",epoch)\n",
    "        #    analyse_NN (model[k], train_ds[k], optimizer[k])\n",
    "    \n",
    "        if epoch in Eupd and k < K/2:\n",
    "            update_NN_model (model[k], train_ds[k], optimizer[k])\n",
    "\n",
    "\n",
    "        X.append(epoch)\n",
    "        Y.append(test_accuracy[k].result() * 100)\n",
    "        if epoch % 20 == 0:\n",
    "            #print(\n",
    "            #    f'Epoch {epoch + 1}, '\n",
    "            #    f'Loss: {train_loss[k].result()}, '\n",
    "            #    f'Val Accuracy: {val_accuracy[k].result() * 100}, '\n",
    "            #    f'Test Accuracy: {test_accuracy[k].result() * 100}'\n",
    "            #  )    \n",
    "            \n",
    "            if val_accuracy[k].result() > best_v:\n",
    "                best_v = val_accuracy[k].result().numpy()\n",
    "                best_t = test_accuracy[k].result().numpy()\n",
    "                print(\"XX\", best_v,best_t)\n",
    "            #print(model.d1.bias.numpy())\n",
    "\n",
    "\n",
    "    #print(\n",
    "    #    f'Epoch {epoch + 1}, '\n",
    "    #    f'Loss: {train_loss[k].result()}, '\n",
    "    #    f'Val Accuracy: {val_accuracy[k].result() * 100}, '\n",
    "    #    f'Test Accuracy: {test_accuracy[k].result() * 100}'\n",
    "    #  )    \n",
    "    #acclist.append(test_accuracy[k].result())\n",
    "    print (\"accuracy:\",best_t)\n",
    "    #plt.plot(X, Y,label=\"Accuracy curve\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.6 7.3972968035627735\n"
     ]
    }
   ],
   "source": [
    "import statistics \n",
    "\n",
    "x = [80.2,80.2,63.4,80.2,79]\n",
    "print (sum(x)/len(x), statistics.stdev(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 0\n",
      "[0.5445604773535244, 0.7246377] [0.5801433622837067, 0.69736844]\n",
      "[0.475445914959562, 0.7826087] [0.535427672298331, 0.7368421]\n",
      "[0.4424303104912025, 0.8115942] [0.5056477392974653, 0.7631579]\n",
      "k= 1\n",
      "k= 2\n",
      "k= 3\n",
      "k= 4\n",
      "k= 5\n",
      "k= 6\n",
      "k= 7\n",
      "k= 8\n",
      "[0.4281147232522135, 0.8405797] [0.5255143140491686, 0.7631579]\n",
      "k= 9\n",
      "acc= 0.7631579 ( 0.8405797 )\n"
     ]
    }
   ],
   "source": [
    "# 10-fold coross validation \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "\n",
    "\n",
    "K = 10\n",
    "\n",
    "#(x_train,y_train,x_test,y_test) = gen_data_array_cv(K)\n",
    "\n",
    "best_v = 0\n",
    "best_t = 0\n",
    "\n",
    "for k in range(K):\n",
    "    print (\"k=\",k)\n",
    "    N = x_train[k].shape[0]\n",
    "    M = x_train[k].shape[1]\n",
    "    C = y_train[k].shape[1]\n",
    "    H = M * 5\n",
    "    E = 80\n",
    "\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(H, input_shape=(M,), activation='relu'),\n",
    "      tf.keras.layers.Dense(C)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=loss_fn,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    es = 20\n",
    "    #loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    for e in range(int(E/es)):\n",
    "        model.fit(x_train[k], y_train[k], validation_data=[x_val[k],y_val[k]], epochs=es,verbose = 0)\n",
    "        res_test = model.evaluate(x_test,  y_test, verbose=0)\n",
    "        res_val = model.evaluate(x_val[k],  y_val[k], verbose=0)\n",
    "        if res_val[1] > best_v:\n",
    "            best_v = res_val[1]\n",
    "            best_t = res_test[1]\n",
    "            print (res_val, res_test)\n",
    "            \n",
    "print (\"acc=\", best_t, \"(\",best_v,\")\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.03999999999999 1.0597169433391156\n"
     ]
    }
   ],
   "source": [
    "import statistics \n",
    "\n",
    "x = [75.1,76.2, 77.6,75.0,76.3]\n",
    "print (sum(x)/len(x), statistics.stdev(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data (768, 9)\n"
     ]
    }
   ],
   "source": [
    "# read benchmark data\n",
    "\n",
    "from pandas import  read_csv\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "def gen_data_array_cv(K):\n",
    "    \n",
    "    \n",
    "    dataT = read_csv (\"pima-indians-diabetes.csv\", header =None)\n",
    "    dataA = np.array(dataT)    \n",
    "        \n",
    "    N = dataA.shape[0]\n",
    "    print (\"Data\",dataA.shape)\n",
    "    M = dataA.shape[1]-1\n",
    "\n",
    "    Nte = int(0.1*N)\n",
    "    Ntr = N - Nte\n",
    "\n",
    "    cval = dict()\n",
    "    for i in range(N):\n",
    "        cval[dataA[i][-1]] = 1\n",
    "    C = len(cval.keys())\n",
    "    \n",
    "    minv = [0 for _ in range(M)]\n",
    "    maxv = [0 for _ in range(M)]\n",
    "    \n",
    "    for j in range(M):\n",
    "        minv[j] =  dataA[0,j]\n",
    "        maxv[j] =  dataA[0,j]\n",
    "        \n",
    "    for i in range(N):\n",
    "        for j in range(M):\n",
    "            minv[j] = min( dataA[i,j] , minv[j])\n",
    "            maxv[j] = max( dataA[i,j] , maxv[j])\n",
    "          \n",
    "    orders = random.sample(range(N), N)\n",
    "    \n",
    "    x2_test = np.zeros((Nte,M),dtype='float32')\n",
    "    y2_test = np.zeros((Nte,C))\n",
    "    \n",
    "    ite = 0\n",
    "    for n in range(Ntr, N):\n",
    "        i = orders[n]\n",
    "        for j in range(M):\n",
    "            x2_test[ite,j] = (dataA[i,j] - minv[j]) / (maxv[j] - minv[j])\n",
    "        y2_test[ite, int(dataA[i,M])] = 1\n",
    "        ite += 1    \n",
    "       \n",
    "    Ntrv = int(Ntr/K)\n",
    "    Ntrt = Ntr - Ntrv\n",
    "    \n",
    "    x2_train = []\n",
    "    y2_train = []\n",
    "    x2_val = []\n",
    "    y2_val = []\n",
    "    \n",
    "    for k in range(K):\n",
    "        x2_train.append(np.zeros((Ntrt,M),dtype='float32'))\n",
    "        y2_train.append(np.zeros((Ntrt,C)))\n",
    "        x2_val.append(np.zeros((Ntrv,M),dtype='float32'))\n",
    "        y2_val.append(np.zeros((Ntrv,C)))\n",
    "    \n",
    "        itr = 0\n",
    "        ite = 0\n",
    "        for n in range(Ntr):\n",
    "            i = orders[n]\n",
    "            if n >= k*Ntrv and n < (k+1)*Ntrv:\n",
    "                for j in range(M):\n",
    "                    x2_val[k][ite,j] = (dataA[i,j] - minv[j]) / (maxv[j] - minv[j])\n",
    "                y2_val[k][ite, int(dataA[i,M])] = 1\n",
    "                ite += 1\n",
    "            else:            \n",
    "                for j in range(M):\n",
    "                    x2_train[k][itr,j] = (dataA[i,j] - minv[j]) / (maxv[j] - minv[j])\n",
    "                y2_train[k][itr,int(dataA[i,M])] = 1\n",
    "                itr += 1\n",
    "                \n",
    "    return (x2_train,y2_train, x2_val, y2_val, x2_test, y2_test)\n",
    "\n",
    "K = 10\n",
    "(x_train,y_train,x_val,y_val, x_test,y_test) = gen_data_array_cv(K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
