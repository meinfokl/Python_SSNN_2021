{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.90179956 0.9417195  0.37377474 0.07043936 0.53236914 0.7381097 ]\n",
      " [0.5166569  0.40206912 0.8782092  0.26794338 0.8631228  0.28210485]]\n",
      "Epoch 1, Loss: 1.7332810163497925, Accuracy: 26.488887786865234, Test Accuracy: 28.857717514038086\n",
      "Epoch 2, Loss: 1.6665451526641846, Accuracy: 32.35555648803711, Test Accuracy: 32.66532897949219\n",
      "Epoch 3, Loss: 1.6180094480514526, Accuracy: 36.31111145019531, Test Accuracy: 31.46292495727539\n",
      "Epoch 4, Loss: 1.5753222703933716, Accuracy: 36.733333587646484, Test Accuracy: 32.46493148803711\n",
      "Epoch 5, Loss: 1.535463809967041, Accuracy: 36.266666412353516, Test Accuracy: 33.46693420410156\n",
      "Epoch 6, Loss: 1.4999189376831055, Accuracy: 36.088890075683594, Test Accuracy: 34.068138122558594\n",
      "Epoch 7, Loss: 1.4675507545471191, Accuracy: 36.91111373901367, Test Accuracy: 35.470943450927734\n",
      "Epoch 8, Loss: 1.4377931356430054, Accuracy: 37.68888854980469, Test Accuracy: 37.474952697753906\n",
      "Epoch 9, Loss: 1.4098310470581055, Accuracy: 38.82221984863281, Test Accuracy: 37.675350189208984\n",
      "Epoch 10, Loss: 1.3822635412216187, Accuracy: 40.4888916015625, Test Accuracy: 38.6773567199707\n",
      "Epoch 11, Loss: 1.3542519807815552, Accuracy: 42.266666412353516, Test Accuracy: 39.47895812988281\n",
      "Epoch 12, Loss: 1.3268994092941284, Accuracy: 43.46666717529297, Test Accuracy: 41.28256607055664\n",
      "Epoch 13, Loss: 1.3001328706741333, Accuracy: 44.511112213134766, Test Accuracy: 43.28657531738281\n",
      "Epoch 14, Loss: 1.273849368095398, Accuracy: 45.64444351196289, Test Accuracy: 44.8897819519043\n",
      "Epoch 15, Loss: 1.2474662065505981, Accuracy: 47.71110916137695, Test Accuracy: 46.492984771728516\n",
      "Epoch 16, Loss: 1.221700668334961, Accuracy: 49.97777557373047, Test Accuracy: 48.09619140625\n",
      "Epoch 17, Loss: 1.1963289976119995, Accuracy: 51.77777862548828, Test Accuracy: 49.699398040771484\n",
      "Epoch 18, Loss: 1.1713321208953857, Accuracy: 53.35555648803711, Test Accuracy: 51.50300979614258\n",
      "Epoch 19, Loss: 1.146313190460205, Accuracy: 54.888885498046875, Test Accuracy: 53.90781784057617\n",
      "Epoch 20, Loss: 1.1214402914047241, Accuracy: 56.44444274902344, Test Accuracy: 55.71142578125\n",
      "Epoch 21, Loss: 1.0960917472839355, Accuracy: 57.77777862548828, Test Accuracy: 56.312625885009766\n",
      "Epoch 22, Loss: 1.0711979866027832, Accuracy: 59.33333206176758, Test Accuracy: 57.31462860107422\n",
      "Epoch 23, Loss: 1.0474079847335815, Accuracy: 60.511112213134766, Test Accuracy: 60.72144317626953\n",
      "Epoch 24, Loss: 1.0237810611724854, Accuracy: 62.62221908569336, Test Accuracy: 63.727455139160156\n",
      "Epoch 25, Loss: 0.9998903870582581, Accuracy: 64.0, Test Accuracy: 65.33065795898438\n",
      "Epoch 26, Loss: 0.9761461615562439, Accuracy: 65.44444274902344, Test Accuracy: 66.53306579589844\n",
      "Epoch 27, Loss: 0.9524087905883789, Accuracy: 67.31111145019531, Test Accuracy: 67.53507232666016\n",
      "Epoch 28, Loss: 0.92620450258255, Accuracy: 68.68888854980469, Test Accuracy: 68.73748016357422\n",
      "Epoch 29, Loss: 0.9007012248039246, Accuracy: 70.0, Test Accuracy: 70.1402816772461\n",
      "Epoch 30, Loss: 0.8754050731658936, Accuracy: 70.93333435058594, Test Accuracy: 70.94188690185547\n",
      "Epoch 31, Loss: 0.8513899445533752, Accuracy: 71.73332977294922, Test Accuracy: 71.14228820800781\n",
      "Epoch 32, Loss: 0.8285120725631714, Accuracy: 72.64443969726562, Test Accuracy: 71.14228820800781\n",
      "Epoch 33, Loss: 0.8064205050468445, Accuracy: 73.5777816772461, Test Accuracy: 72.144287109375\n",
      "Epoch 34, Loss: 0.7850934863090515, Accuracy: 74.73333740234375, Test Accuracy: 73.74749755859375\n",
      "Epoch 35, Loss: 0.7644632458686829, Accuracy: 76.11111450195312, Test Accuracy: 75.15029907226562\n",
      "Epoch 36, Loss: 0.7444404363632202, Accuracy: 77.60000610351562, Test Accuracy: 76.55310821533203\n",
      "Epoch 37, Loss: 0.725047767162323, Accuracy: 78.4000015258789, Test Accuracy: 78.3567123413086\n",
      "Epoch 38, Loss: 0.7062110900878906, Accuracy: 79.33333587646484, Test Accuracy: 78.75751495361328\n",
      "Epoch 39, Loss: 0.6878011226654053, Accuracy: 80.13333129882812, Test Accuracy: 78.95791625976562\n",
      "Epoch 40, Loss: 0.6700646281242371, Accuracy: 81.04444885253906, Test Accuracy: 79.95991516113281\n",
      "Epoch 41, Loss: 0.653016209602356, Accuracy: 81.8888931274414, Test Accuracy: 80.96192169189453\n",
      "Epoch 42, Loss: 0.6365918517112732, Accuracy: 82.4222183227539, Test Accuracy: 80.96192169189453\n",
      "Epoch 43, Loss: 0.6207957863807678, Accuracy: 82.97777557373047, Test Accuracy: 81.56312561035156\n",
      "Epoch 44, Loss: 0.6055970191955566, Accuracy: 83.57777404785156, Test Accuracy: 81.1623306274414\n",
      "Epoch 45, Loss: 0.5909505486488342, Accuracy: 84.13333129882812, Test Accuracy: 81.1623306274414\n",
      "Epoch 46, Loss: 0.5768361687660217, Accuracy: 84.37777709960938, Test Accuracy: 81.7635269165039\n",
      "Epoch 47, Loss: 0.5631487965583801, Accuracy: 84.5999984741211, Test Accuracy: 81.7635269165039\n",
      "Epoch 48, Loss: 0.5498912334442139, Accuracy: 84.84444427490234, Test Accuracy: 81.56312561035156\n",
      "Epoch 49, Loss: 0.5370408296585083, Accuracy: 85.0888900756836, Test Accuracy: 81.96392822265625\n",
      "Epoch 50, Loss: 0.5245915651321411, Accuracy: 85.31111145019531, Test Accuracy: 82.76553344726562\n",
      "Epoch 51, Loss: 0.5124762058258057, Accuracy: 85.64443969726562, Test Accuracy: 82.96593475341797\n",
      "Epoch 52, Loss: 0.5006856918334961, Accuracy: 86.0, Test Accuracy: 83.96793365478516\n",
      "Epoch 53, Loss: 0.489184707403183, Accuracy: 86.42222595214844, Test Accuracy: 84.56913757324219\n",
      "Epoch 54, Loss: 0.4779823124408722, Accuracy: 86.77777862548828, Test Accuracy: 85.9719467163086\n",
      "Epoch 55, Loss: 0.46699219942092896, Accuracy: 87.5777816772461, Test Accuracy: 87.37474822998047\n",
      "Epoch 56, Loss: 0.45618265867233276, Accuracy: 88.04444885253906, Test Accuracy: 87.77555084228516\n",
      "Epoch 57, Loss: 0.4455280601978302, Accuracy: 88.62222290039062, Test Accuracy: 88.17635345458984\n",
      "Epoch 58, Loss: 0.43494144082069397, Accuracy: 89.11111450195312, Test Accuracy: 89.37875366210938\n",
      "Epoch 59, Loss: 0.4245917499065399, Accuracy: 89.86666870117188, Test Accuracy: 90.58116149902344\n",
      "Epoch 60, Loss: 0.4144195318222046, Accuracy: 90.22222137451172, Test Accuracy: 90.98196411132812\n",
      "Epoch 61, Loss: 0.4044678509235382, Accuracy: 90.66667175292969, Test Accuracy: 91.98397064208984\n",
      "Epoch 62, Loss: 0.39470526576042175, Accuracy: 91.15555572509766, Test Accuracy: 93.18637084960938\n",
      "Epoch 63, Loss: 0.38515904545783997, Accuracy: 91.62222290039062, Test Accuracy: 93.18637084960938\n",
      "Epoch 64, Loss: 0.37580591440200806, Accuracy: 92.15555572509766, Test Accuracy: 93.38677215576172\n",
      "Epoch 65, Loss: 0.36657091975212097, Accuracy: 92.64444732666016, Test Accuracy: 93.18637084960938\n",
      "Epoch 66, Loss: 0.35753151774406433, Accuracy: 93.04444122314453, Test Accuracy: 93.38677215576172\n",
      "Epoch 67, Loss: 0.3486618995666504, Accuracy: 93.5111083984375, Test Accuracy: 93.18637084960938\n",
      "Epoch 68, Loss: 0.33995321393013, Accuracy: 93.77777862548828, Test Accuracy: 93.7875747680664\n",
      "Epoch 69, Loss: 0.3313229978084564, Accuracy: 94.24444580078125, Test Accuracy: 93.7875747680664\n",
      "Epoch 70, Loss: 0.32283031940460205, Accuracy: 94.62222290039062, Test Accuracy: 93.7875747680664\n",
      "Epoch 71, Loss: 0.3143137991428375, Accuracy: 95.04444885253906, Test Accuracy: 93.7875747680664\n",
      "Epoch 72, Loss: 0.3051699995994568, Accuracy: 95.24444580078125, Test Accuracy: 94.1883773803711\n",
      "Epoch 73, Loss: 0.29597026109695435, Accuracy: 95.62222290039062, Test Accuracy: 94.7895736694336\n",
      "Epoch 74, Loss: 0.2876223623752594, Accuracy: 95.88888549804688, Test Accuracy: 94.98998260498047\n",
      "Epoch 75, Loss: 0.2795073390007019, Accuracy: 96.35556030273438, Test Accuracy: 95.39077758789062\n",
      "Epoch 76, Loss: 0.2715710997581482, Accuracy: 96.64443969726562, Test Accuracy: 95.39077758789062\n",
      "Epoch 77, Loss: 0.2638469338417053, Accuracy: 96.93333435058594, Test Accuracy: 95.19038391113281\n",
      "Epoch 78, Loss: 0.25631195306777954, Accuracy: 97.11111450195312, Test Accuracy: 95.19038391113281\n",
      "Epoch 79, Loss: 0.24896760284900665, Accuracy: 97.44444274902344, Test Accuracy: 95.19038391113281\n",
      "Epoch 80, Loss: 0.2417973130941391, Accuracy: 97.68888854980469, Test Accuracy: 95.59117889404297\n",
      "Epoch 81, Loss: 0.23483717441558838, Accuracy: 97.84444427490234, Test Accuracy: 95.59117889404297\n",
      "Epoch 82, Loss: 0.2280522882938385, Accuracy: 98.11111450195312, Test Accuracy: 95.79158782958984\n",
      "Epoch 83, Loss: 0.22145725786685944, Accuracy: 98.37777709960938, Test Accuracy: 95.79158782958984\n",
      "Epoch 84, Loss: 0.21504291892051697, Accuracy: 98.66667175292969, Test Accuracy: 95.99198150634766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85, Loss: 0.20882071554660797, Accuracy: 98.77777862548828, Test Accuracy: 95.99198150634766\n",
      "Epoch 86, Loss: 0.20275646448135376, Accuracy: 98.84444427490234, Test Accuracy: 96.1923828125\n",
      "Epoch 87, Loss: 0.19685983657836914, Accuracy: 99.0, Test Accuracy: 96.1923828125\n",
      "Epoch 88, Loss: 0.19113804399967194, Accuracy: 99.13333129882812, Test Accuracy: 96.39278411865234\n",
      "Epoch 89, Loss: 0.18559902906417847, Accuracy: 99.28888702392578, Test Accuracy: 96.79358673095703\n",
      "Epoch 90, Loss: 0.18019333481788635, Accuracy: 99.33333587646484, Test Accuracy: 97.39479064941406\n",
      "Epoch 91, Loss: 0.17493553459644318, Accuracy: 99.4000015258789, Test Accuracy: 97.79559326171875\n",
      "Epoch 92, Loss: 0.16979728639125824, Accuracy: 99.46666717529297, Test Accuracy: 97.79559326171875\n",
      "Epoch 93, Loss: 0.16480779647827148, Accuracy: 99.55555725097656, Test Accuracy: 97.79559326171875\n",
      "Epoch 94, Loss: 0.15995009243488312, Accuracy: 99.5777816772461, Test Accuracy: 97.79559326171875\n",
      "Epoch 95, Loss: 0.15522406995296478, Accuracy: 99.5999984741211, Test Accuracy: 97.79559326171875\n",
      "Epoch 96, Loss: 0.15062303841114044, Accuracy: 99.62222290039062, Test Accuracy: 97.79559326171875\n",
      "Epoch 97, Loss: 0.1461431086063385, Accuracy: 99.62222290039062, Test Accuracy: 97.99598693847656\n",
      "Epoch 98, Loss: 0.14177066087722778, Accuracy: 99.68888854980469, Test Accuracy: 97.99598693847656\n",
      "Epoch 99, Loss: 0.13748912513256073, Accuracy: 99.71110534667969, Test Accuracy: 97.99598693847656\n",
      "Epoch 100, Loss: 0.13323722779750824, Accuracy: 99.73333740234375, Test Accuracy: 97.99598693847656\n",
      "Epoch 101, Loss: 0.1291135847568512, Accuracy: 99.80000305175781, Test Accuracy: 97.99598693847656\n",
      "Epoch 102, Loss: 0.12512755393981934, Accuracy: 99.86666107177734, Test Accuracy: 97.79559326171875\n",
      "Epoch 103, Loss: 0.12121688574552536, Accuracy: 99.86666107177734, Test Accuracy: 97.99598693847656\n",
      "Epoch 104, Loss: 0.11740196496248245, Accuracy: 99.86666107177734, Test Accuracy: 98.19639587402344\n",
      "Epoch 105, Loss: 0.11372499912977219, Accuracy: 99.9111099243164, Test Accuracy: 98.39679718017578\n",
      "Epoch 106, Loss: 0.11012192070484161, Accuracy: 99.9111099243164, Test Accuracy: 98.39679718017578\n",
      "Epoch 107, Loss: 0.1066027507185936, Accuracy: 99.9111099243164, Test Accuracy: 98.39679718017578\n",
      "Epoch 108, Loss: 0.1031927615404129, Accuracy: 99.977783203125, Test Accuracy: 98.39679718017578\n",
      "Epoch 109, Loss: 0.09990755468606949, Accuracy: 99.977783203125, Test Accuracy: 98.39679718017578\n",
      "Epoch 110, Loss: 0.09670533239841461, Accuracy: 99.977783203125, Test Accuracy: 98.39679718017578\n",
      "Epoch 111, Loss: 0.09359326213598251, Accuracy: 99.977783203125, Test Accuracy: 98.39679718017578\n",
      "Epoch 112, Loss: 0.0905233770608902, Accuracy: 99.977783203125, Test Accuracy: 98.5971908569336\n",
      "Epoch 113, Loss: 0.08749408274888992, Accuracy: 99.977783203125, Test Accuracy: 98.39679718017578\n",
      "Epoch 114, Loss: 0.08455482870340347, Accuracy: 99.977783203125, Test Accuracy: 98.39679718017578\n",
      "Epoch 115, Loss: 0.08171955496072769, Accuracy: 99.977783203125, Test Accuracy: 98.39679718017578\n",
      "Epoch 116, Loss: 0.0790020152926445, Accuracy: 100.0, Test Accuracy: 98.39679718017578\n",
      "Epoch 117, Loss: 0.07638933509588242, Accuracy: 100.0, Test Accuracy: 98.5971908569336\n",
      "Epoch 118, Loss: 0.07386443763971329, Accuracy: 100.0, Test Accuracy: 98.5971908569336\n",
      "Epoch 119, Loss: 0.07143361121416092, Accuracy: 100.0, Test Accuracy: 98.5971908569336\n",
      "Epoch 120, Loss: 0.06907128542661667, Accuracy: 100.0, Test Accuracy: 98.5971908569336\n",
      "Epoch 121, Loss: 0.06678216904401779, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 122, Loss: 0.06458292156457901, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 123, Loss: 0.06245844438672066, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 124, Loss: 0.06040484458208084, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 125, Loss: 0.05842982605099678, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 126, Loss: 0.05652471259236336, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 127, Loss: 0.05466607213020325, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 128, Loss: 0.05287998542189598, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 129, Loss: 0.051163043826818466, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 130, Loss: 0.04951240122318268, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 131, Loss: 0.04791702330112457, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 132, Loss: 0.04637741670012474, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 133, Loss: 0.044897451996803284, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 134, Loss: 0.04347449541091919, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 135, Loss: 0.042096711695194244, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 136, Loss: 0.0407676063477993, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 137, Loss: 0.03948492184281349, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 138, Loss: 0.03824658691883087, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 139, Loss: 0.0370449423789978, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 140, Loss: 0.03589480370283127, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 141, Loss: 0.034782346338033676, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 142, Loss: 0.03371577337384224, Accuracy: 100.0, Test Accuracy: 98.79759216308594\n",
      "Epoch 143, Loss: 0.03268367052078247, Accuracy: 100.0, Test Accuracy: 98.99800109863281\n",
      "Epoch 144, Loss: 0.031685661524534225, Accuracy: 100.0, Test Accuracy: 98.99800109863281\n",
      "Epoch 145, Loss: 0.030715536326169968, Accuracy: 100.0, Test Accuracy: 98.99800109863281\n",
      "Epoch 146, Loss: 0.029781483113765717, Accuracy: 100.0, Test Accuracy: 98.99800109863281\n",
      "Epoch 147, Loss: 0.02887948416173458, Accuracy: 100.0, Test Accuracy: 98.99800109863281\n",
      "Epoch 148, Loss: 0.028008373454213142, Accuracy: 100.0, Test Accuracy: 98.99800109863281\n",
      "Epoch 149, Loss: 0.02716807834804058, Accuracy: 100.0, Test Accuracy: 98.99800109863281\n",
      "Epoch 150, Loss: 0.026359548792243004, Accuracy: 100.0, Test Accuracy: 98.99800109863281\n",
      "Epoch 151, Loss: 0.025571996346116066, Accuracy: 100.0, Test Accuracy: 98.99800109863281\n",
      "Epoch 152, Loss: 0.024820903316140175, Accuracy: 100.0, Test Accuracy: 98.99800109863281\n",
      "Epoch 153, Loss: 0.02409362979233265, Accuracy: 100.0, Test Accuracy: 98.99800109863281\n",
      "Epoch 154, Loss: 0.023392686620354652, Accuracy: 100.0, Test Accuracy: 98.99800109863281\n",
      "Epoch 155, Loss: 0.022717276588082314, Accuracy: 100.0, Test Accuracy: 98.99800109863281\n",
      "Epoch 156, Loss: 0.02206597477197647, Accuracy: 100.0, Test Accuracy: 98.99800109863281\n",
      "Epoch 157, Loss: 0.02143094129860401, Accuracy: 100.0, Test Accuracy: 98.99800109863281\n",
      "Epoch 158, Loss: 0.02080715075135231, Accuracy: 100.0, Test Accuracy: 98.99800109863281\n",
      "Epoch 159, Loss: 0.020232759416103363, Accuracy: 100.0, Test Accuracy: 98.99800109863281\n",
      "Epoch 160, Loss: 0.0196368545293808, Accuracy: 100.0, Test Accuracy: 98.99800109863281\n",
      "Epoch 161, Loss: 0.01908862590789795, Accuracy: 100.0, Test Accuracy: 98.99800109863281\n",
      "Epoch 162, Loss: 0.0185482706874609, Accuracy: 100.0, Test Accuracy: 99.19839477539062\n",
      "Epoch 163, Loss: 0.01802540011703968, Accuracy: 100.0, Test Accuracy: 99.19839477539062\n",
      "Epoch 164, Loss: 0.01751897484064102, Accuracy: 100.0, Test Accuracy: 99.19839477539062\n",
      "Epoch 165, Loss: 0.01702614687383175, Accuracy: 100.0, Test Accuracy: 99.19839477539062\n",
      "Epoch 166, Loss: 0.016542872413992882, Accuracy: 100.0, Test Accuracy: 99.19839477539062\n",
      "Epoch 167, Loss: 0.0160902701318264, Accuracy: 100.0, Test Accuracy: 99.19839477539062\n",
      "Epoch 168, Loss: 0.015641456469893456, Accuracy: 100.0, Test Accuracy: 99.19839477539062\n",
      "Epoch 169, Loss: 0.015216149389743805, Accuracy: 100.0, Test Accuracy: 99.39879608154297\n",
      "Epoch 170, Loss: 0.014792479574680328, Accuracy: 100.0, Test Accuracy: 99.39879608154297\n",
      "Epoch 171, Loss: 0.014387269504368305, Accuracy: 100.0, Test Accuracy: 99.39879608154297\n",
      "Epoch 172, Loss: 0.013994966633617878, Accuracy: 100.0, Test Accuracy: 99.39879608154297\n",
      "Epoch 173, Loss: 0.013618756085634232, Accuracy: 100.0, Test Accuracy: 99.39879608154297\n",
      "Epoch 174, Loss: 0.013251935131847858, Accuracy: 100.0, Test Accuracy: 99.39879608154297\n",
      "Epoch 175, Loss: 0.012893611565232277, Accuracy: 100.0, Test Accuracy: 99.39879608154297\n",
      "Epoch 176, Loss: 0.0125502934679389, Accuracy: 100.0, Test Accuracy: 99.39879608154297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177, Loss: 0.012216067872941494, Accuracy: 100.0, Test Accuracy: 99.39879608154297\n",
      "Epoch 178, Loss: 0.011892624199390411, Accuracy: 100.0, Test Accuracy: 99.39879608154297\n",
      "Epoch 179, Loss: 0.011580039747059345, Accuracy: 100.0, Test Accuracy: 99.39879608154297\n",
      "Epoch 180, Loss: 0.011271899566054344, Accuracy: 100.0, Test Accuracy: 99.39879608154297\n",
      "Epoch 181, Loss: 0.010966322384774685, Accuracy: 100.0, Test Accuracy: 99.39879608154297\n",
      "Epoch 182, Loss: 0.010680991224944592, Accuracy: 100.0, Test Accuracy: 99.39879608154297\n",
      "Epoch 183, Loss: 0.01040224265307188, Accuracy: 100.0, Test Accuracy: 99.39879608154297\n",
      "Epoch 184, Loss: 0.010135459713637829, Accuracy: 100.0, Test Accuracy: 99.39879608154297\n",
      "Epoch 185, Loss: 0.009873486123979092, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 186, Loss: 0.0096201803535223, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 187, Loss: 0.009374545887112617, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 188, Loss: 0.009140768088400364, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 189, Loss: 0.00891125574707985, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 190, Loss: 0.008691772818565369, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 191, Loss: 0.008476341143250465, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 192, Loss: 0.008266269229352474, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 193, Loss: 0.008064178749918938, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 194, Loss: 0.00786872860044241, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 195, Loss: 0.007678158115595579, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 196, Loss: 0.0074936081655323505, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 197, Loss: 0.007313669193536043, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 198, Loss: 0.007140342146158218, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 199, Loss: 0.006971647497266531, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 200, Loss: 0.0068092672154307365, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 201, Loss: 0.006650845054537058, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 202, Loss: 0.006498260889202356, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 203, Loss: 0.006347935181111097, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 204, Loss: 0.006201541516929865, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 205, Loss: 0.0060586449690163136, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 206, Loss: 0.005919073708355427, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 207, Loss: 0.005785385612398386, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 208, Loss: 0.00565643236041069, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 209, Loss: 0.005531197879463434, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 210, Loss: 0.005416797939687967, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 211, Loss: 0.005294867791235447, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 212, Loss: 0.0051760016940534115, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 213, Loss: 0.005061513278633356, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 214, Loss: 0.004950657021254301, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 215, Loss: 0.004841763060539961, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 216, Loss: 0.004737214185297489, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 217, Loss: 0.004635213874280453, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 218, Loss: 0.004534673411399126, Accuracy: 100.0, Test Accuracy: 99.59919738769531\n",
      "Epoch 219, Loss: 0.004437759984284639, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 220, Loss: 0.004343866370618343, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 221, Loss: 0.004250840283930302, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 222, Loss: 0.004162624478340149, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 223, Loss: 0.004075697157531977, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 224, Loss: 0.003990205470472574, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 225, Loss: 0.003908693324774504, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 226, Loss: 0.0038274831604212523, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 227, Loss: 0.0037493668496608734, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 228, Loss: 0.0036701986100524664, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 229, Loss: 0.003598535666242242, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 230, Loss: 0.0035256431438028812, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 231, Loss: 0.003454878693446517, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 232, Loss: 0.003385089337825775, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 233, Loss: 0.003321644151583314, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 234, Loss: 0.003255335381254554, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 235, Loss: 0.003189277369529009, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 236, Loss: 0.00312489434145391, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 237, Loss: 0.0030645723454654217, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 238, Loss: 0.003005031030625105, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 239, Loss: 0.002944779582321644, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 240, Loss: 0.0028877612203359604, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 241, Loss: 0.0028313167858868837, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 242, Loss: 0.002777360612526536, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 243, Loss: 0.0027219774201512337, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 244, Loss: 0.002672511152923107, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 245, Loss: 0.002619650913402438, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 246, Loss: 0.0025716654490679502, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 247, Loss: 0.0025224920827895403, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 248, Loss: 0.002474714070558548, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 249, Loss: 0.0024262480437755585, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 250, Loss: 0.0023824800737202168, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 251, Loss: 0.002336489036679268, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 252, Loss: 0.0022939303889870644, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 253, Loss: 0.0022493028081953526, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 254, Loss: 0.0022102370858192444, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 255, Loss: 0.0021689243149012327, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 256, Loss: 0.0021298041101545095, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 257, Loss: 0.002088815439492464, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 258, Loss: 0.0020547902677208185, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 259, Loss: 0.0020160553976893425, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 260, Loss: 0.001979239284992218, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 261, Loss: 0.001944765797816217, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 262, Loss: 0.001909991493448615, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 263, Loss: 0.0018757309298962355, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 264, Loss: 0.0018417329993098974, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 265, Loss: 0.0018093119142577052, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 266, Loss: 0.001776767079718411, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 267, Loss: 0.0017446049023419619, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 268, Loss: 0.0017157885013148189, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 269, Loss: 0.0016859687166288495, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 270, Loss: 0.001655637752264738, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 271, Loss: 0.0016274622175842524, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 272, Loss: 0.0015973899280652404, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 273, Loss: 0.0015717752976343036, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 274, Loss: 0.0015441224677488208, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 275, Loss: 0.0015182890929281712, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 276, Loss: 0.0014915192732587457, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 277, Loss: 0.0014665893977507949, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 278, Loss: 0.0014414630131796002, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 279, Loss: 0.001417367486283183, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 280, Loss: 0.001392598613165319, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 281, Loss: 0.0013697545509785414, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 282, Loss: 0.001346335862763226, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 283, Loss: 0.0013224277645349503, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 284, Loss: 0.0013021397171542048, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 285, Loss: 0.001279899151995778, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 286, Loss: 0.0012592198327183723, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 287, Loss: 0.001238016877323389, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 288, Loss: 0.0012178056640550494, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 289, Loss: 0.0011971652274951339, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 290, Loss: 0.0011779541382566094, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 291, Loss: 0.0011583507293835282, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 292, Loss: 0.0011394304456189275, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 293, Loss: 0.0011208336800336838, Accuracy: 100.0, Test Accuracy: 99.79959869384766\n",
      "Epoch 294, Loss: 0.0011015761410817504, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 295, Loss: 0.001082821749150753, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 296, Loss: 0.0010655131191015244, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 297, Loss: 0.001046497724018991, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 298, Loss: 0.0010306664044037461, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 299, Loss: 0.0010140999220311642, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 300, Loss: 0.0009974493877962232, Accuracy: 100.0, Test Accuracy: 100.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23137be0ef0>]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hybrid model L_S \n",
    "\n",
    "# P 05\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.activations import relu\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LSLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,  num_outputs_s, num_outputs_l, activation=sigmoid, wstd = 0.3, bstd = 0.5):\n",
    "        super(LSLayer, self).__init__()\n",
    "        self.num_outputs_l = num_outputs_l\n",
    "        self.num_outputs_s = num_outputs_s\n",
    "        self.num_outputs = num_outputs_l + num_outputs_s\n",
    "        self.activation = activation\n",
    "        self.wstd = wstd\n",
    "        self.bstd = bstd\n",
    "\n",
    "        \n",
    "    def build(self, input_shape):  \n",
    "        self.kernel = self.add_weight(\"kernel\",\n",
    "                                      shape=(int(input_shape[-1]),\n",
    "                                             self.num_outputs), \n",
    "                                      initializer=tf.keras.initializers.RandomNormal(stddev=self.wstd),\n",
    "                                     trainable=True)\n",
    "        \n",
    "        self.bias = self.add_weight(\"bias\",\n",
    "                                      shape=[self.num_outputs],\n",
    "                                    initializer=tf.keras.initializers.RandomNormal(stddev=self.bstd),\n",
    "                                   trainable=True)\n",
    "\n",
    "    \n",
    "    # F2 method LS layer\n",
    "    def call(self, input):\n",
    "        \n",
    "        isp = input.shape\n",
    "        In1 = tf.transpose(input)\n",
    "        kernel_S, kernel_L  = tf.split(self.kernel,[ self.num_outputs_s, self.num_outputs_l ], axis = 1 )\n",
    "        bias_S, bias_L  = tf.split(self.bias,[ self.num_outputs_s, self.num_outputs_l ], axis = 0 )\n",
    "        \n",
    "        # case spherical\n",
    "        \n",
    "        s_shape  = self.num_outputs_s\n",
    "        In2 = tf.stack([In1] * s_shape)\n",
    "        InD = tf.transpose(In2)\n",
    "        WD = tf.stack([kernel_S] * isp[0])\n",
    "        ddd = WD - InD\n",
    "        dd0 = tf.math.multiply(ddd, ddd)\n",
    "        dd1 = tf.math.reduce_sum(dd0, axis =1)\n",
    "        dd2 = tf.cast(dd1,tf.double)\n",
    "        dd3 = tf.sqrt(dd2)\n",
    "        d_r = tf.cast(dd3,tf.float32)\n",
    "        d_R = tf.abs(bias_S)\n",
    "        d_rR = tf.math.divide_no_nan(d_r,d_R)\n",
    "        d_x0 = tf.ones(d_rR.shape) - d_rR\n",
    "        result_S = tf.math.scalar_mul(6,d_x0)\n",
    "        result_S = sigmoid(result_S)\n",
    "        \n",
    "        # case linear\n",
    "        \n",
    "        d_1 = tf.stack([bias_L] * isp[0])\n",
    "        result_L = tf.matmul(input, kernel_L) + d_1 \n",
    "        result_L = relu(result_L)\n",
    "        \n",
    "        # merge\n",
    "        \n",
    "        result = tf.concat([result_S, result_L],axis=1)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "\n",
    "class NN_Model(Model):\n",
    "    \n",
    "    def __init__(self,c,l,n,m,hs,hl):\n",
    "        self.C=c\n",
    "        self.L=l\n",
    "        self.N=n\n",
    "        self.M=m\n",
    "        self.HS = hs\n",
    "        self.HL = hl\n",
    "        super(NN_Model, self).__init__()\n",
    "        self.d1 = LSLayer(self.HS,self.HL)\n",
    "        self.d2 = Dense(self.C)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        #print (\"call benn:\",x, tf.math.reduce_sum(x))\n",
    "        return self.d2(x)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(datas, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(datas, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step(datas, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    \n",
    "    predictions = model(datas, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "\n",
    "C= 6\n",
    "L= 50\n",
    "N= 5000\n",
    "M= 6\n",
    "HS = 0\n",
    "HL = 35\n",
    "EPOCHS = 300\n",
    "\n",
    "# Create an instance of the model\n",
    "model = NN_Model(C,L,N,M,HS,HL)\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "#loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "(x_train,y_train,x_test,y_test) = gen_data_array(C, L, N, M)\n",
    "print (x_train[:2])\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).batch(32)\n",
    "#print (train_ds)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for epoch in range(EPOCHS):\n",
    "  # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    for datas, labels in train_ds:\n",
    "        train_step(datas, labels)\n",
    "        \n",
    "                \n",
    "    for test_datas, test_labels in test_ds:\n",
    "        #print (\"test_data_shape\", test_datas.shape)\n",
    "        predictions = model(test_datas, training=False)\n",
    "        #print (\"ttttttttttttttttttt\")\n",
    "        #for i in range(test_datas.shape[0]):\n",
    "        #    print (predictions.numpy()[i], test_labels.numpy()[i])\n",
    "        test_step(test_datas, test_labels)\n",
    "    \n",
    "    X.append(epoch)\n",
    "    Y.append(test_accuracy.result() * 100)\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "        f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "        f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "      )    \n",
    "\n",
    "plt.plot(X, Y,label=\"Accuracy curve\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.5 6.883958809354461\n"
     ]
    }
   ],
   "source": [
    "import statistics \n",
    "\n",
    "x = [92,75,77,74,87,75,68,81,76,80]\n",
    "print (sum(x)/len(x), statistics.stdev(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "def gen_cluster_data_list(Cv, Lv, Nv, Mv):\n",
    "    Tr = []\n",
    "    Ts = []\n",
    "    C = Cv  # number of categories\n",
    "    L = Lv   # number of centers\n",
    "    N = Nv  # number of elements\n",
    "    M = Mv  # number of dimensions\n",
    "    X, y = make_blobs(n_samples=N, centers=L, n_features=M,cluster_std=.5, random_state=11)\n",
    "    cmap = []\n",
    "    for _ in range(L):\n",
    "        cmap.append(random.randint(0,C-1))\n",
    "    cols = []\n",
    "    for i in range(N):\n",
    "        cols.append(cmap[y[i]])\n",
    "\n",
    "    for i in range(int(0.9*N)):\n",
    "        row = [X[i,j] for j in range(M)]\n",
    "        row.append(cols[i])\n",
    "        Tr.append(row)\n",
    "    \n",
    "    for i in range(int(0.9*N)+1,N):\n",
    "        row = [X[i,j] for j in range(M)]\n",
    "        row.append(cols[i])\n",
    "        Ts.append(row)\n",
    "        \n",
    "    return (Tr, Ts)\n",
    "\n",
    "def normalize (train):\n",
    "    mx = []\n",
    "    mn = []\n",
    "    for i in range(len(train[0])-1):\n",
    "        mx.append(max([x[i] for x in train ]))\n",
    "        mn.append(min([x[i] for x in train ]))\n",
    "    for row in train:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i] - mn[i]) / (mx[i] - mn[i]) \n",
    "    return train\n",
    "\n",
    "\n",
    "def gen_data_array(Cv, Lv, Nv, Mv):\n",
    "    C = Cv  # number of categories\n",
    "    L = Lv   # number of centers\n",
    "    N = Nv  # number of elements\n",
    "    M = Mv  # number of dimensions\n",
    "    (T1,T2)  = gen_cluster_data_list(C, L, N, M)\n",
    "\n",
    "    T = normalize(T1)\n",
    "    N = len(T)\n",
    "    x2_train = np.zeros((N,M),dtype='float32')\n",
    "    y2_train = np.zeros((N,C))\n",
    "    for i in range(N):\n",
    "        row = T[i]\n",
    "        for j in range(M):\n",
    "            x2_train[i,j] = row[j]\n",
    "        y2_train[i,row[-1]] = 1\n",
    "\n",
    "    Ts = normalize(T2)\n",
    "    Ns = len(Ts)\n",
    "    x2_test = np.zeros((Ns,M),dtype='float32')\n",
    "    y2_test = np.zeros((Ns,C))\n",
    "    for i in range(Ns):\n",
    "        row = Ts[i]\n",
    "        for j in range(M):\n",
    "            x2_test[i,j] = row[j]\n",
    "        y2_test[i, row[-1]] = 1\n",
    "        \n",
    "    return (x2_train,y2_train, x2_test, y2_test)\n",
    "\n",
    "def gen_data_array_s(Cv, Lv, Nv, Mv):\n",
    "    C = Cv  # number of categories\n",
    "    L = Lv   # number of centers\n",
    "    N = Nv  # number of elements\n",
    "    M = Mv  # number of dimensions\n",
    "    (T1,T2)  = gen_cluster_data_list(C, L, N, M)\n",
    "\n",
    "    T = normalize(T1)\n",
    "    N = len(T)\n",
    "    x2_train = np.zeros((N,M),dtype='float32')\n",
    "    y2_train = np.zeros((N,1))\n",
    "    for i in range(N):\n",
    "        row = T[i]\n",
    "        for j in range(M):\n",
    "            x2_train[i,j] = row[j]\n",
    "        y2_train[i] = row[-1]\n",
    "\n",
    "    Ts = normalize(T2)\n",
    "    Ns = len(Ts)\n",
    "    x2_test = np.zeros((Ns,M),dtype='float32')\n",
    "    y2_test = np.zeros((Ns,1))\n",
    "    for i in range(Ns):\n",
    "        row = Ts[i]\n",
    "        for j in range(M):\n",
    "            x2_test[i,j] = row[j]\n",
    "        y2_test[i] = row[-1]\n",
    "        \n",
    "    return (x2_train,y2_train, x2_test, y2_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
