{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "special-slovakia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.90179956 0.9417195  0.37377474 0.07043936 0.53236914 0.7381097 ]\n",
      " [0.5166569  0.40206912 0.8782092  0.26794338 0.8631228  0.28210485]]\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method LSLayer.call of <__main__.LSLayer object at 0x000001938C68BE20>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method LSLayer.call of <__main__.LSLayer object at 0x000001938C68BE20>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1, Loss: 1.772323489189148, Accuracy: 24.77777671813965, Test Accuracy: 30.861722946166992\n",
      "Epoch 21, Loss: 1.2829891443252563, Accuracy: 48.599998474121094, Test Accuracy: 46.492984771728516\n",
      "Epoch 41, Loss: 1.1647391319274902, Accuracy: 54.77777862548828, Test Accuracy: 55.91181945800781\n",
      "Epoch 61, Loss: 1.0767983198165894, Accuracy: 59.11111068725586, Test Accuracy: 57.51502990722656\n",
      "Epoch 81, Loss: 1.0096687078475952, Accuracy: 61.19999694824219, Test Accuracy: 59.91984176635742\n",
      "Epoch 101, Loss: 0.9644312262535095, Accuracy: 62.28888702392578, Test Accuracy: 62.124244689941406\n",
      "Epoch 121, Loss: 0.9321534633636475, Accuracy: 62.599998474121094, Test Accuracy: 63.927852630615234\n",
      "Epoch 141, Loss: 0.9076644778251648, Accuracy: 63.20000457763672, Test Accuracy: 65.13025665283203\n",
      "Epoch 161, Loss: 0.8880993723869324, Accuracy: 63.80000305175781, Test Accuracy: 66.53306579589844\n",
      "Epoch 181, Loss: 0.8716264367103577, Accuracy: 64.44444274902344, Test Accuracy: 66.93386840820312\n",
      "Epoch 201, Loss: 0.8568849563598633, Accuracy: 65.46666717529297, Test Accuracy: 69.33867645263672\n",
      "Epoch 221, Loss: 0.8429645299911499, Accuracy: 66.5777816772461, Test Accuracy: 70.54108428955078\n",
      "Epoch 241, Loss: 0.82977694272995, Accuracy: 67.13333129882812, Test Accuracy: 69.7394790649414\n",
      "Epoch 261, Loss: 0.817657470703125, Accuracy: 67.44444274902344, Test Accuracy: 69.53907775878906\n",
      "Epoch 281, Loss: 0.8066970109939575, Accuracy: 68.1111068725586, Test Accuracy: 69.7394790649414\n",
      "Epoch 301, Loss: 0.7967891693115234, Accuracy: 69.19999694824219, Test Accuracy: 70.7414779663086\n",
      "Epoch 321, Loss: 0.7877896428108215, Accuracy: 70.53333282470703, Test Accuracy: 72.144287109375\n",
      "Epoch 341, Loss: 0.7795515060424805, Accuracy: 71.71111297607422, Test Accuracy: 71.54308319091797\n",
      "Epoch 361, Loss: 0.7719102501869202, Accuracy: 72.02222442626953, Test Accuracy: 71.54308319091797\n",
      "Epoch 381, Loss: 0.7646748423576355, Accuracy: 72.35556030273438, Test Accuracy: 71.14228820800781\n",
      "Epoch 401, Loss: 0.7576215863227844, Accuracy: 72.93333435058594, Test Accuracy: 70.1402816772461\n",
      "Epoch 421, Loss: 0.7504656910896301, Accuracy: 73.4000015258789, Test Accuracy: 70.1402816772461\n",
      "Epoch 441, Loss: 0.7427768707275391, Accuracy: 73.9111099243164, Test Accuracy: 69.33867645263672\n",
      "Epoch 461, Loss: 0.7339569330215454, Accuracy: 74.28888702392578, Test Accuracy: 68.73748016357422\n",
      "Epoch 481, Loss: 0.7237751483917236, Accuracy: 74.02222442626953, Test Accuracy: 69.13827514648438\n",
      "Epoch 501, Loss: 0.7130910158157349, Accuracy: 74.75555419921875, Test Accuracy: 70.54108428955078\n",
      "Epoch 521, Loss: 0.7029794454574585, Accuracy: 75.97777557373047, Test Accuracy: 70.54108428955078\n",
      "Epoch 541, Loss: 0.6938115358352661, Accuracy: 76.57777404785156, Test Accuracy: 70.94188690185547\n",
      "Epoch 561, Loss: 0.6854807138442993, Accuracy: 76.80000305175781, Test Accuracy: 71.94388580322266\n",
      "Epoch 581, Loss: 0.6777424216270447, Accuracy: 76.66666412353516, Test Accuracy: 72.144287109375\n",
      "Epoch 601, Loss: 0.6703542470932007, Accuracy: 76.5999984741211, Test Accuracy: 72.34468841552734\n",
      "Epoch 621, Loss: 0.6630977392196655, Accuracy: 76.75555419921875, Test Accuracy: 72.94589233398438\n",
      "Epoch 641, Loss: 0.6557644605636597, Accuracy: 77.13333129882812, Test Accuracy: 72.74549102783203\n",
      "Epoch 661, Loss: 0.64814293384552, Accuracy: 77.35556030273438, Test Accuracy: 72.94589233398438\n",
      "Epoch 681, Loss: 0.6400642991065979, Accuracy: 77.55555725097656, Test Accuracy: 72.34468841552734\n",
      "Epoch 701, Loss: 0.631628692150116, Accuracy: 77.84444427490234, Test Accuracy: 71.14228820800781\n",
      "Epoch 721, Loss: 0.6236062049865723, Accuracy: 78.31111145019531, Test Accuracy: 71.14228820800781\n",
      "Epoch 741, Loss: 0.6167111396789551, Accuracy: 78.73332977294922, Test Accuracy: 70.54108428955078\n",
      "Epoch 761, Loss: 0.6106074452400208, Accuracy: 79.11111450195312, Test Accuracy: 69.7394790649414\n",
      "Epoch 781, Loss: 0.6049473881721497, Accuracy: 79.37777709960938, Test Accuracy: 69.13827514648438\n",
      "Epoch 801, Loss: 0.5996263027191162, Accuracy: 79.68888854980469, Test Accuracy: 68.73748016357422\n",
      "Epoch 821, Loss: 0.5945965647697449, Accuracy: 79.82222747802734, Test Accuracy: 68.73748016357422\n",
      "Epoch 841, Loss: 0.5898270010948181, Accuracy: 80.28888702392578, Test Accuracy: 68.13627624511719\n",
      "Epoch 861, Loss: 0.5852929353713989, Accuracy: 80.5777816772461, Test Accuracy: 67.53507232666016\n",
      "Epoch 881, Loss: 0.5809745788574219, Accuracy: 80.64444732666016, Test Accuracy: 67.33467102050781\n",
      "Epoch 901, Loss: 0.5768547058105469, Accuracy: 80.66666412353516, Test Accuracy: 67.13426971435547\n",
      "Epoch 921, Loss: 0.5729191899299622, Accuracy: 80.73333740234375, Test Accuracy: 67.33467102050781\n",
      "Epoch 941, Loss: 0.569153904914856, Accuracy: 80.73333740234375, Test Accuracy: 67.33467102050781\n",
      "Epoch 961, Loss: 0.5655476450920105, Accuracy: 80.977783203125, Test Accuracy: 67.33467102050781\n",
      "Epoch 981, Loss: 0.5620890855789185, Accuracy: 80.8888931274414, Test Accuracy: 67.93587493896484\n",
      "Epoch 1001, Loss: 0.5587676763534546, Accuracy: 81.04444885253906, Test Accuracy: 68.13627624511719\n",
      "Epoch 1021, Loss: 0.5555746555328369, Accuracy: 81.13333892822266, Test Accuracy: 68.13627624511719\n",
      "Epoch 1041, Loss: 0.5525010228157043, Accuracy: 81.19999694824219, Test Accuracy: 68.13627624511719\n",
      "Epoch 1061, Loss: 0.5495389699935913, Accuracy: 81.28888702392578, Test Accuracy: 68.13627624511719\n",
      "Epoch 1081, Loss: 0.5466810464859009, Accuracy: 81.44444274902344, Test Accuracy: 68.13627624511719\n",
      "Epoch 1101, Loss: 0.5439206957817078, Accuracy: 81.44444274902344, Test Accuracy: 68.13627624511719\n",
      "Epoch 1121, Loss: 0.5412516593933105, Accuracy: 81.4888916015625, Test Accuracy: 68.13627624511719\n",
      "Epoch 1141, Loss: 0.5386677980422974, Accuracy: 81.53333282470703, Test Accuracy: 68.336669921875\n",
      "Epoch 1161, Loss: 0.5361642241477966, Accuracy: 81.57777404785156, Test Accuracy: 68.73748016357422\n",
      "Epoch 1181, Loss: 0.5337362885475159, Accuracy: 81.53333282470703, Test Accuracy: 68.73748016357422\n",
      "Epoch 1201, Loss: 0.5313785672187805, Accuracy: 81.5999984741211, Test Accuracy: 68.93787384033203\n",
      "Epoch 1221, Loss: 0.5290877819061279, Accuracy: 81.64444732666016, Test Accuracy: 68.93787384033203\n",
      "Epoch 1241, Loss: 0.5268594622612, Accuracy: 81.75555419921875, Test Accuracy: 68.93787384033203\n",
      "Epoch 1261, Loss: 0.5246908068656921, Accuracy: 81.86666107177734, Test Accuracy: 68.73748016357422\n",
      "Epoch 1281, Loss: 0.5225779414176941, Accuracy: 81.95555877685547, Test Accuracy: 68.53707122802734\n",
      "Epoch 1301, Loss: 0.5205177664756775, Accuracy: 82.0, Test Accuracy: 68.73748016357422\n",
      "Epoch 1321, Loss: 0.5185081362724304, Accuracy: 82.0888900756836, Test Accuracy: 68.73748016357422\n",
      "Epoch 1341, Loss: 0.5165454745292664, Accuracy: 82.022216796875, Test Accuracy: 68.73748016357422\n",
      "Epoch 1361, Loss: 0.5146281719207764, Accuracy: 82.1111068725586, Test Accuracy: 68.73748016357422\n",
      "Epoch 1381, Loss: 0.5127538442611694, Accuracy: 82.26666259765625, Test Accuracy: 68.53707122802734\n",
      "Epoch 1401, Loss: 0.5109202265739441, Accuracy: 82.24444580078125, Test Accuracy: 68.53707122802734\n",
      "Epoch 1421, Loss: 0.5091256499290466, Accuracy: 82.26666259765625, Test Accuracy: 68.93787384033203\n",
      "Epoch 1441, Loss: 0.5073677897453308, Accuracy: 82.31111145019531, Test Accuracy: 68.93787384033203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1461, Loss: 0.5056449770927429, Accuracy: 82.37777709960938, Test Accuracy: 68.93787384033203\n",
      "Epoch 1481, Loss: 0.5039562582969666, Accuracy: 82.4222183227539, Test Accuracy: 68.73748016357422\n",
      "Epoch 1501, Loss: 0.5022994875907898, Accuracy: 82.4222183227539, Test Accuracy: 68.93787384033203\n",
      "Epoch 1521, Loss: 0.500673234462738, Accuracy: 82.5111083984375, Test Accuracy: 68.93787384033203\n",
      "Epoch 1541, Loss: 0.4990761876106262, Accuracy: 82.64444732666016, Test Accuracy: 69.33867645263672\n",
      "Epoch 1561, Loss: 0.49750757217407227, Accuracy: 82.80000305175781, Test Accuracy: 69.33867645263672\n",
      "Epoch 1581, Loss: 0.4959656596183777, Accuracy: 82.86666870117188, Test Accuracy: 69.13827514648438\n",
      "Epoch 1601, Loss: 0.494449645280838, Accuracy: 82.88888549804688, Test Accuracy: 68.93787384033203\n",
      "Epoch 1621, Loss: 0.49295774102211, Accuracy: 82.97777557373047, Test Accuracy: 68.73748016357422\n",
      "Epoch 1641, Loss: 0.4914894998073578, Accuracy: 83.06666564941406, Test Accuracy: 68.73748016357422\n",
      "Epoch 1661, Loss: 0.49004364013671875, Accuracy: 83.24444580078125, Test Accuracy: 68.73748016357422\n",
      "Epoch 1681, Loss: 0.4886191487312317, Accuracy: 83.33332824707031, Test Accuracy: 68.73748016357422\n",
      "Epoch 1701, Loss: 0.4872148036956787, Accuracy: 83.4222183227539, Test Accuracy: 68.73748016357422\n",
      "Epoch 1721, Loss: 0.48582935333251953, Accuracy: 83.44444274902344, Test Accuracy: 68.73748016357422\n",
      "Epoch 1741, Loss: 0.48446258902549744, Accuracy: 83.44444274902344, Test Accuracy: 68.73748016357422\n",
      "Epoch 1761, Loss: 0.48311370611190796, Accuracy: 83.53333282470703, Test Accuracy: 68.73748016357422\n",
      "Epoch 1781, Loss: 0.4817810654640198, Accuracy: 83.57777404785156, Test Accuracy: 68.73748016357422\n",
      "Epoch 1801, Loss: 0.4804641008377075, Accuracy: 83.64444732666016, Test Accuracy: 68.73748016357422\n",
      "Epoch 1821, Loss: 0.4791620075702667, Accuracy: 83.68888854980469, Test Accuracy: 68.73748016357422\n",
      "Epoch 1841, Loss: 0.4778733551502228, Accuracy: 83.73332977294922, Test Accuracy: 68.73748016357422\n",
      "Epoch 1861, Loss: 0.47659751772880554, Accuracy: 83.82221984863281, Test Accuracy: 68.73748016357422\n",
      "Epoch 1881, Loss: 0.47533372044563293, Accuracy: 83.93333435058594, Test Accuracy: 68.73748016357422\n",
      "Epoch 1901, Loss: 0.4740810990333557, Accuracy: 83.93333435058594, Test Accuracy: 68.73748016357422\n",
      "Epoch 1921, Loss: 0.4728391468524933, Accuracy: 84.04444122314453, Test Accuracy: 68.73748016357422\n",
      "Epoch 1941, Loss: 0.4716067910194397, Accuracy: 84.13333129882812, Test Accuracy: 68.73748016357422\n",
      "Epoch 1961, Loss: 0.4703834354877472, Accuracy: 84.17778015136719, Test Accuracy: 68.53707122802734\n",
      "Epoch 1981, Loss: 0.4691676199436188, Accuracy: 84.26667022705078, Test Accuracy: 68.53707122802734\n",
      "Epoch 2001, Loss: 0.46795886754989624, Accuracy: 84.19999694824219, Test Accuracy: 68.336669921875\n",
      "Epoch 2021, Loss: 0.46675652265548706, Accuracy: 84.22222137451172, Test Accuracy: 68.53707122802734\n",
      "Epoch 2041, Loss: 0.4655599296092987, Accuracy: 84.26667022705078, Test Accuracy: 68.53707122802734\n",
      "Epoch 2061, Loss: 0.46436807513237, Accuracy: 84.28888702392578, Test Accuracy: 68.53707122802734\n",
      "Epoch 2081, Loss: 0.4631797969341278, Accuracy: 84.24444580078125, Test Accuracy: 68.73748016357422\n",
      "Epoch 2101, Loss: 0.461995005607605, Accuracy: 84.33332824707031, Test Accuracy: 68.73748016357422\n",
      "Epoch 2121, Loss: 0.46081238985061646, Accuracy: 84.4000015258789, Test Accuracy: 68.53707122802734\n",
      "Epoch 2141, Loss: 0.45963168144226074, Accuracy: 84.46666717529297, Test Accuracy: 68.93787384033203\n",
      "Epoch 2161, Loss: 0.45845192670822144, Accuracy: 84.48888397216797, Test Accuracy: 68.93787384033203\n",
      "Epoch 2181, Loss: 0.4572717249393463, Accuracy: 84.53333282470703, Test Accuracy: 68.73748016357422\n",
      "Epoch 2201, Loss: 0.45609062910079956, Accuracy: 84.57777404785156, Test Accuracy: 68.73748016357422\n",
      "Epoch 2221, Loss: 0.45490768551826477, Accuracy: 84.51111602783203, Test Accuracy: 68.73748016357422\n",
      "Epoch 2241, Loss: 0.4537222385406494, Accuracy: 84.5999984741211, Test Accuracy: 68.73748016357422\n",
      "Epoch 2261, Loss: 0.45253321528434753, Accuracy: 84.66667175292969, Test Accuracy: 68.73748016357422\n",
      "Epoch 2281, Loss: 0.45133984088897705, Accuracy: 84.71111297607422, Test Accuracy: 68.73748016357422\n",
      "Epoch 2301, Loss: 0.4501410722732544, Accuracy: 84.66667175292969, Test Accuracy: 68.93787384033203\n",
      "Epoch 2321, Loss: 0.448935866355896, Accuracy: 84.68888854980469, Test Accuracy: 68.93787384033203\n",
      "Epoch 2341, Loss: 0.4477233290672302, Accuracy: 84.77777862548828, Test Accuracy: 68.93787384033203\n",
      "Epoch 2361, Loss: 0.44650182127952576, Accuracy: 84.82221984863281, Test Accuracy: 68.93787384033203\n",
      "Epoch 2381, Loss: 0.4452703595161438, Accuracy: 84.79999542236328, Test Accuracy: 69.13827514648438\n",
      "Epoch 2401, Loss: 0.44402775168418884, Accuracy: 84.86666870117188, Test Accuracy: 69.13827514648438\n",
      "Epoch 2421, Loss: 0.44277292490005493, Accuracy: 84.84444427490234, Test Accuracy: 69.13827514648438\n",
      "Epoch 2441, Loss: 0.4415043294429779, Accuracy: 84.77777862548828, Test Accuracy: 69.33867645263672\n",
      "Epoch 2461, Loss: 0.4402202367782593, Accuracy: 84.82221984863281, Test Accuracy: 69.33867645263672\n",
      "Epoch 2481, Loss: 0.4389195740222931, Accuracy: 84.82221984863281, Test Accuracy: 69.33867645263672\n",
      "Epoch 2501, Loss: 0.4376004934310913, Accuracy: 84.88888549804688, Test Accuracy: 69.33867645263672\n",
      "Epoch 2521, Loss: 0.4362616539001465, Accuracy: 85.0, Test Accuracy: 69.33867645263672\n",
      "Epoch 2541, Loss: 0.4349008798599243, Accuracy: 84.95555877685547, Test Accuracy: 69.33867645263672\n",
      "Epoch 2561, Loss: 0.43351680040359497, Accuracy: 85.0, Test Accuracy: 69.33867645263672\n",
      "Epoch 2581, Loss: 0.4321073889732361, Accuracy: 84.97777557373047, Test Accuracy: 69.33867645263672\n",
      "Epoch 2601, Loss: 0.43067091703414917, Accuracy: 85.02222442626953, Test Accuracy: 69.33867645263672\n",
      "Epoch 2621, Loss: 0.42920544743537903, Accuracy: 85.11111450195312, Test Accuracy: 69.33867645263672\n",
      "Epoch 2641, Loss: 0.42770862579345703, Accuracy: 85.15555572509766, Test Accuracy: 69.33867645263672\n",
      "Epoch 2661, Loss: 0.42617911100387573, Accuracy: 85.19999694824219, Test Accuracy: 69.33867645263672\n",
      "Epoch 2681, Loss: 0.42461493611335754, Accuracy: 85.35555267333984, Test Accuracy: 69.53907775878906\n",
      "Epoch 2701, Loss: 0.42301374673843384, Accuracy: 85.33333587646484, Test Accuracy: 69.33867645263672\n",
      "Epoch 2721, Loss: 0.42137375473976135, Accuracy: 85.26667022705078, Test Accuracy: 69.33867645263672\n",
      "Epoch 2741, Loss: 0.4196932911872864, Accuracy: 85.24444580078125, Test Accuracy: 69.13827514648438\n",
      "Epoch 2761, Loss: 0.4179697632789612, Accuracy: 85.26667022705078, Test Accuracy: 68.93787384033203\n",
      "Epoch 2781, Loss: 0.4162016212940216, Accuracy: 85.35555267333984, Test Accuracy: 68.93787384033203\n",
      "Epoch 2801, Loss: 0.41438642144203186, Accuracy: 85.28888702392578, Test Accuracy: 68.73748016357422\n",
      "Epoch 2821, Loss: 0.41252121329307556, Accuracy: 85.4888916015625, Test Accuracy: 68.93787384033203\n",
      "Epoch 2841, Loss: 0.41060250997543335, Accuracy: 85.55555725097656, Test Accuracy: 69.13827514648438\n",
      "Epoch 2861, Loss: 0.40862607955932617, Accuracy: 85.71111297607422, Test Accuracy: 69.33867645263672\n",
      "Epoch 2881, Loss: 0.4065852761268616, Accuracy: 85.75555419921875, Test Accuracy: 69.53907775878906\n",
      "Epoch 2901, Loss: 0.4044727385044098, Accuracy: 85.95555114746094, Test Accuracy: 69.33867645263672\n",
      "Epoch 2921, Loss: 0.40227577090263367, Accuracy: 86.13333129882812, Test Accuracy: 69.53907775878906\n",
      "Epoch 2941, Loss: 0.39997828006744385, Accuracy: 86.28888702392578, Test Accuracy: 69.7394790649414\n",
      "Epoch 2961, Loss: 0.397558331489563, Accuracy: 86.53333282470703, Test Accuracy: 70.1402816772461\n",
      "Epoch 2981, Loss: 0.39498892426490784, Accuracy: 86.93333435058594, Test Accuracy: 71.34268188476562\n",
      "Epoch 3001, Loss: 0.3922439217567444, Accuracy: 87.28888702392578, Test Accuracy: 71.14228820800781\n",
      "Epoch 3021, Loss: 0.3893120288848877, Accuracy: 87.5111083984375, Test Accuracy: 72.54508972167969\n",
      "Epoch 3041, Loss: 0.38621988892555237, Accuracy: 88.06666564941406, Test Accuracy: 73.5470962524414\n",
      "Epoch 3061, Loss: 0.3830419182777405, Accuracy: 88.4222183227539, Test Accuracy: 74.34870147705078\n",
      "Epoch 3081, Loss: 0.3798738121986389, Accuracy: 89.0888900756836, Test Accuracy: 74.74949645996094\n",
      "Epoch 3101, Loss: 0.37679004669189453, Accuracy: 89.53333282470703, Test Accuracy: 75.15029907226562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3121, Loss: 0.37382689118385315, Accuracy: 89.95555877685547, Test Accuracy: 75.15029907226562\n",
      "Epoch 3141, Loss: 0.370993971824646, Accuracy: 90.26667022705078, Test Accuracy: 75.15029907226562\n",
      "Epoch 3161, Loss: 0.3682858943939209, Accuracy: 90.51111602783203, Test Accuracy: 75.35070037841797\n",
      "Epoch 3181, Loss: 0.36569344997406006, Accuracy: 90.66667175292969, Test Accuracy: 75.35070037841797\n",
      "Epoch 3201, Loss: 0.3632058799266815, Accuracy: 90.82221984863281, Test Accuracy: 75.75150299072266\n",
      "Epoch 3221, Loss: 0.3608120381832123, Accuracy: 90.95555877685547, Test Accuracy: 75.75150299072266\n",
      "Epoch 3241, Loss: 0.3585008680820465, Accuracy: 91.13333129882812, Test Accuracy: 76.15230560302734\n",
      "Epoch 3261, Loss: 0.3562600016593933, Accuracy: 91.17778015136719, Test Accuracy: 76.35270690917969\n",
      "Epoch 3281, Loss: 0.3540755808353424, Accuracy: 91.33333587646484, Test Accuracy: 76.35270690917969\n",
      "Epoch 3301, Loss: 0.351930171251297, Accuracy: 91.46666717529297, Test Accuracy: 76.35270690917969\n",
      "Epoch 3321, Loss: 0.3498021364212036, Accuracy: 91.44444274902344, Test Accuracy: 76.15230560302734\n",
      "Epoch 3341, Loss: 0.34766021370887756, Accuracy: 91.53333282470703, Test Accuracy: 76.35270690917969\n",
      "Epoch 3361, Loss: 0.3454570770263672, Accuracy: 91.64443969726562, Test Accuracy: 76.75350952148438\n",
      "Epoch 3381, Loss: 0.3431229293346405, Accuracy: 91.82222747802734, Test Accuracy: 76.95390319824219\n",
      "Epoch 3401, Loss: 0.34057602286338806, Accuracy: 91.84444427490234, Test Accuracy: 77.55510711669922\n",
      "Epoch 3421, Loss: 0.33780843019485474, Accuracy: 91.93333435058594, Test Accuracy: 78.3567123413086\n",
      "Epoch 3441, Loss: 0.33498644828796387, Accuracy: 92.06666564941406, Test Accuracy: 79.15831756591797\n",
      "Epoch 3461, Loss: 0.3322925269603729, Accuracy: 92.15555572509766, Test Accuracy: 79.55912017822266\n",
      "Epoch 3481, Loss: 0.329786479473114, Accuracy: 92.24444580078125, Test Accuracy: 79.759521484375\n",
      "Epoch 3501, Loss: 0.3274558186531067, Accuracy: 92.4888916015625, Test Accuracy: 80.36072540283203\n",
      "Epoch 3521, Loss: 0.3252716064453125, Accuracy: 92.5111083984375, Test Accuracy: 80.76152038574219\n",
      "Epoch 3541, Loss: 0.32320642471313477, Accuracy: 92.53333282470703, Test Accuracy: 80.96192169189453\n",
      "Epoch 3561, Loss: 0.32123786211013794, Accuracy: 92.62222290039062, Test Accuracy: 81.1623306274414\n",
      "Epoch 3581, Loss: 0.319349080324173, Accuracy: 92.62222290039062, Test Accuracy: 80.76152038574219\n",
      "Epoch 3601, Loss: 0.3175271451473236, Accuracy: 92.5777816772461, Test Accuracy: 80.96192169189453\n",
      "Epoch 3621, Loss: 0.3157622516155243, Accuracy: 92.62222290039062, Test Accuracy: 80.96192169189453\n",
      "Epoch 3641, Loss: 0.31404754519462585, Accuracy: 92.68888854980469, Test Accuracy: 81.1623306274414\n",
      "Epoch 3661, Loss: 0.31237754225730896, Accuracy: 92.64444732666016, Test Accuracy: 80.96192169189453\n",
      "Epoch 3681, Loss: 0.31074798107147217, Accuracy: 92.68888854980469, Test Accuracy: 81.1623306274414\n",
      "Epoch 3701, Loss: 0.3091556131839752, Accuracy: 92.79999542236328, Test Accuracy: 81.1623306274414\n",
      "Epoch 3721, Loss: 0.3075970411300659, Accuracy: 92.79999542236328, Test Accuracy: 80.96192169189453\n",
      "Epoch 3741, Loss: 0.306071013212204, Accuracy: 92.82222747802734, Test Accuracy: 80.96192169189453\n",
      "Epoch 3761, Loss: 0.3045748770236969, Accuracy: 92.977783203125, Test Accuracy: 80.96192169189453\n",
      "Epoch 3781, Loss: 0.30310750007629395, Accuracy: 93.04444122314453, Test Accuracy: 81.1623306274414\n",
      "Epoch 3801, Loss: 0.30166733264923096, Accuracy: 93.04444122314453, Test Accuracy: 81.1623306274414\n",
      "Epoch 3821, Loss: 0.30025291442871094, Accuracy: 93.06666564941406, Test Accuracy: 81.56312561035156\n",
      "Epoch 3841, Loss: 0.2988634407520294, Accuracy: 93.13333892822266, Test Accuracy: 81.7635269165039\n",
      "Epoch 3861, Loss: 0.2974974811077118, Accuracy: 93.22222137451172, Test Accuracy: 81.96392822265625\n",
      "Epoch 3881, Loss: 0.2961544394493103, Accuracy: 93.37777709960938, Test Accuracy: 81.7635269165039\n",
      "Epoch 3901, Loss: 0.2948336899280548, Accuracy: 93.35555267333984, Test Accuracy: 81.96392822265625\n",
      "Epoch 3921, Loss: 0.29353395104408264, Accuracy: 93.37777709960938, Test Accuracy: 81.96392822265625\n",
      "Epoch 3941, Loss: 0.292254775762558, Accuracy: 93.46666717529297, Test Accuracy: 81.96392822265625\n",
      "Epoch 3961, Loss: 0.2909950017929077, Accuracy: 93.4888916015625, Test Accuracy: 81.7635269165039\n",
      "Epoch 3981, Loss: 0.2897542417049408, Accuracy: 93.55555725097656, Test Accuracy: 81.96392822265625\n",
      "Epoch 4000, Loss: 0.28859269618988037, Accuracy: 93.5777816772461, Test Accuracy: 81.96392822265625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1938c603670>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hybrid model L_S \n",
    "\n",
    "# P 05\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.activations import relu\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LSLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,  num_outputs_s, num_outputs_l, activation=sigmoid, wstd = 0.3, bstd = 0.5):\n",
    "        super(LSLayer, self).__init__()\n",
    "        self.num_outputs_l = num_outputs_l\n",
    "        self.num_outputs_s = num_outputs_s\n",
    "        self.num_outputs = num_outputs_l + num_outputs_s\n",
    "        self.activation = activation\n",
    "        self.wstd = wstd\n",
    "        self.bstd = bstd\n",
    "\n",
    "        \n",
    "    def build(self, input_shape):  \n",
    "        self.kernel = self.add_weight(\"kernel\",\n",
    "                                      shape=(int(input_shape[-1]),\n",
    "                                             self.num_outputs), \n",
    "                                      initializer=tf.keras.initializers.RandomNormal(stddev=self.wstd),\n",
    "                                     trainable=True)\n",
    "        \n",
    "        self.bias = self.add_weight(\"bias\",\n",
    "                                      shape=[self.num_outputs],\n",
    "                                    initializer=tf.keras.initializers.RandomNormal(stddev=self.bstd),\n",
    "                                   trainable=True)\n",
    "\n",
    "    \n",
    "    # F2 method LS layer\n",
    "    def call(self, input):\n",
    "        \n",
    "        isp = input.shape\n",
    "        In1 = tf.transpose(input)\n",
    "        kernel_S, kernel_L  = tf.split(self.kernel,[ self.num_outputs_s, self.num_outputs_l ], axis = 1 )\n",
    "        bias_S, bias_L  = tf.split(self.bias,[ self.num_outputs_s, self.num_outputs_l ], axis = 0 )\n",
    "        \n",
    "        # case spherical\n",
    "        \n",
    "        s_shape  = self.num_outputs_s\n",
    "        In2 = tf.stack([In1] * s_shape)\n",
    "        InD = tf.transpose(In2)\n",
    "        WD = tf.stack([kernel_S] * isp[0])\n",
    "        ddd = WD - InD\n",
    "        dd0 = tf.math.multiply(ddd, ddd)\n",
    "        dd1 = tf.math.reduce_sum(dd0, axis =1)\n",
    "        dd2 = tf.cast(dd1,tf.double)\n",
    "        dd3 = tf.sqrt(dd2)\n",
    "        d_r = tf.cast(dd3,tf.float32)\n",
    "        d_R = tf.abs(bias_S)\n",
    "        d_rR = tf.math.divide_no_nan(d_r,d_R)\n",
    "        d_x0 = tf.ones(d_rR.shape) - d_rR\n",
    "        result_S = tf.math.scalar_mul(6,d_x0)\n",
    "        result_S = sigmoid(result_S)\n",
    "        \n",
    "        # case linear\n",
    "        \n",
    "        d_1 = tf.stack([bias_L] * isp[0])\n",
    "        result_L = tf.matmul(input, kernel_L) + d_1 \n",
    "        result_L = relu(result_L)\n",
    "        \n",
    "        # merge\n",
    "        \n",
    "        result = tf.concat([result_S, result_L],axis=1)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "\n",
    "class NN_Model(Model):\n",
    "    \n",
    "    def __init__(self,c,l,n,m,hs,hl):\n",
    "        self.C=c\n",
    "        self.L=l\n",
    "        self.N=n\n",
    "        self.M=m\n",
    "        self.HS = hs\n",
    "        self.HL = hl\n",
    "        super(NN_Model, self).__init__()\n",
    "        self.d1 = LSLayer(self.HS,self.HL)\n",
    "        self.d2 = Dense(self.C)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        #print (\"call benn:\",x, tf.math.reduce_sum(x))\n",
    "        return self.d2(x)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(datas, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(datas, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step(datas, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    \n",
    "    predictions = model(datas, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "\n",
    "C= 6\n",
    "L= 50\n",
    "N= 5000\n",
    "M= 6\n",
    "HS = 10\n",
    "HL = 0\n",
    "EPOCHS = 4000\n",
    "\n",
    "# Create an instance of the model\n",
    "model = NN_Model(C,L,N,M,HS,HL)\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "#loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "(x_train,y_train,x_test,y_test) = gen_data_array(C, L, N, M)\n",
    "print (x_train[:2])\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).batch(32)\n",
    "#print (train_ds)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for epoch in range(EPOCHS):\n",
    "  # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    for datas, labels in train_ds:\n",
    "        train_step(datas, labels)\n",
    "        \n",
    "                \n",
    "    for test_datas, test_labels in test_ds:\n",
    "        #print (\"test_data_shape\", test_datas.shape)\n",
    "        predictions = model(test_datas, training=False)\n",
    "        #print (\"ttttttttttttttttttt\")\n",
    "        #for i in range(test_datas.shape[0]):\n",
    "        #    print (predictions.numpy()[i], test_labels.numpy()[i])\n",
    "        test_step(test_datas, test_labels)\n",
    "    \n",
    "    X.append(epoch)\n",
    "    Y.append(test_accuracy.result() * 100)\n",
    "    if epoch % 20 == 0:\n",
    "        print(\n",
    "            f'Epoch {epoch + 1}, '\n",
    "            f'Loss: {train_loss.result()}, '\n",
    "            f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "            f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "          )    \n",
    "print(\n",
    "    f'Epoch {epoch + 1}, '\n",
    "    f'Loss: {train_loss.result()}, '\n",
    "    f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "    f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "  )    \n",
    "\n",
    "plt.plot(X, Y,label=\"Accuracy curve\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "vanilla-furniture",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.0 13.311649033834989\n"
     ]
    }
   ],
   "source": [
    "import statistics \n",
    "\n",
    "x = [86,84,96,56,82,82]\n",
    "print (sum(x)/len(x), statistics.stdev(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "received-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "def gen_cluster_data_list(Cv, Lv, Nv, Mv):\n",
    "    Tr = []\n",
    "    Ts = []\n",
    "    C = Cv  # number of categories\n",
    "    L = Lv   # number of centers\n",
    "    N = Nv  # number of elements\n",
    "    M = Mv  # number of dimensions\n",
    "    X, y = make_blobs(n_samples=N, centers=L, n_features=M,cluster_std=.5, random_state=11)\n",
    "    cmap = []\n",
    "    for _ in range(L):\n",
    "        cmap.append(random.randint(0,C-1))\n",
    "    cols = []\n",
    "    for i in range(N):\n",
    "        cols.append(cmap[y[i]])\n",
    "\n",
    "    for i in range(int(0.9*N)):\n",
    "        row = [X[i,j] for j in range(M)]\n",
    "        row.append(cols[i])\n",
    "        Tr.append(row)\n",
    "    \n",
    "    for i in range(int(0.9*N)+1,N):\n",
    "        row = [X[i,j] for j in range(M)]\n",
    "        row.append(cols[i])\n",
    "        Ts.append(row)\n",
    "        \n",
    "    return (Tr, Ts)\n",
    "\n",
    "def normalize (train):\n",
    "    mx = []\n",
    "    mn = []\n",
    "    for i in range(len(train[0])-1):\n",
    "        mx.append(max([x[i] for x in train ]))\n",
    "        mn.append(min([x[i] for x in train ]))\n",
    "    for row in train:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i] - mn[i]) / (mx[i] - mn[i]) \n",
    "    return train\n",
    "\n",
    "\n",
    "def gen_data_array(Cv, Lv, Nv, Mv):\n",
    "    C = Cv  # number of categories\n",
    "    L = Lv   # number of centers\n",
    "    N = Nv  # number of elements\n",
    "    M = Mv  # number of dimensions\n",
    "    (T1,T2)  = gen_cluster_data_list(C, L, N, M)\n",
    "\n",
    "    T = normalize(T1)\n",
    "    N = len(T)\n",
    "    x2_train = np.zeros((N,M),dtype='float32')\n",
    "    y2_train = np.zeros((N,C))\n",
    "    for i in range(N):\n",
    "        row = T[i]\n",
    "        for j in range(M):\n",
    "            x2_train[i,j] = row[j]\n",
    "        y2_train[i,row[-1]] = 1\n",
    "\n",
    "    Ts = normalize(T2)\n",
    "    Ns = len(Ts)\n",
    "    x2_test = np.zeros((Ns,M),dtype='float32')\n",
    "    y2_test = np.zeros((Ns,C))\n",
    "    for i in range(Ns):\n",
    "        row = Ts[i]\n",
    "        for j in range(M):\n",
    "            x2_test[i,j] = row[j]\n",
    "        y2_test[i, row[-1]] = 1\n",
    "        \n",
    "    return (x2_train,y2_train, x2_test, y2_test)\n",
    "\n",
    "def gen_data_array_s(Cv, Lv, Nv, Mv):\n",
    "    C = Cv  # number of categories\n",
    "    L = Lv   # number of centers\n",
    "    N = Nv  # number of elements\n",
    "    M = Mv  # number of dimensions\n",
    "    (T1,T2)  = gen_cluster_data_list(C, L, N, M)\n",
    "\n",
    "    T = normalize(T1)\n",
    "    N = len(T)\n",
    "    x2_train = np.zeros((N,M),dtype='float32')\n",
    "    y2_train = np.zeros((N,1))\n",
    "    for i in range(N):\n",
    "        row = T[i]\n",
    "        for j in range(M):\n",
    "            x2_train[i,j] = row[j]\n",
    "        y2_train[i] = row[-1]\n",
    "\n",
    "    Ts = normalize(T2)\n",
    "    Ns = len(Ts)\n",
    "    x2_test = np.zeros((Ns,M),dtype='float32')\n",
    "    y2_test = np.zeros((Ns,1))\n",
    "    for i in range(Ns):\n",
    "        row = Ts[i]\n",
    "        for j in range(M):\n",
    "            x2_test[i,j] = row[j]\n",
    "        y2_test[i] = row[-1]\n",
    "        \n",
    "    return (x2_train,y2_train, x2_test, y2_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-prisoner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
