{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N= 623\n",
      "k= 0 ------------------------------\n",
      "0 XX 0.6183575 0.65789473\n",
      "update layer\n",
      "20 XX 0.7004831 0.81578946\n",
      "40 XX 0.7215742 0.81578946\n",
      "60 XX 0.7347747 0.81578946\n",
      "accuracy: 0.81578946 0.81578946\n",
      "k= 1 ------------------------------\n",
      "0 XX 0.62801933 0.65789473\n",
      "update layer\n",
      "20 XX 0.7004831 0.7894737\n",
      "40 XX 0.7146224 0.7894737\n",
      "60 XX 0.72907263 0.80263156\n",
      "accuracy: 0.80263156 0.81578946\n",
      "k= 2 ------------------------------\n",
      "0 XX 0.5797101 0.6447368\n",
      "update layer\n",
      "20 XX 0.70508397 0.7631579\n",
      "40 XX 0.714858 0.7763158\n",
      "60 XX 0.7270135 0.7894737\n",
      "accuracy: 0.7894737 0.81578946\n",
      "k= 3 ------------------------------\n",
      "0 XX 0.6666667 0.67105263\n",
      "update layer\n",
      "20 XX 0.7412008 0.7631579\n",
      "40 XX 0.75303406 0.7894737\n",
      "60 XX 0.7659777 0.80263156\n",
      "accuracy: 0.80263156 0.80263156\n",
      "k= 4 ------------------------------\n",
      "0 XX 0.65217394 0.65789473\n",
      "update layer\n",
      "20 XX 0.71428573 0.7631579\n",
      "40 XX 0.72993994 0.7894737\n",
      "60 XX 0.7476835 0.81578946\n",
      "accuracy: 0.81578946 0.80263156\n",
      "k= 5 ------------------------------\n",
      "0 XX 0.4879227 0.46052632\n",
      "update layer\n",
      "20 XX 0.6988728 0.7894737\n",
      "40 XX 0.7202781 0.7894737\n",
      "60 XX 0.7304189 0.7763158\n",
      "accuracy: 0.7763158 0.80263156\n",
      "k= 6 ------------------------------\n",
      "0 XX 0.6135266 0.6315789\n",
      "update layer\n",
      "20 XX 0.71037495 0.75\n",
      "40 XX 0.7318252 0.7631579\n",
      "60 XX 0.7433278 0.81578946\n",
      "accuracy: 0.81578946 0.80263156\n",
      "k= 7 ------------------------------\n",
      "0 XX 0.3478261 0.34210527\n",
      "update layer\n",
      "20 XX 0.7004831 0.7368421\n",
      "40 XX 0.72487336 0.7894737\n",
      "60 XX 0.73722976 0.7894737\n",
      "accuracy: 0.7894737 0.80263156\n",
      "k= 8 ------------------------------\n",
      "0 XX 0.6328502 0.65789473\n",
      "update layer\n",
      "20 XX 0.7280883 0.7763158\n",
      "40 XX 0.75091314 0.80263156\n",
      "60 XX 0.766928 0.81578946\n",
      "accuracy: 0.81578946 0.81578946\n",
      "k= 9 ------------------------------\n",
      "0 XX 0.6038647 0.65789473\n",
      "update layer\n",
      "20 XX 0.70439386 0.7894737\n",
      "40 XX 0.7208672 0.80263156\n",
      "60 XX 0.7378633 0.80263156\n",
      "accuracy: 0.80263156 0.81578946\n",
      "final accuracy: 0.81578946\n"
     ]
    }
   ],
   "source": [
    "# hybrid model DLS my proposal\n",
    "\n",
    "# P 05\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.activations import relu\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "#import statistics \n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "class LSLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,  num_outputs_s, num_outputs_r, num_outputs_l, indata,  Ara = 1.0, activation=sigmoid, wstd = 0.3, bstd = 0.5):\n",
    "        super(LSLayer, self).__init__()\n",
    "        self.num_outputs_l = num_outputs_l\n",
    "        self.num_outputs_s = num_outputs_s \n",
    "        self.num_outputs_r = num_outputs_r\n",
    "        self.num_outputs = num_outputs_l + num_outputs_s + num_outputs_r\n",
    "        self.activation = activation\n",
    "        self.wstd = wstd\n",
    "        self.bstd = bstd\n",
    "        self.traindata = indata\n",
    "        self.Ara = Ara\n",
    "        \n",
    "    def build(self, input_shape):  \n",
    "        self.num_inputs = input_shape[-1]\n",
    "        self.kernel = self.add_weight(\"kernel\",\n",
    "                                      shape=(int(input_shape[-1]),\n",
    "                                             self.num_outputs), \n",
    "                                      initializer=tf.keras.initializers.RandomNormal(stddev=self.wstd),\n",
    "                                     trainable=True)\n",
    "\n",
    "        self.bias = self.add_weight(\"bias\",\n",
    "                                      shape=[self.num_outputs],\n",
    "                                    initializer=tf.keras.initializers.RandomNormal(stddev=self.bstd),\n",
    "                                   trainable=True)\n",
    "            \n",
    "        #print ( \"set_circles ---------------------------\")\n",
    "        \n",
    "        M = self.num_outputs_s + self.num_outputs_r\n",
    "        \n",
    "        if M == 0:\n",
    "            return\n",
    "        \n",
    "        x_train =  self.traindata[0]\n",
    "        y_train =  self.traindata[1]\n",
    "        N = x_train.shape[0]   \n",
    "        D = x_train.shape[1]   \n",
    "        C = min (M*10,int(0.3*N))\n",
    "        #print (\"C\",C,\"M\",M)\n",
    "        \n",
    "        cls = KMeans(n_clusters=C).fit(x_train)\n",
    "        \n",
    "        centers = cls.cluster_centers_\n",
    "        #print (\"centers\", centers.shape)\n",
    "        #print (centers)\n",
    "        labels = cls.labels_\n",
    "\n",
    "        cvals = []\n",
    "        for c in range(C):\n",
    "            db1 = 0\n",
    "            db2 = 0\n",
    "            for i in range(N):\n",
    "                if labels[i] == c:\n",
    "                    if y_train[i][0] == 1:\n",
    "                        db1 += 1\n",
    "                    else:\n",
    "                        db2 += 1\n",
    "            try:\n",
    "                h = -math.log(db1/(db1+db2))*db1/(db1+db2) - math.log(db2/(db1+db2))*db2/(db1+db2) \n",
    "            except:\n",
    "                h = 0\n",
    "            #print (\"class:\", c,db1 + db2, db1, db2,h)\n",
    "            cvals.append((c,(db1+db2)* (1 - h)))\n",
    "\n",
    "        #print (cvals)\n",
    "        cvals.sort(key = lambda x: x[1] )\n",
    "        #print (cvals)\n",
    "\n",
    "        winc = []\n",
    "        #print (M)\n",
    "        for i in range(M):\n",
    "            cw = cvals[-i][0]\n",
    "            d0 = 0\n",
    "            for j in range(N):\n",
    "                if labels[j] == cw:\n",
    "                    d = math.sqrt(sum([ (centers[cw][k] - x_train[j][k])**2 for k in range(D) ]  ))\n",
    "                    if d > d0:\n",
    "                        d0 = d\n",
    "            winc.append((cw, centers[cw], d0 ) )\n",
    "\n",
    "        #for i in range(len(winc)):\n",
    "        #    print (i,  ':', winc[i])\n",
    "        #print (\"end -------------------------------\")\n",
    "\n",
    "        xu = self.get_weights()\n",
    "\n",
    "        #print (xu[0].shape, xu[1].shape)\n",
    "        \n",
    "        for c in range(D):\n",
    "            for m in range(M):\n",
    "                xu[0][c,m] = winc[m][1][c]\n",
    "        \n",
    "        \n",
    "        for m in range(M):\n",
    "            xu[1][m] = winc[m][2]* self.Ara\n",
    "            \n",
    "        #print (xu[0])\n",
    "        #print (xu[1])\n",
    "            \n",
    "        self.set_weights(xu )\n",
    "        #print (\"end ==============================\")\n",
    "        \n",
    "    \n",
    "    # F2 method LS layer\n",
    "    def call(self, input):\n",
    "        \n",
    "        #print (\"CALL :\", input.numpy())\n",
    "        isp = input.shape\n",
    "        In1 = tf.transpose(input)\n",
    "        kernel_S, kernel_L  = tf.split(self.kernel,[ self.num_outputs_s + self.num_outputs_r, self.num_outputs_l ], axis = 1 )\n",
    "        bias_S, bias_L  = tf.split(self.bias,[ self.num_outputs_s +  self.num_outputs_r, self.num_outputs_l ], axis = 0 )\n",
    "        \n",
    "        # case spherical\n",
    "        \n",
    "        s_shape  = self.num_outputs_s + self.num_outputs_r\n",
    "        In2 = tf.stack([In1] * s_shape)\n",
    "        InD = tf.transpose(In2)\n",
    "        WD = tf.stack([kernel_S] * isp[0])\n",
    "        ddd = WD - InD\n",
    "        dd0 = tf.math.multiply(ddd, ddd)\n",
    "        dd1 = tf.math.reduce_sum(dd0, axis =1)\n",
    "        dd2 = tf.cast(dd1,tf.double)\n",
    "        dd3 = tf.sqrt(dd2)\n",
    "        d_r = tf.cast(dd3,tf.float32)\n",
    "        result_S = relu(d_r)\n",
    "        \n",
    "        #d_R = tf.abs(bias_S)\n",
    "        #d_rR = tf.math.divide_no_nan(d_r,d_R)\n",
    "        #d_x0 = tf.ones(d_rR.shape) - d_rR\n",
    "        #result_S = tf.math.scalar_mul(6,d_x0)\n",
    "        #result_S = sigmoid(result_S)\n",
    "        \n",
    "        # case linear\n",
    "\n",
    "        d_1 = tf.stack([bias_L] * isp[0])\n",
    "        result_L = tf.matmul(input, kernel_L) + d_1 \n",
    "        result_L = relu(result_L)\n",
    "\n",
    "        #case empty, merge\n",
    "        \n",
    "        '''\n",
    "        #print (self.num_outputs_r)\n",
    "        if self.num_outputs_r > 0:\n",
    "            r_S, _ = tf.split (result_S,[self.num_outputs_s, self.num_outputs_r],axis=1 )\n",
    "            r_1 = np.zeros((result_S.shape[0],self.num_outputs_r))\n",
    "            result_R = tf.cast(tf.constant(r_1),tf.float32)\n",
    "            result = tf.concat([r_S, result_R, result_L],axis=1)            \n",
    "            #print (self.num_outputs_s, self.num_outputs_r)\n",
    "            #print (\"result_S\", result_S)\n",
    "            #print (\"result_L\", result_L)\n",
    "            #print (\"result\", result)\n",
    "        else:\n",
    "            result = tf.concat([result_S, result_L],axis=1)        \n",
    "        '''\n",
    "        \n",
    "        result = tf.concat([result_S, result_L],axis=1)        \n",
    "        \n",
    "        return result\n",
    "    \n",
    "        \n",
    "\n",
    "class NN_Model(Model):\n",
    "    \n",
    "    def __init__(self,c,hs,hr,hl,indata,A):\n",
    "        super(NN_Model, self).__init__()\n",
    "        self.d1 = LSLayer(hs,hr,hl,indata,A)\n",
    "        self.d2 = Dense(c)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        #print (\"call benn:\",x, tf.math.reduce_sum(x))\n",
    "        return self.d2(x)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "#@tf.function\n",
    "def train_step(datas, labels,modelk,loss_objectk,optimizerk,train_lossk,train_accuracyk):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = modelk(datas, training=True)\n",
    "        loss = loss_objectk(labels, predictions)\n",
    "    gradients = tape.gradient(loss, modelk.trainable_variables)\n",
    "    optimizerk.apply_gradients(zip(gradients, modelk.trainable_variables))\n",
    "\n",
    "    train_lossk(loss)\n",
    "    train_accuracyk(labels, predictions)\n",
    "\n",
    "#@tf.function\n",
    "def test_step(datas, labels,modelk,loss_objectk,test_lossk,test_accuracyk):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    \n",
    "    predictions = modelk(datas, training=False)\n",
    "    t_loss = loss_objectk(labels, predictions)\n",
    "\n",
    "    test_lossk(t_loss)\n",
    "    test_accuracyk(labels, predictions)\n",
    "        \n",
    "\n",
    "    \n",
    "def analyze_NN (model, train_ds, L=0,verbose = 0):\n",
    "    cnt = 0\n",
    "    xu = model.layers[L].get_weights()\n",
    "    C = xu[0].shape[1]\n",
    "    M = xu[0].shape[0]\n",
    "    C = model.d1.num_outputs_s + model.d1.num_outputs_r\n",
    "    #print (C,M)\n",
    "    Cvals = []\n",
    "    for c in range(C):\n",
    "        cnt = 0\n",
    "        outs = []\n",
    "        for datas, labels in train_ds:\n",
    "            dA = datas.numpy()\n",
    "            dL = labels.numpy()\n",
    "            for i in range(dA.shape[0]):\n",
    "                cnt += 1\n",
    "                cat = np.argmax(dL[i]) + 1\n",
    "                tA1 = tf.constant(dA[i])\n",
    "                tA = tf.reshape(tA1,[1,dA.shape[1]])\n",
    "                if L == 1:\n",
    "                    to1 = model.d1 (tA)\n",
    "                    to2 = model.d2 (to1)\n",
    "                else:\n",
    "                    to2 =  model.d1 (tA)\n",
    "                #print (tA.numpy())\n",
    "                #print (to2.numpy())\n",
    "                outs.append((to2[0][c].numpy(), cat))\n",
    "        #print (\"layer\", L, \" node\",  c, \" R=\",xu[1][c])\n",
    "        X = [x[0] for x in outs]\n",
    "        Y = [x[1] for x in outs]\n",
    "        X1 = [x[0] for x in outs if x[1] == 1]\n",
    "        X2 = [x[0] for x in outs if x[1] == 2]\n",
    "        sim_level = ks_2samp(X1,X2).pvalue\n",
    "        if verbose == 1:\n",
    "            plt.scatter(X,Y)\n",
    "            plt.show()\n",
    "            print (sim_level)\n",
    "        Cvals.append((c,sim_level))\n",
    "        \n",
    "    Cvals.sort(key = lambda x : -x[1])\n",
    "    return Cvals\n",
    "    \n",
    "    \n",
    "    \n",
    "def update_NN_model (model, train_ds ):\n",
    "\n",
    "    \n",
    "    rdb = 0\n",
    "    odb = 0\n",
    "    #N = min(5, model.d1.num_outputs_r)   # number of new SSN nodes\n",
    "    M = model.d1.num_outputs_r   # number of updated nodes\n",
    "    if M == 0:\n",
    "        return \n",
    "    \n",
    "    Clist = analyze_NN (model, train_ds)\n",
    "\n",
    "    baditems = []\n",
    "    yitems = []\n",
    "    for datas, labels in train_ds:\n",
    "        predictions = model(datas, training=False)\n",
    "        for i in range(datas.shape[0]):\n",
    "            #print (datas.numpy()[i], predictions.numpy()[i], np.argmax(predictions.numpy()[i]), labels.numpy()[i],np.argmax(labels.numpy()[i]))\n",
    "            if np.argmax(predictions.numpy()[i]) != np.argmax(labels.numpy()[i]):\n",
    "                rdb = rdb + 1\n",
    "                baditems.append(datas.numpy()[i])\n",
    "                yitems.append(labels.numpy()[i])\n",
    "            odb = odb + 1        \n",
    "\n",
    "    N = len(baditems)\n",
    "    C = min(2*M, len(baditems))  # number of cluster centers\n",
    "    if C == 0:\n",
    "        return\n",
    "    \n",
    "    print (\"update layer\")\n",
    "\n",
    "    # k-means\n",
    "\n",
    "    cls = KMeans(n_clusters=C).fit(baditems)\n",
    "        \n",
    "    centers = cls.cluster_centers_\n",
    "    labels = cls.labels_\n",
    "    D = centers.shape[1]\n",
    "\n",
    "    cvals = []\n",
    "    for c in range(C):\n",
    "        db1 = 0\n",
    "        db2 = 0\n",
    "        for i in range(N):\n",
    "            if labels[i] == c:\n",
    "                if yitems[i][0] == 1:\n",
    "                    db1 += 1\n",
    "                else:\n",
    "                    db2 += 1\n",
    "        try:\n",
    "            h = -math.log(db1/(db1+db2))*db1/(db1+db2) - math.log(db2/(db1+db2))*db2/(db1+db2) \n",
    "        except:\n",
    "            h = 0\n",
    "        #print (\"class:\", c,db1 + db2, db1, db2,h)\n",
    "        cvals.append((c,(db1+db2)* (1 - h)))\n",
    "\n",
    "    cvals.sort(key = lambda x: x[1] )\n",
    "\n",
    "    winc = []\n",
    "    #print (M)\n",
    "    for i in range(M):\n",
    "        cw = cvals[-i][0]\n",
    "        d0 = 0\n",
    "        for j in range(N):\n",
    "            if labels[j] == cw:\n",
    "                d = math.sqrt(sum([ (centers[cw][k] - baditems[j][k])**2 for k in range(D) ]  ))\n",
    "                if d > d0:\n",
    "                    d0 = d\n",
    "        winc.append((cw, centers[cw], d0 ) )\n",
    "\n",
    "    \n",
    "    xu = model.d1.get_weights()\n",
    "\n",
    "\n",
    "    for c in range(D):    # input dim\n",
    "        for m in range(M):   # neuron sorszam\n",
    "            nn = Clist[m][0]\n",
    "            xu[0][c,nn] = winc[m][1][c]\n",
    "\n",
    "\n",
    "    for m in range(M):\n",
    "        nn = Clist[m][0]\n",
    "        xu[1][nn] = winc[m][2]*model.d1.Ara\n",
    "\n",
    "    #print (xu[0])\n",
    "    #print (xu[1])\n",
    "\n",
    "    model.d1.set_weights(xu )\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# (x_train,y_train,x_test,y_test) = gen_data_array_cv(K)\n",
    "\n",
    "M = x_train[0].shape[1]\n",
    "C = y_train[0].shape[1]\n",
    "\n",
    "EPOCHS = 60+1\n",
    "Eupd = [20]\n",
    "\n",
    "K = 10\n",
    "\n",
    "H = M * 5\n",
    "HS = 0\n",
    "HR = 20\n",
    "HL = 20\n",
    "\n",
    "B = 32\n",
    "A = 0.1\n",
    "\n",
    "model = []\n",
    "loss_object =  []\n",
    "optimizer = []\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "val_loss = []\n",
    "val_accuracy = []\n",
    "train_ds = []\n",
    "val_ds = []\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(B)\n",
    "\n",
    "\n",
    "\n",
    "gbest_v = 0\n",
    "gbest_t = 0\n",
    "\n",
    "print (\"N=\", x_train[0].shape[0])\n",
    "\n",
    "for k in range(K):\n",
    "    \n",
    "    best_v = 0\n",
    "    best_t = 0\n",
    "    print (\"k=\",k, \"------------------------------\")\n",
    "    # Create an instance of the model\n",
    "    model.append( NN_Model(C,HS,HR,HL,(x_train[k], y_train[k]),  A))\n",
    "    #model.append( NN_Model(H,C,0,0))\n",
    "\n",
    "    loss_object.append(tf.keras.losses.CategoricalCrossentropy(from_logits=True))\n",
    "    #loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "    #optimizer.append(tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07,))\n",
    "    #optimizer.append(tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07,))\n",
    "    optimizer.append(tf.keras.optimizers.Adam())\n",
    "    train_loss.append(tf.keras.metrics.Mean(name='train_loss'))\n",
    "    train_accuracy.append(tf.keras.metrics.CategoricalAccuracy(name='train_accuracy'))\n",
    "\n",
    "    test_loss.append(tf.keras.metrics.Mean(name='test_loss'))\n",
    "    test_accuracy.append(tf.keras.metrics.CategoricalAccuracy(name='test_accuracy'))\n",
    "    \n",
    "    val_loss.append(tf.keras.metrics.Mean(name='val_loss'))\n",
    "    val_accuracy.append(tf.keras.metrics.CategoricalAccuracy(name='val_accuracy'))\n",
    "    \n",
    "\n",
    "    #print (x_train[:2])\n",
    "    train_ds.append( tf.data.Dataset.from_tensor_slices(\n",
    "        (x_train[k], y_train[k])).batch(B))\n",
    "    if K > 1:\n",
    "        val_ds.append( tf.data.Dataset.from_tensor_slices((x_val[k], y_val[k])).batch(B))\n",
    "        #val_ds.append( tf.data.Dataset.from_tensor_slices((xA_val[k], yA_val[k])).batch(B))\n",
    "        #print (train_ds)\n",
    "\n",
    "\n",
    "    ##set_circles(model[k],x_train[k], y_train[k], HS+HR)\n",
    "\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for epoch in range(EPOCHS):\n",
    "      # Reset the metrics at the start of the next epoch\n",
    "\n",
    "        train_loss[k].reset_states()\n",
    "        train_accuracy[k].reset_states()\n",
    "        test_loss[k].reset_states()\n",
    "        test_accuracy[k].reset_states()\n",
    "        \n",
    "\n",
    "        for datas, labels in train_ds[k]:\n",
    "            train_step(datas, labels,model[k],loss_object[k],optimizer[k],train_loss[k],train_accuracy[k])\n",
    "\n",
    "\n",
    "        for test_datas, test_labels in test_ds:\n",
    "            #tpredictions = model[k](test_datas, training=False)\n",
    "            test_step(test_datas, test_labels,model[k],loss_object[k],test_loss[k],test_accuracy[k])\n",
    "\n",
    "        if K > 1:\n",
    "            for val_datas, val_labels in val_ds[k]:\n",
    "                #vpredictions = model[k](val_datas, training=False)\n",
    "                test_step(val_datas, val_labels,model[k],loss_object[k],val_loss[k],val_accuracy[k])\n",
    "            \n",
    "            \n",
    "        \n",
    "        #if (k == 0 or k == 8)  and epoch == 75:\n",
    "        #    analyze_NN(model[k], train_ds[k])\n",
    "    \n",
    "        if epoch in Eupd:\n",
    "            update_NN_model (model[k], train_ds[k])\n",
    "\n",
    "\n",
    "        X.append(epoch)\n",
    "        Y.append(test_accuracy[k].result() * 100)\n",
    "        if epoch % 20 == 0:\n",
    "            #print(\n",
    "            #    f'Epoch {epoch + 1}, '\n",
    "            #    f'Loss: {train_loss[k].result()}, '\n",
    "            #    f'Val Accuracy: {val_accuracy[k].result() * 100}, '\n",
    "            #    f'Test Accuracy: {test_accuracy[k].result() * 100}'\n",
    "            #  )    \n",
    "            if K > 1:\n",
    "                if val_accuracy[k].result() > best_v :\n",
    "                    best_v = val_accuracy[k].result().numpy()\n",
    "                    best_t = test_accuracy[k].result().numpy()\n",
    "                    print(epoch, \"XX\", best_v,best_t)\n",
    "                else:\n",
    "                    print (epoch)\n",
    "                #print(model.d1.bias.numpy())\n",
    "            else:\n",
    "                print(epoch, \"XX\", test_accuracy[k].result().numpy())\n",
    "            \n",
    "\n",
    "    #print(\n",
    "    #    f'Epoch {epoch + 1}, '\n",
    "    #    f'Loss: {train_loss[k].result()}, '\n",
    "    #    f'Val Accuracy: {val_accuracy[k].result() * 100}, '\n",
    "    #    f'Test Accuracy: {test_accuracy[k].result() * 100}'\n",
    "    #  )    \n",
    "    #acclist.append(test_accuracy[k].result())\n",
    "    if best_v > gbest_v:\n",
    "        gbest_v = best_v\n",
    "        gbest_t = best_t\n",
    "        \n",
    "    print (\"accuracy:\",best_t, gbest_t)\n",
    "    #plt.plot(X, Y,label=\"Accuracy curve\")\n",
    "print (\"final accuracy:\",gbest_t)\n",
    "if K == 1:\n",
    "    print (\"final\",  test_accuracy[0].result().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.175 2.1422340363897368\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "x = [79,84,80.2,81.5] \n",
    "print (sum(x)/len(x), statistics.stdev(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import statistics \n",
    "\n",
    "\n",
    "x = [75,76.1,75,75,73.7,75]\n",
    "print (sum(x)/len(x), statistics.stdev(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 0\n",
      "0.73913044 0.82894737\n",
      "X [0.5467918417304034, 0.73913044] [0.4974383649073149, 0.82894737]\n",
      "0.7777778 0.7631579\n",
      "X [0.492919125130787, 0.7777778] [0.44118981926064743, 0.7631579]\n",
      "0.77294683 0.7763158\n",
      "k= 1\n",
      "0.647343 0.7236842\n",
      "0.6714976 0.80263156\n",
      "0.73913044 0.7763158\n",
      "k= 2\n",
      "0.6231884 0.7105263\n",
      "0.7198068 0.7631579\n",
      "0.7487923 0.7368421\n",
      "k= 3\n",
      "0.74396133 0.75\n",
      "0.79227054 0.7763158\n",
      "X [0.47826142040427755, 0.79227054] [0.45516292986116913, 0.7763158]\n",
      "0.8067633 0.80263156\n",
      "X [0.4502598620267306, 0.8067633] [0.42770381350266307, 0.80263156]\n",
      "k= 4\n",
      "0.73913044 0.7894737\n",
      "0.77294683 0.7894737\n",
      "0.77294683 0.7894737\n",
      "k= 5\n",
      "0.73913044 0.7894737\n",
      "0.74396133 0.80263156\n",
      "0.76811594 0.7763158\n",
      "k= 6\n",
      "0.71014494 0.80263156\n",
      "0.7584541 0.7894737\n",
      "0.76328504 0.82894737\n",
      "k= 7\n",
      "0.7294686 0.7894737\n",
      "0.76811594 0.7763158\n",
      "0.7584541 0.81578946\n",
      "k= 8\n",
      "0.7294686 0.7631579\n",
      "0.7826087 0.80263156\n",
      "0.82125604 0.7894737\n",
      "X [0.44757889319157257, 0.82125604] [0.4292657971382141, 0.7894737]\n",
      "k= 9\n",
      "0.7004831 0.80263156\n",
      "0.7536232 0.7631579\n",
      "0.7777778 0.81578946\n",
      "acc= 0.7894737 ( 0.82125604 )\n"
     ]
    }
   ],
   "source": [
    "# 10-fold coross validation \n",
    "# factory model\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "\n",
    "\n",
    "K = 10\n",
    "\n",
    "#(x_train,y_train,x_test,y_test) = gen_data_array_cv(K)\n",
    "\n",
    "best_v = 0\n",
    "best_t = 0\n",
    "\n",
    "for k in range(K):\n",
    "    print (\"k=\",k)\n",
    "    N = x_train[k].shape[0]\n",
    "    M = x_train[k].shape[1]\n",
    "    C = y_train[k].shape[1]\n",
    "    H = 40\n",
    "    E = 60\n",
    "\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(H, input_shape=(M,), activation='relu'),\n",
    "      tf.keras.layers.Dense(C)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=loss_fn,\n",
    "                  metrics=['CategoricalAccuracy'])\n",
    "\n",
    "    #print (model.summary())\n",
    "    #print (x_val[k].shape,y_val[k].shape,)\n",
    "    es = 20\n",
    "    #loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    for e in range(int(E/es)):\n",
    "        model.fit(x_train[k], y_train[k], validation_data=(x_val[k],y_val[k]), epochs=es,verbose = 0)\n",
    "        #model.fit(x_train[k], y_train[k],  epochs=es,verbose = 0)\n",
    "        res_test = model.evaluate(x_test,  y_test, verbose=0)\n",
    "        res_val = model.evaluate(x_val[k],  y_val[k], verbose=0)\n",
    "        print (res_val[1], res_test[1])\n",
    "        if res_val[1] > best_v:\n",
    "            best_v = res_val[1]\n",
    "            best_t = res_test[1]\n",
    "            print ('X', res_val, res_test)\n",
    "            \n",
    "print (\"acc=\", best_t, \"(\",best_v,\")\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.375 0.47871355387816905\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "x = [79.5,79,80,79]\n",
    "print (sum(x)/len(x),statistics.stdev(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data (768, 9)\n",
      "692 76 623 69 207\n"
     ]
    }
   ],
   "source": [
    "# read benchmark data\n",
    "\n",
    "from pandas import  read_csv\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "def gen_data_array_cv2(K):\n",
    "    \n",
    "    \n",
    "    dataT = read_csv (\"pima-indians-diabetes.csv\", header =None)\n",
    "    dataA = np.array(dataT)    \n",
    "        \n",
    "    N = dataA.shape[0]\n",
    "    print (\"Data\",dataA.shape)\n",
    "    M = dataA.shape[1]-1\n",
    "\n",
    "    Nte = int(0.1*N)\n",
    "    Ntr = N - Nte\n",
    "\n",
    "    cval = dict()\n",
    "    for i in range(N):\n",
    "        cval[dataA[i][-1]] = 1\n",
    "    C = len(cval.keys())\n",
    "    \n",
    "    minv = [0 for _ in range(M)]\n",
    "    maxv = [0 for _ in range(M)]\n",
    "    \n",
    "    for j in range(M):\n",
    "        minv[j] =  dataA[0,j]\n",
    "        maxv[j] =  dataA[0,j]\n",
    "        \n",
    "    for i in range(N):\n",
    "        for j in range(M):\n",
    "            minv[j] = min( dataA[i,j] , minv[j])\n",
    "            maxv[j] = max( dataA[i,j] , maxv[j])\n",
    "          \n",
    "    orders = random.sample(range(N), N)\n",
    "    \n",
    "    x2_test = np.zeros((Nte,M),dtype='float32')\n",
    "    y2_test = np.zeros((Nte,C))\n",
    "    \n",
    "    ite = 0\n",
    "    for n in range(Ntr, N):\n",
    "        i = orders[n]\n",
    "        for j in range(M):\n",
    "            x2_test[ite,j] = (dataA[i,j] - minv[j]) / (maxv[j] - minv[j])\n",
    "        y2_test[ite, int(dataA[i,M])] = 1\n",
    "        ite += 1    \n",
    "       \n",
    "    if K == 1:\n",
    "        Ntrv = 0\n",
    "        Ntrt = Ntr \n",
    "\n",
    "        x2_train = []\n",
    "        y2_train = []\n",
    "        x2_val = []\n",
    "        y2_val = []\n",
    "        x2A_val = []\n",
    "        y2A_val = []\n",
    "\n",
    "        x2_train.append(np.zeros((Ntrt,M),dtype='float32'))\n",
    "        y2_train.append(np.zeros((Ntrt,C)))\n",
    "        k = 0\n",
    "        itr = 0\n",
    "        ite = 0\n",
    "        for n in range(Ntr):\n",
    "            i = orders[n]\n",
    "            for j in range(M):\n",
    "                x2_train[k][itr,j] = (dataA[i,j] - minv[j]) / (maxv[j] - minv[j])\n",
    "            y2_train[k][itr,int(dataA[i,M])] = 1\n",
    "            itr += 1\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        Ntrv = int(Ntr/K)\n",
    "        Ntrt = Ntr - Ntrv\n",
    "\n",
    "        x2_train = []\n",
    "        y2_train = []\n",
    "        x2_val = []\n",
    "        y2_val = []\n",
    "        x2A_val = []\n",
    "        y2A_val = []\n",
    "\n",
    "    \n",
    "        Ntrv2 = min(3*Ntrv, Ntrt)\n",
    "        print (Ntr, Nte, Ntrt, Ntrv, Ntrv2)\n",
    "        for k in range(K):\n",
    "            #print (k,\" ----------- \")\n",
    "            #print (Ntrt, Ntrv, Ntrv2)\n",
    "            x2_train.append(np.zeros((Ntrt,M),dtype='float32'))\n",
    "            y2_train.append(np.zeros((Ntrt,C)))\n",
    "            x2_val.append(np.zeros((Ntrv2,M),dtype='float32'))\n",
    "            y2_val.append(np.zeros((Ntrv2,C)))\n",
    "            x2A_val.append(np.zeros((Ntrv,M),dtype='float32'))\n",
    "            y2A_val.append(np.zeros((Ntrv,C)))\n",
    "\n",
    "            #print (\"x2_val\", x2_val[k].shape)\n",
    "            itr = 0\n",
    "            ite = 0\n",
    "            for n in range(Ntr):\n",
    "                i = orders[n]\n",
    "                if n >= k*Ntrv and n < (k+1)*Ntrv:\n",
    "                    for j in range(M):\n",
    "                        x2_val[k][ite,j] = (dataA[i,j] - minv[j]) / (maxv[j] - minv[j])\n",
    "                        x2A_val[k][ite,j] = (dataA[i,j] - minv[j]) / (maxv[j] - minv[j])\n",
    "                    y2_val[k][ite, int(dataA[i,M])] = 1\n",
    "                    y2A_val[k][ite, int(dataA[i,M])] = 1\n",
    "                    ite += 1\n",
    "                else:            \n",
    "                    for j in range(M):\n",
    "                        x2_train[k][itr,j] = (dataA[i,j] - minv[j]) / (maxv[j] - minv[j])\n",
    "                    y2_train[k][itr,int(dataA[i,M])] = 1\n",
    "                    itr += 1\n",
    "                \n",
    "            extra = random.sample(range(Ntrt),Ntrv2)\n",
    "            ite = Ntrv\n",
    "            for n in range(Ntrv2 - Ntrv):\n",
    "                i = extra[n]\n",
    "                for j in range(M):\n",
    "                    x2_val[k][ite,j] = x2_train[k][i,j]\n",
    "                pos = list(y2_train[k][i,:]).index(1)\n",
    "                #print (y2_train[k][i,:],pos)\n",
    "                y2_val[k][ite, pos] = 1\n",
    "                ite += 1\n",
    "                \n",
    "                \n",
    "    return (x2_train,y2_train, x2A_val, y2A_val, x2_val, y2_val, x2_test, y2_test)\n",
    "\n",
    "K = 10\n",
    "(x_train,y_train,xA_val, yA_val, x_val,y_val, x_test,y_test) = gen_data_array_cv2(K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
