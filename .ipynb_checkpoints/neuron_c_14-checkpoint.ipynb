{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N= 623\n",
      "k= 0 ------------------------------\n",
      "XX 0.71014494 0.7894737\n"
     ]
    }
   ],
   "source": [
    "# hybrid model DLS\n",
    "\n",
    "# P 05\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.activations import relu\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "#import statistics \n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "class LSLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,  num_outputs_s, num_outputs_r, num_outputs_l, indata,  activation=sigmoid, wstd = 0.3, bstd = 0.5):\n",
    "        super(LSLayer, self).__init__()\n",
    "        self.num_outputs_l = num_outputs_l\n",
    "        self.num_outputs_s = num_outputs_s \n",
    "        self.num_outputs_r = num_outputs_r\n",
    "        self.num_outputs = num_outputs_l + num_outputs_s + num_outputs_r\n",
    "        self.activation = activation\n",
    "        self.wstd = wstd\n",
    "        self.bstd = bstd\n",
    "        self.traindata = indata\n",
    "        \n",
    "    def build(self, input_shape):  \n",
    "        self.num_inputs = input_shape[-1]\n",
    "        self.kernel = self.add_weight(\"kernel\",\n",
    "                                      shape=(int(input_shape[-1]),\n",
    "                                             self.num_outputs), \n",
    "                                      initializer=tf.keras.initializers.RandomNormal(stddev=self.wstd),\n",
    "                                     trainable=True)\n",
    "\n",
    "        self.bias = self.add_weight(\"bias\",\n",
    "                                      shape=[self.num_outputs],\n",
    "                                    initializer=tf.keras.initializers.RandomNormal(stddev=self.bstd),\n",
    "                                   trainable=True)\n",
    "            \n",
    "        #print ( \"set_circles ---------------------------\")\n",
    "        \n",
    "        M = self.num_outputs_s + self.num_outputs_r\n",
    "        \n",
    "        if M == 0:\n",
    "            return\n",
    "        \n",
    "        x_train =  self.traindata[0]\n",
    "        y_train =  self.traindata[1]\n",
    "        N = x_train.shape[0]   \n",
    "        D = x_train.shape[1]   \n",
    "        C = min (M*10,int(0.3*N))\n",
    "        #print (\"C\",C,\"M\",M)\n",
    "        \n",
    "        cls = KMeans(n_clusters=C).fit(x_train)\n",
    "        \n",
    "        centers = cls.cluster_centers_\n",
    "        #print (\"centers\", centers.shape)\n",
    "        #print (centers)\n",
    "        labels = cls.labels_\n",
    "\n",
    "        cvals = []\n",
    "        for c in range(C):\n",
    "            db1 = 0\n",
    "            db2 = 0\n",
    "            for i in range(N):\n",
    "                if labels[i] == c:\n",
    "                    if y_train[i][0] == 1:\n",
    "                        db1 += 1\n",
    "                    else:\n",
    "                        db2 += 1\n",
    "            try:\n",
    "                h = -math.log(db1/(db1+db2))*db1/(db1+db2) - math.log(db2/(db1+db2))*db2/(db1+db2) \n",
    "            except:\n",
    "                h = 0\n",
    "            #print (\"class:\", c,db1 + db2, db1, db2,h)\n",
    "            cvals.append((c,(db1+db2)* (1 - h)))\n",
    "\n",
    "        #print (cvals)\n",
    "        cvals.sort(key = lambda x: x[1] )\n",
    "        #print (cvals)\n",
    "\n",
    "        winc = []\n",
    "        #print (M)\n",
    "        for i in range(M):\n",
    "            cw = cvals[-i][0]\n",
    "            d0 = 0\n",
    "            for j in range(N):\n",
    "                if labels[j] == cw:\n",
    "                    d = math.sqrt(sum([ (centers[cw][k] - x_train[j][k])**2 for k in range(D) ]  ))\n",
    "                    if d > d0:\n",
    "                        d0 = d\n",
    "            winc.append((cw, centers[cw], d0 ) )\n",
    "\n",
    "        #for i in range(len(winc)):\n",
    "        #    print (i,  ':', winc[i])\n",
    "        #print (\"end -------------------------------\")\n",
    "\n",
    "        xu = self.get_weights()\n",
    "\n",
    "        #print (xu[0].shape, xu[1].shape)\n",
    "        \n",
    "        for c in range(D):\n",
    "            for m in range(M):\n",
    "                xu[0][c,m] = winc[m][1][c]\n",
    "        \n",
    "        \n",
    "        for m in range(M):\n",
    "            xu[1][m] = winc[m][2]*(.5 + random.random()) \n",
    "            \n",
    "        #print (xu[0])\n",
    "        #print (xu[1])\n",
    "            \n",
    "        self.set_weights(xu )\n",
    "        #print (\"end ==============================\")\n",
    "        \n",
    "    \n",
    "    # F2 method LS layer\n",
    "    def call(self, input):\n",
    "        \n",
    "        #print (\"CALL :\", input.numpy())\n",
    "        isp = input.shape\n",
    "        In1 = tf.transpose(input)\n",
    "        kernel_S, kernel_L  = tf.split(self.kernel,[ self.num_outputs_s + self.num_outputs_r, self.num_outputs_l ], axis = 1 )\n",
    "        bias_S, bias_L  = tf.split(self.bias,[ self.num_outputs_s +  self.num_outputs_r, self.num_outputs_l ], axis = 0 )\n",
    "        \n",
    "        # case spherical\n",
    "        \n",
    "        s_shape  = self.num_outputs_s + self.num_outputs_r\n",
    "        In2 = tf.stack([In1] * s_shape)\n",
    "        InD = tf.transpose(In2)\n",
    "        WD = tf.stack([kernel_S] * isp[0])\n",
    "        ddd = WD - InD\n",
    "        dd0 = tf.math.multiply(ddd, ddd)\n",
    "        dd1 = tf.math.reduce_sum(dd0, axis =1)\n",
    "        dd2 = tf.cast(dd1,tf.double)\n",
    "        dd3 = tf.sqrt(dd2)\n",
    "        d_r = tf.cast(dd3,tf.float32)\n",
    "        d_R = tf.abs(bias_S)\n",
    "        d_rR = tf.math.divide_no_nan(d_r,d_R)\n",
    "        d_x0 = tf.ones(d_rR.shape) - d_rR\n",
    "        result_S = tf.math.scalar_mul(6,d_x0)\n",
    "        result_S = sigmoid(result_S)\n",
    "        #print (\"OUT\")\n",
    "        #print(result_S.numpy())\n",
    "        \n",
    "        # case linear\n",
    "\n",
    "        d_1 = tf.stack([bias_L] * isp[0])\n",
    "        result_L = tf.matmul(input, kernel_L) + d_1 \n",
    "        result_L = relu(result_L)\n",
    "\n",
    "        #case empty, merge\n",
    "        \n",
    "        '''\n",
    "        #print (self.num_outputs_r)\n",
    "        if self.num_outputs_r > 0:\n",
    "            r_S, _ = tf.split (result_S,[self.num_outputs_s, self.num_outputs_r],axis=1 )\n",
    "            r_1 = np.zeros((result_S.shape[0],self.num_outputs_r))\n",
    "            result_R = tf.cast(tf.constant(r_1),tf.float32)\n",
    "            result = tf.concat([r_S, result_R, result_L],axis=1)            \n",
    "            #print (self.num_outputs_s, self.num_outputs_r)\n",
    "            #print (\"result_S\", result_S)\n",
    "            #print (\"result_L\", result_L)\n",
    "            #print (\"result\", result)\n",
    "        else:\n",
    "            result = tf.concat([result_S, result_L],axis=1)        \n",
    "        '''\n",
    "        \n",
    "        result = tf.concat([result_S, result_L],axis=1)        \n",
    "        \n",
    "        return result\n",
    "    \n",
    "        \n",
    "\n",
    "class NN_Model(Model):\n",
    "    \n",
    "    def __init__(self,c,hs,hr,hl,indata):\n",
    "        super(NN_Model, self).__init__()\n",
    "        self.d1 = LSLayer(hs,hr,hl,indata)\n",
    "        self.d2 = Dense(c)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        #print (\"call benn:\",x, tf.math.reduce_sum(x))\n",
    "        return self.d2(x)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "#@tf.function\n",
    "def train_step(datas, labels,modelk,loss_objectk,optimizerk,train_lossk,train_accuracyk):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = modelk(datas, training=True)\n",
    "        loss = loss_objectk(labels, predictions)\n",
    "    gradients = tape.gradient(loss, modelk.trainable_variables)\n",
    "    optimizerk.apply_gradients(zip(gradients, modelk.trainable_variables))\n",
    "\n",
    "    train_lossk(loss)\n",
    "    train_accuracyk(labels, predictions)\n",
    "\n",
    "#@tf.function\n",
    "def test_step(datas, labels,modelk,loss_objectk,test_lossk,test_accuracyk):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    \n",
    "    predictions = modelk(datas, training=False)\n",
    "    t_loss = loss_objectk(labels, predictions)\n",
    "\n",
    "    test_lossk(t_loss)\n",
    "    test_accuracyk(labels, predictions)\n",
    "        \n",
    "\n",
    "    \n",
    "def analyze_NN (model, train_ds, L=0,verbose = 0):\n",
    "    cnt = 0\n",
    "    xu = model.layers[L].get_weights()\n",
    "    C = xu[0].shape[1]\n",
    "    M = xu[0].shape[0]\n",
    "    C = model.d1.num_outputs_s + model.d1.num_outputs_r\n",
    "    #print (C,M)\n",
    "    Cvals = []\n",
    "    for c in range(C):\n",
    "        cnt = 0\n",
    "        outs = []\n",
    "        for datas, labels in train_ds:\n",
    "            dA = datas.numpy()\n",
    "            dL = labels.numpy()\n",
    "            for i in range(dA.shape[0]):\n",
    "                cnt += 1\n",
    "                cat = np.argmax(dL[i]) + 1\n",
    "                tA1 = tf.constant(dA[i])\n",
    "                tA = tf.reshape(tA1,[1,dA.shape[1]])\n",
    "                if L == 1:\n",
    "                    to1 = model.d1 (tA)\n",
    "                    to2 = model.d2 (to1)\n",
    "                else:\n",
    "                    to2 =  model.d1 (tA)\n",
    "                #print (tA.numpy())\n",
    "                #print (to2.numpy())\n",
    "                outs.append((to2[0][c].numpy(), cat))\n",
    "        #print (\"layer\", L, \" node\",  c, \" R=\",xu[1][c])\n",
    "        X = [x[0] for x in outs]\n",
    "        Y = [x[1] for x in outs]\n",
    "        X1 = [x[0] for x in outs if x[1] == 1]\n",
    "        X2 = [x[0] for x in outs if x[1] == 2]\n",
    "        sim_level = ks_2samp(X1,X2).pvalue\n",
    "        if verbose == 1:\n",
    "            plt.scatter(X,Y)\n",
    "            plt.show()\n",
    "            print (sim_level)\n",
    "        Cvals.append((c,sim_level))\n",
    "        \n",
    "    Cvals.sort(key = lambda x : -x[1])\n",
    "    return Cvals\n",
    "    \n",
    "    \n",
    "    \n",
    "def update_NN_model (model, train_ds ):\n",
    "\n",
    "    \n",
    "    rdb = 0\n",
    "    odb = 0\n",
    "    #N = min(5, model.d1.num_outputs_r)   # number of new SSN nodes\n",
    "    M = model.d1.num_outputs_r   # number of updated nodes\n",
    "    if M == 0:\n",
    "        return \n",
    "    \n",
    "    Clist = analyze_NN (model, train_ds)\n",
    "\n",
    "    baditems = []\n",
    "    yitems = []\n",
    "    for datas, labels in train_ds:\n",
    "        predictions = model(datas, training=False)\n",
    "        for i in range(datas.shape[0]):\n",
    "            #print (datas.numpy()[i], predictions.numpy()[i], np.argmax(predictions.numpy()[i]), labels.numpy()[i],np.argmax(labels.numpy()[i]))\n",
    "            if np.argmax(predictions.numpy()[i]) != np.argmax(labels.numpy()[i]):\n",
    "                rdb = rdb + 1\n",
    "                baditems.append(datas.numpy()[i])\n",
    "                yitems.append(labels.numpy()[i])\n",
    "            odb = odb + 1        \n",
    "\n",
    "    N = len(baditems)\n",
    "    C = min(2*M, len(baditems))  # number of cluster centers\n",
    "    if C == 0:\n",
    "        return\n",
    "    \n",
    "    print (\"update layer\")\n",
    "\n",
    "    # k-means\n",
    "\n",
    "    cls = KMeans(n_clusters=C).fit(baditems)\n",
    "        \n",
    "    centers = cls.cluster_centers_\n",
    "    labels = cls.labels_\n",
    "    D = centers.shape[1]\n",
    "\n",
    "    cvals = []\n",
    "    for c in range(C):\n",
    "        db1 = 0\n",
    "        db2 = 0\n",
    "        for i in range(N):\n",
    "            if labels[i] == c:\n",
    "                if yitems[i][0] == 1:\n",
    "                    db1 += 1\n",
    "                else:\n",
    "                    db2 += 1\n",
    "        try:\n",
    "            h = -math.log(db1/(db1+db2))*db1/(db1+db2) - math.log(db2/(db1+db2))*db2/(db1+db2) \n",
    "        except:\n",
    "            h = 0\n",
    "        #print (\"class:\", c,db1 + db2, db1, db2,h)\n",
    "        cvals.append((c,(db1+db2)* (1 - h)))\n",
    "\n",
    "    cvals.sort(key = lambda x: x[1] )\n",
    "\n",
    "    winc = []\n",
    "    #print (M)\n",
    "    for i in range(M):\n",
    "        cw = cvals[-i][0]\n",
    "        d0 = 0\n",
    "        for j in range(N):\n",
    "            if labels[j] == cw:\n",
    "                d = math.sqrt(sum([ (centers[cw][k] - baditems[j][k])**2 for k in range(D) ]  ))\n",
    "                if d > d0:\n",
    "                    d0 = d\n",
    "        winc.append((cw, centers[cw], d0 ) )\n",
    "\n",
    "    \n",
    "    xu = model.d1.get_weights()\n",
    "\n",
    "\n",
    "    for c in range(D):    # input dim\n",
    "        for m in range(M):   # neuron sorszam\n",
    "            nn = Clist[m][0]\n",
    "            xu[0][c,nn] = winc[m][1][c]\n",
    "\n",
    "\n",
    "    for m in range(M):\n",
    "        nn = Clist[m][0]\n",
    "        xu[1][nn] = winc[m][2]*(.5 + random.random())\n",
    "\n",
    "    #print (xu[0])\n",
    "    #print (xu[1])\n",
    "\n",
    "    model.d1.set_weights(xu )\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "K = 10\n",
    "\n",
    "# (x_train,y_train,x_test,y_test) = gen_data_array_cv(K)\n",
    "\n",
    "M = x_train[0].shape[1]\n",
    "C = y_train[0].shape[1]\n",
    "\n",
    "EPOCHS = 300\n",
    "Eupd = [20,100]\n",
    "\n",
    "H = M * 5\n",
    "HS = 0\n",
    "HR = 0\n",
    "HL = 95\n",
    "B = 32\n",
    "\n",
    "model = []\n",
    "loss_object =  []\n",
    "optimizer = []\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "val_loss = []\n",
    "val_accuracy = []\n",
    "train_ds = []\n",
    "val_ds = []\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(B)\n",
    "\n",
    "\n",
    "\n",
    "best_v = 0\n",
    "best_t = 0\n",
    "\n",
    "print (\"N=\", x_train[0].shape[0])\n",
    "\n",
    "for k in range(K):\n",
    "    print (\"k=\",k, \"------------------------------\")\n",
    "    # Create an instance of the model\n",
    "    model.append( NN_Model(C,HS,HR,HL,(x_train[k], y_train[k])))\n",
    "    #model.append( NN_Model(H,C,0,0))\n",
    "\n",
    "    loss_object.append(tf.keras.losses.CategoricalCrossentropy(from_logits=True))\n",
    "    #loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "    #optimizer.append(tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07,))\n",
    "    optimizer.append(tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07,))\n",
    "    train_loss.append(tf.keras.metrics.Mean(name='train_loss'))\n",
    "    train_accuracy.append(tf.keras.metrics.CategoricalAccuracy(name='train_accuracy'))\n",
    "\n",
    "    test_loss.append(tf.keras.metrics.Mean(name='test_loss'))\n",
    "    test_accuracy.append(tf.keras.metrics.CategoricalAccuracy(name='test_accuracy'))\n",
    "    \n",
    "    val_loss.append(tf.keras.metrics.Mean(name='val_loss'))\n",
    "    val_accuracy.append(tf.keras.metrics.CategoricalAccuracy(name='val_accuracy'))\n",
    "    \n",
    "\n",
    "    #print (x_train[:2])\n",
    "    train_ds.append( tf.data.Dataset.from_tensor_slices(\n",
    "        (x_train[k], y_train[k])).batch(B))\n",
    "    val_ds.append( tf.data.Dataset.from_tensor_slices(\n",
    "        (x_val[k], y_val[k])).batch(B))\n",
    "    #print (train_ds)\n",
    "\n",
    "\n",
    "    ##set_circles(model[k],x_train[k], y_train[k], HS+HR)\n",
    "\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for epoch in range(EPOCHS):\n",
    "      # Reset the metrics at the start of the next epoch\n",
    "\n",
    "        train_loss[k].reset_states()\n",
    "        train_accuracy[k].reset_states()\n",
    "        test_loss[k].reset_states()\n",
    "        test_accuracy[k].reset_states()\n",
    "        \n",
    "\n",
    "        for datas, labels in train_ds[k]:\n",
    "            train_step(datas, labels,model[k],loss_object[k],optimizer[k],train_loss[k],train_accuracy[k])\n",
    "\n",
    "\n",
    "        for test_datas, test_labels in test_ds:\n",
    "            #tpredictions = model[k](test_datas, training=False)\n",
    "            test_step(test_datas, test_labels,model[k],loss_object[k],test_loss[k],test_accuracy[k])\n",
    "\n",
    "        for val_datas, val_labels in val_ds[k]:\n",
    "            #vpredictions = model[k](val_datas, training=False)\n",
    "            test_step(val_datas, val_labels,model[k],loss_object[k],val_loss[k],val_accuracy[k])\n",
    "            \n",
    "            \n",
    "        \n",
    "        #if (k == 0 or k == 8)  and epoch == 75:\n",
    "        #    analyze_NN(model[k], train_ds[k])\n",
    "    \n",
    "        if epoch in Eupd:\n",
    "            update_NN_model (model[k], train_ds[k])\n",
    "\n",
    "\n",
    "        X.append(epoch)\n",
    "        Y.append(test_accuracy[k].result() * 100)\n",
    "        if epoch % 20 == 0:\n",
    "            #print(\n",
    "            #    f'Epoch {epoch + 1}, '\n",
    "            #    f'Loss: {train_loss[k].result()}, '\n",
    "            #    f'Val Accuracy: {val_accuracy[k].result() * 100}, '\n",
    "            #    f'Test Accuracy: {test_accuracy[k].result() * 100}'\n",
    "            #  )    \n",
    "            \n",
    "            if val_accuracy[k].result() > best_v:\n",
    "                best_v = val_accuracy[k].result().numpy()\n",
    "                best_t = test_accuracy[k].result().numpy()\n",
    "                print(\"XX\", best_v,best_t)\n",
    "            #print(model.d1.bias.numpy())\n",
    "\n",
    "\n",
    "    #print(\n",
    "    #    f'Epoch {epoch + 1}, '\n",
    "    #    f'Loss: {train_loss[k].result()}, '\n",
    "    #    f'Val Accuracy: {val_accuracy[k].result() * 100}, '\n",
    "    #    f'Test Accuracy: {test_accuracy[k].result() * 100}'\n",
    "    #  )    \n",
    "    #acclist.append(test_accuracy[k].result())\n",
    "    print (\"accuracy:\",best_t)\n",
    "    #plt.plot(X, Y,label=\"Accuracy curve\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.64 1.4028542333400131\n"
     ]
    }
   ],
   "source": [
    "import statistics \n",
    "\n",
    "x = [80.2,80.2, 82.8, 79, 81]\n",
    "print (sum(x)/len(x), statistics.stdev(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 0\n",
      "[0.5445604773535244, 0.7246377] [0.5801433622837067, 0.69736844]\n",
      "[0.475445914959562, 0.7826087] [0.535427672298331, 0.7368421]\n",
      "[0.4424303104912025, 0.8115942] [0.5056477392974653, 0.7631579]\n",
      "k= 1\n",
      "k= 2\n",
      "k= 3\n",
      "k= 4\n",
      "k= 5\n",
      "k= 6\n",
      "k= 7\n",
      "k= 8\n",
      "[0.4281147232522135, 0.8405797] [0.5255143140491686, 0.7631579]\n",
      "k= 9\n",
      "acc= 0.7631579 ( 0.8405797 )\n"
     ]
    }
   ],
   "source": [
    "# 10-fold coross validation \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "\n",
    "\n",
    "K = 10\n",
    "\n",
    "#(x_train,y_train,x_test,y_test) = gen_data_array_cv(K)\n",
    "\n",
    "best_v = 0\n",
    "best_t = 0\n",
    "\n",
    "for k in range(K):\n",
    "    print (\"k=\",k)\n",
    "    N = x_train[k].shape[0]\n",
    "    M = x_train[k].shape[1]\n",
    "    C = y_train[k].shape[1]\n",
    "    H = M * 5\n",
    "    E = 80\n",
    "\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(H, input_shape=(M,), activation='relu'),\n",
    "      tf.keras.layers.Dense(C)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=loss_fn,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    es = 20\n",
    "    #loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    for e in range(int(E/es)):\n",
    "        model.fit(x_train[k], y_train[k], validation_data=[x_val[k],y_val[k]], epochs=es,verbose = 0)\n",
    "        res_test = model.evaluate(x_test,  y_test, verbose=0)\n",
    "        res_val = model.evaluate(x_val[k],  y_val[k], verbose=0)\n",
    "        if res_val[1] > best_v:\n",
    "            best_v = res_val[1]\n",
    "            best_t = res_test[1]\n",
    "            print (res_val, res_test)\n",
    "            \n",
    "print (\"acc=\", best_t, \"(\",best_v,\")\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.03999999999999 1.0597169433391156\n"
     ]
    }
   ],
   "source": [
    "import statistics \n",
    "\n",
    "x = [75.1,76.2, 77.6,75.0,76.3]\n",
    "print (sum(x)/len(x), statistics.stdev(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data (768, 9)\n"
     ]
    }
   ],
   "source": [
    "# read benchmark data\n",
    "\n",
    "from pandas import  read_csv\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "def gen_data_array_cv(K):\n",
    "    \n",
    "    \n",
    "    dataT = read_csv (\"pima-indians-diabetes.csv\", header =None)\n",
    "    dataA = np.array(dataT)    \n",
    "        \n",
    "    N = dataA.shape[0]\n",
    "    print (\"Data\",dataA.shape)\n",
    "    M = dataA.shape[1]-1\n",
    "\n",
    "    Nte = int(0.1*N)\n",
    "    Ntr = N - Nte\n",
    "\n",
    "    cval = dict()\n",
    "    for i in range(N):\n",
    "        cval[dataA[i][-1]] = 1\n",
    "    C = len(cval.keys())\n",
    "    \n",
    "    minv = [0 for _ in range(M)]\n",
    "    maxv = [0 for _ in range(M)]\n",
    "    \n",
    "    for j in range(M):\n",
    "        minv[j] =  dataA[0,j]\n",
    "        maxv[j] =  dataA[0,j]\n",
    "        \n",
    "    for i in range(N):\n",
    "        for j in range(M):\n",
    "            minv[j] = min( dataA[i,j] , minv[j])\n",
    "            maxv[j] = max( dataA[i,j] , maxv[j])\n",
    "          \n",
    "    orders = random.sample(range(N), N)\n",
    "    \n",
    "    x2_test = np.zeros((Nte,M),dtype='float32')\n",
    "    y2_test = np.zeros((Nte,C))\n",
    "    \n",
    "    ite = 0\n",
    "    for n in range(Ntr, N):\n",
    "        i = orders[n]\n",
    "        for j in range(M):\n",
    "            x2_test[ite,j] = (dataA[i,j] - minv[j]) / (maxv[j] - minv[j])\n",
    "        y2_test[ite, int(dataA[i,M])] = 1\n",
    "        ite += 1    \n",
    "       \n",
    "    Ntrv = int(Ntr/K)\n",
    "    Ntrt = Ntr - Ntrv\n",
    "    \n",
    "    x2_train = []\n",
    "    y2_train = []\n",
    "    x2_val = []\n",
    "    y2_val = []\n",
    "    \n",
    "    for k in range(K):\n",
    "        x2_train.append(np.zeros((Ntrt,M),dtype='float32'))\n",
    "        y2_train.append(np.zeros((Ntrt,C)))\n",
    "        x2_val.append(np.zeros((Ntrv,M),dtype='float32'))\n",
    "        y2_val.append(np.zeros((Ntrv,C)))\n",
    "    \n",
    "        itr = 0\n",
    "        ite = 0\n",
    "        for n in range(Ntr):\n",
    "            i = orders[n]\n",
    "            if n >= k*Ntrv and n < (k+1)*Ntrv:\n",
    "                for j in range(M):\n",
    "                    x2_val[k][ite,j] = (dataA[i,j] - minv[j]) / (maxv[j] - minv[j])\n",
    "                y2_val[k][ite, int(dataA[i,M])] = 1\n",
    "                ite += 1\n",
    "            else:            \n",
    "                for j in range(M):\n",
    "                    x2_train[k][itr,j] = (dataA[i,j] - minv[j]) / (maxv[j] - minv[j])\n",
    "                y2_train[k][itr,int(dataA[i,M])] = 1\n",
    "                itr += 1\n",
    "                \n",
    "    return (x2_train,y2_train, x2_val, y2_val, x2_test, y2_test)\n",
    "\n",
    "K = 10\n",
    "(x_train,y_train,x_val,y_val, x_test,y_test) = gen_data_array_cv(K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ks_2sampResult(statistic=1.0, pvalue=0.05714285714285727) 0.05714285714285727\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "X = [1,2,4,6.7]\n",
    "Y = [25,36,61]\n",
    "\n",
    "test = ks_2samp(X,Y)\n",
    "print(test, test.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
