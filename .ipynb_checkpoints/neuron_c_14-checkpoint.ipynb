{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 0 ------------------------------\n",
      "XX 0.3188406 0.47368422\n",
      "[0.1764706  0.49748743 0.6557377  0.11111111 0.07565012 0.2876304\n",
      " 0.08795901 0.15      ] [[1.575108  0.7671416]] [1. 0.]\n",
      "[[0.8153558  0.5839426  0.5767977  0.6374625  0.8666568  0.41474405\n",
      "  0.76020616 0.6358555  0.6171553  0.6306263  0.6792692  0.5004312\n",
      "  0.71275735 0.48701566 0.5668278  0.8682275  2.2207947  2.5843768\n",
      "  1.7812259  1.3558127  1.828123   2.5618439  1.8417774  1.8029422\n",
      "  1.2477342  2.3064904  2.834803   2.2609591  1.9420176  2.4444392\n",
      "  1.5486091  2.2821596  1.3504708  1.5863352  1.6766706  2.5334852\n",
      "  1.54096    1.9653355  1.9022169  2.2191048  1.9843372  2.5908024\n",
      "  1.9675932  2.370582   2.2832959  2.3257012  2.0202944  1.7419354\n",
      "  2.9928591  1.304437   1.7579466  2.0637932  1.4514133  1.2470984\n",
      "  2.2099564  3.054634  ]]\n",
      "[[1.575108  0.7671416]]\n",
      "-----------\n",
      "[0.1764706  0.79396987 0.52459013 0.13131313 0.4574468  0.46497765\n",
      " 0.09265585 0.05      ] [[1.2618501  0.53175485]] [1. 0.]\n",
      "[[0.57177395 0.42101055 0.3812856  0.38346565 0.6987577  0.39291513\n",
      "  0.6931536  0.5306019  0.37456423 0.62088865 0.56076974 0.36450386\n",
      "  0.58716834 0.2636596  0.29236537 0.73687726 2.3275244  2.3398323\n",
      "  1.5702815  1.1126401  1.9862506  2.2846184  2.2288122  1.7881457\n",
      "  1.1308756  2.061412   2.2837338  2.1555448  1.7725421  2.4764144\n",
      "  1.8991888  1.6907415  1.1769392  2.15824    1.7879868  2.7552025\n",
      "  1.6255959  1.9316764  1.7536647  2.7896922  2.2957826  3.2907333\n",
      "  1.8688552  2.4747574  2.5603414  1.8444837  2.114145   2.0493908\n",
      "  2.8077722  1.6116252  1.9364368  2.2852905  1.1905698  1.215691\n",
      "  1.870224   2.8602602 ]]\n",
      "[[1.2618501  0.53175485]]\n",
      "-----------\n",
      "[0.1764706  0.44723618 0.60655737 0.16161616 0.10047282 0.45305514\n",
      " 0.20196414 0.28333333] [[1.5330054 0.6579229]] [1. 0.]\n",
      "[[0.8338121  0.5880521  0.5579235  0.65235054 0.83800596 0.4356919\n",
      "  0.66814494 0.5954593  0.6218054  0.6011192  0.5931881  0.4365005\n",
      "  0.76376903 0.5369513  0.5673999  0.7980424  1.846565   2.4311054\n",
      "  1.6596113  1.4302541  2.107532   2.4418464  1.9492637  1.8737704\n",
      "  1.2999988  2.358896   2.8298821  2.535509   1.7454538  2.3931012\n",
      "  1.6938515  2.2114787  1.3528306  1.6367261  1.593641   2.7341723\n",
      "  1.6443738  2.0646598  1.9073048  2.383431   2.2576838  2.5065827\n",
      "  1.9660807  2.165418   2.4674435  2.3977075  2.3083677  1.6865196\n",
      "  3.0766158  1.3051485  1.8332868  2.0299585  1.5605066  1.2507142\n",
      "  2.163644   3.5204563 ]]\n",
      "[[1.5330054 0.6579229]]\n",
      "-----------\n",
      "[0.29411766 0.63316584 0.6393443  0.27272728 0.02600473 0.44113263\n",
      " 0.15414175 0.31666666] [[1.2103523 0.5516591]] [1. 0.]\n",
      "[[0.6721912  0.49641663 0.50932807 0.51212686 0.7923212  0.3951028\n",
      "  0.5961787  0.51951396 0.5060398  0.48531055 0.5175853  0.4130452\n",
      "  0.64963436 0.5541469  0.47988838 0.7139154  1.7677271  2.3396823\n",
      "  1.4200248  1.3540051  2.1373944  2.7338595  1.8476825  1.7669405\n",
      "  1.2442716  2.192873   2.8023934  2.3304067  1.8113432  2.4436936\n",
      "  1.5897998  2.406965   1.3283598  1.6504012  1.4291636  2.564156\n",
      "  1.7738063  1.9916315  1.9095687  2.1743045  1.969956   2.552548\n",
      "  1.770395   2.2696443  2.4116557  2.3326654  2.3184323  1.6021961\n",
      "  3.1931853  1.28399    1.8407791  1.8893878  1.6146373  1.2657168\n",
      "  2.1734178  3.5877817 ]]\n",
      "[[1.2103523 0.5516591]]\n",
      "-----------\n",
      "[0.7058824  0.7035176  0.6721311  0.43434343 0.38416076 0.5842027\n",
      " 0.19214347 0.6166667 ] [[ 0.3200922 -0.1731527]] [0. 1.]\n",
      "[[0.27970842 0.37345582 0.34845096 0.20101887 0.45218349 0.37161052\n",
      "  0.2744813  0.2622692  0.21923861 0.33898342 0.30049008 0.37143984\n",
      "  0.34759486 0.5021871  0.22372288 0.33958566 1.1460798  2.1940174\n",
      "  0.60634446 1.2282126  2.3440962  2.3144238  2.2375202  1.4701215\n",
      "  1.2782489  2.1788006  2.702076   2.4365683  1.9251183  2.7838407\n",
      "  1.9958842  1.9654466  1.3332428  2.229031   1.073956   2.2102761\n",
      "  1.8243155  2.0613499  1.829426   2.6139522  1.7415127  2.3796155\n",
      "  1.340577   2.600794   2.4678042  2.4515162  2.645309   1.6851802\n",
      "  3.1752567  1.6012161  1.815721   1.4026229  1.8530394  1.2751198\n",
      "  1.5818124  4.39186   ]]\n",
      "[[ 0.3200922 -0.1731527]]\n",
      "-----------\n",
      "[0.         0.68844223 0.32786885 0.35353535 0.19858156 0.64232486\n",
      " 0.9436379  0.2       ] [[1.495107  0.4861195]] [0. 1.]\n",
      "[[0.57954085 0.37412065 0.35640562 0.6245319  0.6046961  0.39541703\n",
      "  0.29885134 0.45077318 0.4437924  0.42198652 0.35479546 0.17494357\n",
      "  0.47431633 0.05849153 0.20745018 0.49249628 1.0835743  1.402926\n",
      "  1.3759822  1.4796009  2.64149    2.4918568  2.0439982  2.2584512\n",
      "  1.4084591  2.3862453  2.3347852  2.4369712  1.3028512  1.388205\n",
      "  2.007114   2.1001945  0.9802324  2.0928783  1.8296851  3.0478086\n",
      "  2.054153   1.8347979  2.1092196  2.725472   4.0769286  3.518111\n",
      "  1.9633412  0.7643156  2.6558738  2.2991533  2.9626875  2.038367\n",
      "  3.7183483  1.5261083  1.8620657  2.4340415  1.3859407  1.4582305\n",
      "  1.7699223  3.865835  ]]\n",
      "[[1.495107  0.4861195]]\n",
      "-----------\n",
      "[0.47058824 0.49748743 0.6885246  0.         0.         0.5275708\n",
      " 0.1323655  0.48333332] [[1.221187   0.23111339]] [1. 0.]\n",
      "[[0.6570064  0.56211466 0.4839075  0.49706426 0.75002074 0.38468778\n",
      "  0.48648247 0.37188834 0.4326387  0.5577726  0.4561549  0.43697184\n",
      "  0.77133393 0.5340198  0.4303367  0.59915817 1.5729715  2.895528\n",
      "  1.3217374  1.2000651  2.213585   2.6202416  1.8581417  1.6717594\n",
      "  1.1640117  2.7634103  3.0031495  2.6259897  1.9858716  2.2683198\n",
      "  1.5786159  2.2947798  1.4081032  1.6667352  1.1375233  2.4959931\n",
      "  1.534395   2.3529317  1.8940161  2.199444   1.9407475  2.2305896\n",
      "  1.7470555  2.3867736  2.4679418  2.5589814  2.5021138  1.472049\n",
      "  3.1269605  1.1347106  1.8040749  1.646417   1.5649867  1.2698783\n",
      "  2.1171064  4.0483737 ]]\n",
      "[[1.221187   0.23111339]]\n",
      "-----------\n",
      "[0.23529412 0.5527638  0.5409836  0.         0.         0.47540984\n",
      " 0.1678053  0.13333334] [[1.5955052 0.7663785]] [1. 0.]\n",
      "[[0.78456604 0.5645524  0.54029906 0.6553367  0.8471103  0.36722618\n",
      "  0.7479298  0.579745   0.6008251  0.6697749  0.5748468  0.41691726\n",
      "  0.8257983  0.3994905  0.48205033 0.78449494 2.1760325  2.682273\n",
      "  1.7447999  1.2780669  2.1165907  2.7016692  1.7400422  1.8752244\n",
      "  1.1755477  2.5237403  2.715534   2.4545643  1.8108538  2.158022\n",
      "  1.5102897  2.3216972  1.3314416  1.6473016  1.5227308  2.7024102\n",
      "  1.5942384  2.133944   1.8263209  2.2741961  2.4024825  2.863287\n",
      "  1.9163902  2.18247    2.4621594  2.127671   2.265761   1.6894363\n",
      "  3.0684896  1.2008882  1.8289523  2.1629164  1.3282201  1.3210398\n",
      "  2.2175257  3.1585915 ]]\n",
      "[[1.5955052 0.7663785]]\n",
      "-----------\n",
      "[0.23529412 0.86934674 0.57377046 0.14141414 0.19858156 0.44262296\n",
      " 0.12083689 0.2       ] [[1.2519603  0.45337433]] [0. 1.]\n",
      "[[0.5415732  0.4093917  0.38318974 0.39864713 0.727652   0.3919647\n",
      "  0.59429145 0.4556677  0.36067075 0.5274335  0.5212074  0.3680764\n",
      "  0.61018336 0.31038427 0.34376734 0.6936967  2.0060143  2.3900723\n",
      "  1.4446707  1.0860467  2.0681505  2.6914196  2.0198731  1.7404836\n",
      "  1.0800731  2.1821048  2.4476502  2.065755   1.8669927  2.2389429\n",
      "  1.7149905  2.0858684  1.1911476  1.9679879  1.5648491  2.6497889\n",
      "  1.7001745  2.0016284  1.8633558  2.3353972  2.1661658  3.0342839\n",
      "  1.7900769  2.333221   2.4837368  2.0807502  2.2218966  1.7881383\n",
      "  3.0055013  1.384934   1.9400691  2.0525858  1.2874724  1.2484555\n",
      "  2.0664613  3.2901483 ]]\n",
      "[[1.2519603  0.45337433]]\n",
      "-----------\n",
      "[0.11764706 0.53266335 0.45901638 0.27272728 0.19503546 0.43219075\n",
      " 0.14859095 0.01666667] [[1.4933213 1.031852 ]] [1. 0.]\n",
      "[[0.79894626 0.5427534  0.5789286  0.6338039  0.84676254 0.39224976\n",
      "  0.82415795 0.7317431  0.67572105 0.63747346 0.62819314 0.41982612\n",
      "  0.701964   0.4712661  0.5193099  0.844851   2.3912017  2.1645062\n",
      "  1.8187861  1.5128739  2.0498075  2.442463   1.9083327  1.9246383\n",
      "  1.3297698  1.9632778  2.4877193  2.3668075  1.6184833  2.5999303\n",
      "  1.7236009  2.1848629  1.3361335  1.7809527  1.8417525  2.7586622\n",
      "  1.7575074  1.824909   1.7455295  2.5366893  2.428321   3.0401375\n",
      "  1.9291611  2.2645905  2.4402845  1.9142698  2.073246   1.8836603\n",
      "  2.9283586  1.4780796  1.8640864  2.3591933  1.4738883  1.3107493\n",
      "  2.123296   2.7792537 ]]\n",
      "[[1.4933213 1.031852 ]]\n",
      "-----------\n",
      "[0.         0.7638191  0.6721311  0.3939394  0.321513   0.6184799\n",
      " 0.08198121 0.1       ] [[1.094471  0.5117754]] [1. 0.]\n",
      "[[0.59032387 0.39269114 0.3929293  0.34370923 0.71702796 0.36940914\n",
      "  0.5520778  0.5553685  0.36831033 0.43605208 0.4329521  0.22610801\n",
      "  0.52412194 0.3647766  0.21074724 0.662878   2.0546823  2.0222843\n",
      "  1.6090565  1.3172545  2.2016766  2.4153516  2.1946578  1.9863398\n",
      "  1.236453   1.6873724  2.6125555  2.466582   1.352382   2.8030126\n",
      "  1.727596   1.8901024  1.1623573  1.6680981  1.7265466  3.1870553\n",
      "  1.912346   1.8476804  1.8501062  2.8058472  1.8931124  3.0483859\n",
      "  1.9956     2.3209527  2.8423657  1.8456649  2.338964   1.8937492\n",
      "  3.176656   1.5213605  2.01413    2.3867848  1.4272019  1.0467968\n",
      "  2.1240346  3.02511   ]]\n",
      "[[1.094471  0.5117754]]\n",
      "-----------\n",
      "[0.5294118  0.7035176  0.7704918  0.         0.         0.48733234\n",
      " 0.2801025  0.4       ] [[ 1.0433295e+00 -5.5675209e-04]] [0. 1.]\n",
      "[[0.50481355 0.453013   0.4016689  0.42558378 0.66827863 0.33794758\n",
      "  0.42629838 0.30506635 0.32064945 0.500427   0.40865797 0.37819186\n",
      "  0.6458218  0.29983923 0.27995718 0.5373991  1.4430528  2.8275487\n",
      "  1.0943617  0.99236643 2.178009   2.8361664  1.782764   1.6776955\n",
      "  1.0919067  2.8461337  3.0147536  2.3701108  2.0797586  1.9496789\n",
      "  1.44429    2.2983952  1.2279797  1.7299619  1.061024   2.3462875\n",
      "  1.5694373  2.2571733  2.0172918  2.1650417  2.0193462  2.5864005\n",
      "  1.6529564  2.1485493  2.4200168  2.5244443  2.6281767  1.6082772\n",
      "  3.4767835  1.1747688  1.6947049  1.667806   1.3833158  1.2812287\n",
      "  1.9394122  3.94293   ]]\n",
      "[[ 1.0433295e+00 -5.5675209e-04]]\n",
      "-----------\n",
      "[0.11764706 0.4723618  0.6229508  0.18181819 0.07801419 0.4709389\n",
      " 0.24380872 0.03333334] [[1.5273209 0.824716 ]] [1. 0.]\n",
      "[[0.8377265  0.5604663  0.58152544 0.6846449  0.8527551  0.3621174\n",
      "  0.77371603 0.683296   0.6631345  0.62854576 0.58325094 0.3673776\n",
      "  0.74540323 0.40674403 0.4345293  0.8268035  2.169765   2.3760974\n",
      "  1.7951112  1.4366832  2.0806537  2.5454705  1.7923898  2.0168052\n",
      "  1.3176385  2.2653096  2.8552063  2.5485997  1.6105694  2.3951793\n",
      "  1.4932287  2.2499619  1.2788193  1.4936798  1.6748011  2.825411\n",
      "  1.6995943  1.9294544  1.8899131  2.504772   2.3357773  2.9186976\n",
      "  2.0141125  2.0428226  2.5163615  2.0875764  2.2977915  1.8417405\n",
      "  3.28779    1.3403915  1.7534579  2.342969   1.4140241  1.2453666\n",
      "  2.145225   2.9163566 ]]\n",
      "[[1.5273209 0.824716 ]]\n",
      "-----------\n",
      "[0.11764706 0.53768843 0.60655737 0.3030303  0.11820331 0.5007452\n",
      " 0.13919726 0.03333334] [[1.3474319 0.8697292]] [1. 0.]\n",
      "[[0.7819896  0.52242625 0.56741667 0.58956605 0.8382274  0.35628992\n",
      "  0.7695483  0.69395804 0.6251767  0.5712607  0.5521131  0.34824523\n",
      "  0.6878639  0.4985602  0.40627208 0.8025662  2.253273   2.2335002\n",
      "  1.7560573  1.4785948  2.1085794  2.5520914  1.8445421  1.9783657\n",
      "  1.3232532  1.9761487  2.760261   2.5165145  1.5289689  2.668895\n",
      "  1.5369505  2.2507877  1.3029586  1.5218763  1.6865715  2.8683233\n",
      "  1.804852   1.856613   1.818254   2.536055   2.109025   2.9310436\n",
      "  1.9534837  2.2374482  2.563387   1.9428596  2.230075   1.8201872\n",
      "  3.1971226  1.397687   1.8304443  2.3475032  1.4970572  1.2147022\n",
      "  2.1798117  2.848936  ]]\n",
      "[[1.3474319 0.8697292]]\n",
      "-----------\n",
      "[0.3529412  0.74371856 0.59016395 0.35353535 0.         0.5007452\n",
      " 0.23441502 0.48333332] [[1.0410141  0.36126018]] [0. 1.]\n",
      "[[0.52962255 0.43250474 0.4356161  0.42071503 0.7143619  0.4073645\n",
      "  0.41087967 0.40336084 0.39907295 0.38041162 0.42082992 0.3639086\n",
      "  0.56902826 0.5005774  0.42779207 0.5665345  1.369405   2.1465306\n",
      "  1.1732084  1.3366848  2.32414    2.837363   1.9003328  1.715929\n",
      "  1.22696    2.1728008  2.7002425  2.2764945  1.7935461  2.3010616\n",
      "  1.7061119  2.4787498  1.304849   1.7911075  1.3100669  2.5438693\n",
      "  1.9023561  2.025465   1.945962   2.069864   2.110489   2.4884384\n",
      "  1.6490855  2.1330445  2.4395676  2.451817   2.4937367  1.4963257\n",
      "  3.2530718  1.2661034  1.9208058  1.7120275  1.7160835  1.3075174\n",
      "  2.1496327  4.0979514 ]]\n",
      "[[1.0410141  0.36126018]]\n",
      "-----------\n",
      "[0.4117647  0.53768843 0.60655737 0.         0.         0.44113263\n",
      " 0.07514945 0.16666667] [[1.3495389 0.6608066]] [0. 1.]\n",
      "[[0.712008   0.5648775  0.5529566  0.57878596 0.82163584 0.33900678\n",
      "  0.7515761  0.5365064  0.55555505 0.66038245 0.5559865  0.4733596\n",
      "  0.78945684 0.49014845 0.45753464 0.74971944 2.2188423  2.8576589\n",
      "  1.5834901  1.2317579  2.042336   2.724308   1.6824641  1.7354693\n",
      "  1.1645385  2.5620947  2.822602   2.4390092  1.9689983  2.344252\n",
      "  1.4373274  2.3632572  1.3920608  1.6544585  1.355419   2.4498758\n",
      "  1.538475   2.1538737  1.7830797  2.222599   2.039341   2.7195086\n",
      "  1.7676111  2.458616   2.3662033  2.1463194  2.2197323  1.6506093\n",
      "  3.05478    1.2087026  1.7492335  1.9831268  1.3940995  1.3270127\n",
      "  2.1392882  3.1681705 ]]\n",
      "[[1.3495389 0.6608066]]\n",
      "-----------\n",
      "[0.7058824  0.5025126  0.6885246  0.33333334 0.12411348 0.4470939\n",
      " 0.17506404 0.41666666] [[0.6368946 0.2732278]] [1. 0.]\n",
      "[[0.49581978 0.4992057  0.53366697 0.39880162 0.6579379  0.33606482\n",
      "  0.5356306  0.42300868 0.43945426 0.46777365 0.42624855 0.4969282\n",
      "  0.5152885  0.6456292  0.39083254 0.5518073  1.5937011  2.46931\n",
      "  0.94793797 1.3691235  2.1539216  2.561259   1.8060867  1.5482801\n",
      "  1.3378423  2.362088   2.960052   2.4711137  2.0291808  2.7242103\n",
      "  1.6175618  2.374131   1.4365869  1.8448985  1.1030054  2.0378373\n",
      "  1.7171367  2.0130465  1.8221138  2.3196115  1.7260063  2.3555815\n",
      "  1.4197488  2.5495572  2.244484   2.417497   2.4441314  1.6346058\n",
      "  3.2779827  1.4257779  1.6250545  1.5428561  1.8259602  1.3595688\n",
      "  1.7788308  3.8280404 ]]\n",
      "[[0.6368946 0.2732278]]\n",
      "-----------\n",
      "[0.47058824 0.6683417  0.59016395 0.         0.         0.49031296\n",
      " 0.08198121 0.3       ] [[1.2163906  0.44973254]] [0. 1.]\n",
      "[[0.58778006 0.50550485 0.4692733  0.47348747 0.7612834  0.34860215\n",
      "  0.6175012  0.418945   0.43141925 0.59035474 0.4836072  0.4401567\n",
      "  0.757094   0.45456675 0.40701762 0.64112604 1.9599245  2.8338218\n",
      "  1.386736   1.1245053  2.1554492  2.8096535  1.7551786  1.6573896\n",
      "  1.0900402  2.593123   2.7289984  2.3654048  2.007436   2.222077\n",
      "  1.5198413  2.353097   1.3628714  1.7917845  1.231623   2.4385998\n",
      "  1.5846128  2.2345588  1.8000658  2.1431844  2.0444758  2.6790555\n",
      "  1.6732605  2.4562247  2.4108496  2.2175074  2.330094   1.5640508\n",
      "  3.0420656  1.1781408  1.8333415  1.8238579  1.4031174  1.3327713\n",
      "  2.1253796  3.5486317 ]]\n",
      "[[1.2163906  0.44973254]]\n",
      "-----------\n",
      "[0.05882353 0.35678393 0.39344263 0.18181819 0.08983452 0.30402383\n",
      " 0.10461144 0.01666667] [[1.8383048 1.3096889]] [1. 0.]\n",
      "[[0.8971212  0.6512904  0.6759378  0.77276003 0.91472733 0.4289135\n",
      "  0.8756795  0.8012003  0.80509245 0.7117374  0.73988485 0.52264816\n",
      "  0.7771667  0.5121531  0.7065791  0.9168856  2.5857208  2.3122137\n",
      "  2.1032057  1.6525453  1.9014131  2.4274049  1.8031172  1.9174527\n",
      "  1.3737048  2.082303   2.5164795  2.34933    1.7409152  2.5575206\n",
      "  1.7119305  2.3497627  1.4591405  1.6858014  1.9727787  2.6696067\n",
      "  1.6301223  1.8541421  1.729519   2.3153756  2.5348845  2.803057\n",
      "  2.0369055  2.291888   2.2568645  2.070358   1.8471799  1.7967921\n",
      "  2.7034864  1.3794297  1.8398725  2.3411398  1.5261979  1.3761882\n",
      "  2.2974749  2.7002673 ]]\n",
      "[[1.8383048 1.3096889]]\n",
      "-----------\n",
      "[0.1764706  0.5025126  0.55737704 0.23232323 0.09574468 0.4709389\n",
      " 0.37190434 0.11666667] [[1.4831736  0.75936675]] [1. 0.]\n",
      "[[0.81064343 0.5395398  0.5629271  0.6948887  0.8211738  0.38263452\n",
      "  0.7214816  0.64790446 0.6572807  0.60105485 0.56519306 0.38200468\n",
      "  0.713862   0.37730297 0.47669688 0.7922633  1.905438   2.1984217\n",
      "  1.6213502  1.4569328  2.171183   2.5428839  1.8242801  1.988969\n",
      "  1.3468378  2.3092566  2.7532742  2.4827316  1.6406991  2.2300258\n",
      "  1.6163057  2.2702382  1.2577407  1.6682438  1.6502616  2.7162971\n",
      "  1.7581575  1.9106119  1.9209682  2.4787667  2.6235504  2.940212\n",
      "  1.921444   1.8625934  2.4577339  2.1968818  2.4053607  1.8466522\n",
      "  3.337066   1.3851942  1.7474514  2.2398887  1.4762436  1.3234437\n",
      "  2.0322926  3.2108927 ]]\n",
      "[[1.4831736  0.75936675]]\n",
      "-----------\n",
      "[0.3529412  0.5125628  0.6721311  0.         0.         0.45901638\n",
      " 0.04355252 0.25      ] [[1.3742785 0.551414 ]] [0. 1.]\n",
      "[[0.7343458  0.57470757 0.5367955  0.5578776  0.8239335  0.36270878\n",
      "  0.6837256  0.5062389  0.52471936 0.6273497  0.5456879  0.45078152\n",
      "  0.7933658  0.53133917 0.45430377 0.74054223 2.0788164  2.8891332\n",
      "  1.602122   1.2313542  2.0482755  2.6692436  1.767189   1.7460706\n",
      "  1.1611781  2.563236   2.930591   2.5143275  1.9357857  2.3959107\n",
      "  1.4571552  2.3165338  1.3946761  1.5626862  1.3413701  2.5638998\n",
      "  1.5246794  2.2091334  1.8344452  2.2236059  1.8762656  2.5101516\n",
      "  1.8446221  2.4794831  2.4370236  2.2781272  2.2569797  1.5886054\n",
      "  3.0544295  1.1695669  1.7899603  1.9261657  1.4380949  1.2516664\n",
      "  2.2097287  3.3663092 ]]\n",
      "[[1.3742785 0.551414 ]]\n",
      "-----------\n",
      "[0.29411766 0.36683416 0.4918033  0.         0.         0.39940387\n",
      " 0.08112724 0.1       ] [[1.6502886 1.0059379]] [1. 0.]\n",
      "[[0.848685   0.6535787  0.64679885 0.72634953 0.8860196  0.3700322\n",
      "  0.8446679  0.6734757  0.72583205 0.7385589  0.6507419  0.5256118\n",
      "  0.8498037  0.53643763 0.611663   0.84537953 2.4397159  2.7776544\n",
      "  1.874084   1.4378312  1.9977913  2.5702605  1.6728196  1.8191757\n",
      "  1.2623992  2.5066617  2.7401605  2.5220137  1.8922396  2.4084408\n",
      "  1.5239965  2.3872273  1.476337   1.6301587  1.5822731  2.5319865\n",
      "  1.5120981  2.1065836  1.7236749  2.2593226  2.3230162  2.69437\n",
      "  1.8979993  2.4040022  2.3113093  2.124816   2.0667508  1.6789236\n",
      "  2.859517   1.2288597  1.7642324  2.135246   1.4477773  1.3784517\n",
      "  2.228839   2.9637237 ]]\n",
      "[[1.6502886 1.0059379]]\n",
      "-----------\n",
      "[0.11764706 0.98994976 0.57377046 0.45454547 0.641844   0.45454547\n",
      " 0.03415884 0.53333336] [[ 0.87608516 -0.00302412]] [0. 1.]\n",
      "[[0.2865075  0.30033886 0.23081458 0.1504814  0.48823175 0.52628225\n",
      "  0.22663403 0.29443777 0.15320435 0.3021136  0.4205927  0.28894722\n",
      "  0.2625652  0.26714247 0.21587458 0.47685337 1.4655349  1.7834408\n",
      "  1.1281476  1.1428707  2.0863135  2.0916119  2.76778    1.5788455\n",
      "  1.1449285  1.5811522  2.1251109  1.9149123  1.7642461  2.7593193\n",
      "  2.4055183  1.5083213  1.1559186  2.4542165  1.7862365  2.7990632\n",
      "  1.8367214  1.8849081  1.8868352  2.7015429  1.8750556  2.658879\n",
      "  1.7745038  2.6267312  2.6135445  2.3390746  2.1824942  1.8590902\n",
      "  2.602919   1.7481245  2.203192   1.7759429  1.5844122  1.0833645\n",
      "  1.8698266  4.0318904 ]]\n",
      "[[ 0.87608516 -0.00302412]]\n",
      "-----------\n",
      "[0.4117647  0.5728643  0.52459013 0.         0.         0.40834576\n",
      " 0.2792485  0.21666667] [[1.4644425 0.6228415]] [0. 1.]\n",
      "[[0.7143731  0.54871845 0.53069633 0.64809865 0.8019621  0.36806393\n",
      "  0.70343006 0.51091975 0.57114017 0.6571045  0.56808704 0.48611045\n",
      "  0.7747643  0.36161667 0.5148297  0.74199986 1.9436536  2.6703186\n",
      "  1.4805088  1.2323016  2.105804   2.7440412  1.6962385  1.7590663\n",
      "  1.1853082  2.6997588  2.6994681  2.3151534  2.0160222  1.9754555\n",
      "  1.5543939  2.3837345  1.3270936  1.8333677  1.4029671  2.3622754\n",
      "  1.5671359  2.1286082  1.865436   2.1667585  2.545153   2.8309813\n",
      "  1.7399166  2.1039848  2.2803197  2.29935    2.331548   1.6969217\n",
      "  3.1461258  1.2362112  1.7254732  1.9440483  1.3758712  1.4253777\n",
      "  2.0308344  3.4229426 ]]\n",
      "[[1.4644425 0.6228415]]\n",
      "-----------\n",
      "[0.5294118  0.72864324 0.6557377  0.46464646 0.1536643  0.56482863\n",
      " 0.23868488 0.31666666] [[0.5842949  0.29713225]] [0. 1.]\n",
      "[[0.427974   0.39184794 0.44725147 0.33847457 0.62427306 0.3208938\n",
      "  0.4936503  0.43160594 0.37269217 0.38758966 0.35934758 0.34023383\n",
      "  0.45893493 0.50960946 0.27293444 0.5059032  1.5642182  2.0936458\n",
      "  0.9597652  1.3298793  2.3241     2.6987433  1.8741548  1.7060746\n",
      "  1.2982198  2.037162   2.7658367  2.3976235  1.7279358  2.6504035\n",
      "  1.6277629  2.3228996  1.2781577  1.8474323  1.215004   2.39724\n",
      "  1.9509971  1.8876927  1.849249   2.4447372  1.901914   2.7857745\n",
      "  1.4788322  2.306057   2.4776032  2.1365662  2.5670466  1.7174112\n",
      "  3.4507885  1.4767289  1.7651149  1.8162768  1.7154788  1.2981229\n",
      "  1.8313053  3.6681895 ]]\n",
      "[[0.5842949  0.29713225]]\n",
      "-----------\n",
      "[0.5294118  0.8241206  0.6393443  0.         0.         0.48882264\n",
      " 0.02988898 0.4       ] [[1.0256829  0.20048977]] [0. 1.]\n",
      "[[0.41925916 0.4322251  0.3807748  0.3329623  0.6849734  0.34378976\n",
      "  0.46378735 0.3059014  0.28730896 0.4969972  0.4171487  0.4027761\n",
      "  0.6623153  0.38833013 0.30807284 0.5273035  1.7954187  2.8733587\n",
      "  1.2017034  0.9748821  2.1622086  2.930376   1.8111502  1.5504757\n",
      "  0.9949382  2.5799084  2.7000673  2.2137396  2.1051989  2.1784115\n",
      "  1.529211   2.3466744  1.3230008  1.8676076  1.1082361  2.3736143\n",
      "  1.5993332  2.2791872  1.8339844  2.0254867  1.8176342  2.6191196\n",
      "  1.5830374  2.5628548  2.418114   2.281183   2.3588498  1.4921222\n",
      "  3.0421157  1.1497133  1.8862202  1.6559393  1.387105   1.2994124\n",
      "  2.1257944  3.789345  ]]\n",
      "[[1.0256829  0.20048977]]\n",
      "-----------\n",
      "[0.1764706  0.53768843 0.5081967  0.13131313 0.05673759 0.34128168\n",
      " 0.25619128 0.03333334] [[1.6398262 0.9439604]] [0. 1.]\n",
      "[[0.8143561  0.5597492  0.5849694  0.7149718  0.8622643  0.38443285\n",
      "  0.8119137  0.6838447  0.68287444 0.6650629  0.65853626 0.46182668\n",
      "  0.7354672  0.35861105 0.5525368  0.86019456 2.2652984  2.3766975\n",
      "  1.7848524  1.4022782  1.9735829  2.6460505  1.73599    1.9065819\n",
      "  1.2773191  2.3230352  2.6312118  2.24769    1.8292145  2.1939535\n",
      "  1.5648811  2.3483648  1.2983207  1.7126071  1.740562   2.5604477\n",
      "  1.6457698  1.8944554  1.8640383  2.283172   2.563078   3.005888\n",
      "  1.9219545  2.053964   2.290267   2.127514   2.1223037  1.8416289\n",
      "  3.1032596  1.3460952  1.7458205  2.25456    1.3725512  1.3699462\n",
      "  2.1256752  2.8890424 ]]\n",
      "[[1.6398262 0.9439604]]\n",
      "-----------\n",
      "[0.47058824 0.9849246  0.6229508  0.2929293  0.33096927 0.55886734\n",
      " 0.22502135 0.6       ] [[ 0.6862707  -0.19970731]] [0. 1.]\n",
      "[[0.2653557  0.31138274 0.25572318 0.19662851 0.4749056  0.41717166\n",
      "  0.23089129 0.22287512 0.16708493 0.3273073  0.3240229  0.3059538\n",
      "  0.38348815 0.27797586 0.21154368 0.36683136 1.1181023  2.1735356\n",
      "  0.77362597 0.9931278  2.334681   2.564296   2.310679   1.5405519\n",
      "  1.0810314  2.2215595  2.4440434  2.1005845  1.9308219  2.248231\n",
      "  2.0210426  1.9292066  1.1633406  2.301616   1.2318952  2.4803448\n",
      "  1.8215196  2.1147919  1.9415026  2.404398   2.0519512  2.6685743\n",
      "  1.5100738  2.3335958  2.5529826  2.4464228  2.6074872  1.6698385\n",
      "  3.1167011  1.4798924  1.9993973  1.5404412  1.5476449  1.2394986\n",
      "  1.7854933  4.4151206 ]]\n",
      "[[ 0.6862707  -0.19970731]]\n",
      "-----------\n",
      "[0.3529412  0.7236181  0.59016395 0.27272728 0.26950353 0.5052161\n",
      " 0.07557643 0.31666666] [[0.9980705  0.40759283]] [1. 0.]\n",
      "[[0.5523174  0.4449742  0.42899024 0.37153837 0.6999556  0.40036714\n",
      "  0.5798227  0.46742606 0.3919775  0.5017604  0.48455432 0.40357587\n",
      "  0.59232855 0.53084296 0.38730568 0.647231   1.8568511  2.315439\n",
      "  1.2861052  1.2422414  2.153389   2.4741714  2.1024675  1.6753473\n",
      "  1.1959299  2.0728056  2.5510654  2.304809   1.8067378  2.6309447\n",
      "  1.8230562  2.0448775  1.3042686  1.9821063  1.4598036  2.5819447\n",
      "  1.7491444  2.0074809  1.793437   2.4889483  1.9366021  2.7313907\n",
      "  1.6882955  2.5337405  2.5078368  2.1461768  2.290852   1.7376679\n",
      "  2.9608676  1.4680462  1.921881   1.8935908  1.5477111  1.2378263\n",
      "  1.9704363  3.544972  ]]\n",
      "[[0.9980705  0.40759283]]\n",
      "-----------\n",
      "[0.1764706  0.40201005 0.6721311  0.3131313  0.08274232 0.509687\n",
      " 0.5183604  0.1       ] [[1.3627453 0.6216433]] [0. 1.]\n",
      "[[0.81930053 0.5373242  0.581286   0.7153762  0.79393363 0.3507008\n",
      "  0.65199757 0.6448689  0.66263306 0.5492712  0.50279707 0.31973398\n",
      "  0.64418894 0.31940737 0.36703974 0.7511275  1.6789999  2.0937438\n",
      "  1.5362798  1.5327361  2.2263918  2.49088    1.7790354  2.1037006\n",
      "  1.4515519  2.3646274  3.030895   2.6755824  1.526542   2.239838\n",
      "  1.5115178  2.2814813  1.2086259  1.481012   1.5911167  2.7509804\n",
      "  1.811031   1.8529425  2.0275233  2.6174514  2.6241336  2.8978832\n",
      "  1.955864   1.6174599  2.5110736  2.2937222  2.6186085  1.9111763\n",
      "  3.7029617  1.4133079  1.6160613  2.2801542  1.544884   1.2842238\n",
      "  1.9268613  3.262797  ]]\n",
      "[[1.3627453 0.6216433]]\n",
      "-----------\n",
      "[0.47058824 0.60301507 0.6393443  0.         0.         0.37257823\n",
      " 0.1413322  0.71666664] [[1.3080876  0.06894718]] [1. 0.]\n",
      "[[0.55049694 0.5304283  0.41178823 0.42647192 0.7094888  0.47720596\n",
      "  0.31567436 0.27514625 0.33312055 0.4701477  0.49152145 0.50681585\n",
      "  0.66395795 0.4578297  0.5267432  0.5619666  1.2379446  2.8006818\n",
      "  1.19102    1.1460404  2.1160457  2.6626198  2.0023255  1.4968417\n",
      "  1.1094117  2.7904422  2.8301847  2.2572985  2.2488315  2.0446482\n",
      "  1.807149   2.339557   1.4136157  1.9117972  1.1839498  2.2759476\n",
      "  1.4939506  2.3728676  1.9885197  1.8650881  2.000643   1.9354128\n",
      "  1.694173   2.3819816  2.2582135  2.9454668  2.380772   1.3495817\n",
      "  2.8980412  1.1135439  1.8870732  1.3164179  1.6501222  1.319636\n",
      "  2.1577775  4.6075754 ]]\n",
      "[[1.3080876  0.06894718]]\n",
      "-----------\n",
      "[0.05882353 0.7386935  0.7704918  0.41414142 0.         0.7347243\n",
      " 0.11955594 0.1       ] [[0.9847613  0.52027667]] [0. 1.]\n",
      "[[0.57621646 0.39183313 0.43312877 0.3723658  0.7504151  0.28983563\n",
      "  0.49545395 0.5245812  0.38698304 0.36521596 0.32364058 0.17010987\n",
      "  0.5830863  0.39851302 0.15958554 0.5655242  1.9119736  2.1469326\n",
      "  1.5806154  1.3609595  2.4069033  2.8535457  1.8239248  2.0932655\n",
      "  1.2380208  1.8151863  2.9776983  2.7152214  1.2166897  2.710644\n",
      "  1.3208425  2.3388867  1.1848121  1.20761    1.4191015  3.2759533\n",
      "  2.0455654  1.9245492  1.9077203  2.5459816  1.7099813  2.945793\n",
      "  1.9616479  2.125072   2.941988   1.8301907  2.5944731  1.665989\n",
      "  3.6170382  1.2562766  1.9534833  2.3839796  1.4990172  1.0426424\n",
      "  2.3416686  3.1217537 ]]\n",
      "[[0.9847613  0.52027667]]\n",
      "-----------\n",
      "[0.23529412 0.5929648  0.57377046 0.         0.         0.6631893\n",
      " 0.35269    0.08333334] [[1.4619831 0.5756138]] [1. 0.]\n",
      "[[0.7343699  0.5063073  0.4811314  0.6343363  0.78481936 0.31181097\n",
      "  0.6603228  0.5174296  0.5333292  0.6371549  0.4353928  0.28045723\n",
      "  0.82534516 0.2550457  0.2871874  0.6606743  1.9432608  2.6022964\n",
      "  1.618349   1.2145991  2.3726795  2.7352793  1.7133185  2.0411344\n",
      "  1.1780038  2.6568294  2.8036537  2.7007074  1.5795026  1.9459915\n",
      "  1.428472   2.24298    1.209185   1.5870546  1.3985717  2.9342008\n",
      "  1.6990783  2.1813285  1.878778   2.5209455  2.6939125  3.152192\n",
      "  1.9274795  1.8261604  2.7039297  2.0035021  2.643222   1.7705767\n",
      "  3.4543555  1.1986146  1.8027596  2.325137   1.2256522  1.2991502\n",
      "  2.0944853  3.2286162 ]]\n",
      "[[1.4619831 0.5756138]]\n",
      "-----------\n",
      "[0.         0.33668342 0.6229508  0.         0.         0.6751118\n",
      " 0.04953032 0.41666666] [[1.7552305 0.5732625]] [1. 0.]\n",
      "[[0.8693358  0.6422865  0.5151661  0.61482364 0.8581537  0.44501236\n",
      "  0.49434775 0.5062538  0.55383706 0.58636683 0.48424688 0.3036598\n",
      "  0.8890567  0.55261725 0.4521842  0.69679344 1.786669   2.7484484\n",
      "  1.955287   1.4393867  2.3355265  2.3795025  2.0415862  1.9951336\n",
      "  1.2252953  2.5015333  2.975168   3.013228   1.485827   2.4418762\n",
      "  1.6824634  2.149007   1.4373806  1.3460217  1.5133206  3.3152537\n",
      "  1.6004045  2.404285   1.8679563  2.4327765  2.131619   2.156036\n",
      "  2.2241104  2.241028   2.8450015  2.4302778  2.4525878  1.4289615\n",
      "  2.9095187  1.0526807  2.065257   2.0930274  1.5530932  1.106565\n",
      "  2.5290923  3.8594322 ]]\n",
      "[[1.7552305 0.5732625]]\n",
      "-----------\n",
      "[0.29411766 0.6834171  0.6721311  0.         0.         0.\n",
      " 0.23996584 0.8       ] [[1.6645294  0.01086532]] [1. 0.]\n",
      "[[0.5300464  0.50916487 0.3700232  0.44698834 0.7261263  0.60162497\n",
      "  0.21972847 0.24941206 0.28790748 0.39958027 0.6575115  0.6058276\n",
      "  0.45018765 0.23802063 0.64562947 0.6838537  1.0517828  2.6072617\n",
      "  1.2778345  1.1162617  1.7085359  2.704248   2.1085918  1.4125541\n",
      "  1.1012874  2.7901502  2.7514222  1.6168853  2.6127143  1.666982\n",
      "  1.9461427  2.397115   1.3242942  2.0308692  1.5097718  1.9767156\n",
      "  1.371309   2.189139   2.2425585  1.4452944  2.1247492  1.7348542\n",
      "  1.8218894  2.15788    1.8572047  3.463673   2.0569382  1.3977236\n",
      "  2.7887113  1.1425568  1.8360419  1.1399627  1.6264226  1.3440337\n",
      "  2.206768   4.7588186 ]]\n",
      "[[1.6645294  0.01086532]]\n",
      "-----------\n",
      "[0.11764706 0.879397   0.72131145 0.         0.         0.34128168\n",
      " 0.1058924  0.01666667] [[1.4702281 0.5154344]] [1. 0.]\n",
      "[[0.5761697  0.41471624 0.39817756 0.4521757  0.7968757  0.33566165\n",
      "  0.6238326  0.47355863 0.3598442  0.5392095  0.5527429  0.3206404\n",
      "  0.61480033 0.18215886 0.25367516 0.7606367  2.2814708  2.6743906\n",
      "  1.7195612  0.9855583  1.8674086  2.9967875  1.7502195  1.8557518\n",
      "  1.009237   2.3500612  2.7300248  1.9764035  1.9342363  2.015098\n",
      "  1.3160985  2.293013   1.1213552  1.5647976  1.5864217  2.6945128\n",
      "  1.5965552  1.9851302  1.9856284  2.1015146  1.9779873  3.1302788\n",
      "  1.9653386  2.2036078  2.4343753  2.067772   2.125874   1.7870283\n",
      "  3.2535129  1.199128   1.8247443  2.233136   1.0706412  1.1871971\n",
      "  2.2714255  2.8097537 ]]\n",
      "[[1.4702281 0.5154344]]\n",
      "-----------\n",
      "[0.23529412 0.6482412  0.4918033  0.12121212 0.27304965 0.40983605\n",
      " 0.19171648 0.16666667] [[1.4459709 0.6554253]] [1. 0.]\n",
      "[[0.71335685 0.50584304 0.47606957 0.55417234 0.77810395 0.42490253\n",
      "  0.72623813 0.57157546 0.52606153 0.64622736 0.6153983  0.4591827\n",
      "  0.69557834 0.3759391  0.49941224 0.790984   2.0968833  2.369114\n",
      "  1.5676738  1.2558998  2.0284243  2.4078503  2.049285   1.7796555\n",
      "  1.2046658  2.2775996  2.432638   2.21604    1.864827   2.292699\n",
      "  1.8422952  1.9986161  1.2700077  2.0423863  1.7015418  2.587801\n",
      "  1.6166058  1.9846396  1.813384   2.5020955  2.4805307  2.983077\n",
      "  1.8434572  2.2770739  2.3993804  2.1344469  2.169813   1.8830025\n",
      "  2.8839388  1.4731892  1.8645518  2.1108422  1.3549662  1.3194486\n",
      "  1.9607642  3.2064903 ]]\n",
      "[[1.4459709 0.6554253]]\n",
      "-----------\n",
      "[0.05882353 0.7236181  0.6721311  0.4040404  0.         0.61549926\n",
      " 0.22587532 0.11666667] [[1.1777879  0.63834786]] [1. 0.]\n",
      "[[0.6455281  0.41756335 0.46557564 0.47474188 0.7824899  0.33104545\n",
      "  0.55426425 0.5709599  0.47106183 0.40562853 0.4049429  0.2231687\n",
      "  0.5975118  0.37746012 0.2549562  0.65560186 1.8614984  2.0301654\n",
      "  1.5901668  1.4095047  2.330616   2.8439333  1.8126462  2.0609114\n",
      "  1.2704606  1.8884144  2.8307564  2.5136127  1.3544521  2.4851036\n",
      "  1.4356757  2.3967316  1.187221   1.3796762  1.5421228  3.084507\n",
      "  2.0117674  1.8623013  1.9402735  2.415752   2.0758529  2.976288\n",
      "  1.9411976  1.9599638  2.7435737  1.9715943  2.5035396  1.7047392\n",
      "  3.5319061  1.2928144  1.9166164  2.3320289  1.5086741  1.1539888\n",
      "  2.2867298  3.1883454 ]]\n",
      "[[1.1777879  0.63834786]]\n",
      "-----------\n",
      "[0.05882353 0.43718594 0.55737704 0.34343433 0.09101655 0.5603577\n",
      " 0.13791631 0.05      ] [[1.4249047 1.0002604]] [1. 0.]\n",
      "[[0.834303   0.5612798  0.60770726 0.6381169  0.86067075 0.36391392\n",
      "  0.7697177  0.73637897 0.6929186  0.5725905  0.53972536 0.32873034\n",
      "  0.7322081  0.5655098  0.43968615 0.80183387 2.251459   2.1583693\n",
      "  1.8662732  1.6202984  2.2042682  2.485258   1.845479   2.0480897\n",
      "  1.3873795  1.9146769  2.7708898  2.6967864  1.3849887  2.767952\n",
      "  1.5749365  2.291535   1.3623134  1.4425184  1.7289182  3.0186112\n",
      "  1.8569771  1.8733782  1.7805263  2.5836015  2.2029238  2.8396232\n",
      "  2.0153193  2.1956902  2.6352592  1.9273881  2.2592444  1.7675455\n",
      "  3.1521487  1.3748426  1.8786705  2.4090369  1.5924507  1.2166239\n",
      "  2.2672365  2.8993149 ]]\n",
      "[[1.4249047 1.0002604]]\n",
      "-----------\n",
      "[0.3529412  0.77386934 0.60655737 0.32323232 0.22813238 0.4366617\n",
      " 0.32493594 0.3       ] [[1.0186455 0.3196391]] [1. 0.]\n",
      "[[0.5345555  0.40792686 0.41569838 0.4274043  0.6712413  0.39801353\n",
      "  0.52380395 0.4518212  0.3956419  0.46058285 0.4830244  0.38666037\n",
      "  0.5040222  0.34950462 0.36977547 0.6435254  1.5695639  2.0903668\n",
      "  1.1453837  1.23373    2.1581018  2.5932715  2.022641   1.7409942\n",
      "  1.2400692  2.183978   2.590004   2.143113   1.8514853  2.263697\n",
      "  1.7849255  2.142414   1.1850842  2.0129633  1.480175   2.4425123\n",
      "  1.8083045  1.8901716  1.9513693  2.4086676  2.3011494  2.895683\n",
      "  1.6607707  2.0909684  2.3924463  2.3029647  2.4361491  1.8314308\n",
      "  3.2929492  1.4986906  1.7997235  1.8931676  1.5171767  1.3062794\n",
      "  1.834253   3.6474345 ]]\n",
      "[[1.0186455 0.3196391]]\n",
      "-----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update layer\n",
      "XX 0.66045547 0.67105263\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-7772bb68b290>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtest_datas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[1;31m#tpredictions = model[k](test_datas, training=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m             \u001b[0mtest_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_datas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_object\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mval_datas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-7772bb68b290>\u001b[0m in \u001b[0;36mtest_step\u001b[1;34m(datas, labels, modelk, loss_objectk, test_lossk, test_accuracyk)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[0mtest_lossk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m     \u001b[0mtest_accuracyk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistributed_training_utils\u001b[0m  \u001b[1;31m# pylint:disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     return distributed_training_utils.call_replica_local_fn(\n\u001b[1;32m--> 194\u001b[1;33m         replica_local_fn, *args, **kwargs)\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\distribute\\distributed_training_utils.py\u001b[0m in \u001b[0;36mcall_replica_local_fn\u001b[1;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextended\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mreplica_local_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreplica_local_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m       \u001b[1;34m\"\"\"Updates the state of the metric in a replica-local context.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m       \u001b[0mupdate_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[0mresult_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[1;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_context_for_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m       \u001b[0mupdate_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mupdate_op\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# update_op will be None in eager execution.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m       \u001b[0mmetric_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mupdate_state\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mmatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m     return super(MeanMetricWrapper, self).update_state(\n\u001b[1;32m--> 584\u001b[1;33m         matches, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mupdate_state\u001b[1;34m(self, values, sample_weight)\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[0mvalue_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalue_sum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m       \u001b[0mupdate_total_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_sum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[1;31m# Exit early if the reduction doesn't have a denominator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36massign_add\u001b[1;34m(self, delta, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m    779\u001b[0m       assign_add_op = gen_resource_variable_ops.assign_add_variable_op(\n\u001b[0;32m    780\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 781\u001b[1;33m           name=name)\n\u001b[0m\u001b[0;32m    782\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_add_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_add_variable_op\u001b[1;34m(resource, value, name)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;34m\"AssignAddVariableOp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         value)\n\u001b[0m\u001b[0;32m     52\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# hybrid model DLS\n",
    "\n",
    "# P 05\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.activations import relu\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "class LSLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,  num_outputs_s, num_outputs_r, num_outputs_l, activation=sigmoid, wstd = 0.3, bstd = 0.5):\n",
    "        super(LSLayer, self).__init__()\n",
    "        self.num_outputs_l = num_outputs_l\n",
    "        self.num_outputs_s = num_outputs_s \n",
    "        self.num_outputs_r = num_outputs_r\n",
    "        self.num_outputs = num_outputs_l + num_outputs_s + num_outputs_r\n",
    "        self.activation = activation\n",
    "        self.wstd = wstd\n",
    "        self.bstd = bstd\n",
    "        \n",
    "    def build(self, input_shape):  \n",
    "        self.num_inputs = input_shape[-1]\n",
    "        self.kernel = self.add_weight(\"kernel\",\n",
    "                                      shape=(int(input_shape[-1]),\n",
    "                                             self.num_outputs), \n",
    "                                      initializer=tf.keras.initializers.RandomNormal(stddev=self.wstd),\n",
    "                                     trainable=True)\n",
    "\n",
    "        self.bias = self.add_weight(\"bias\",\n",
    "                                      shape=[self.num_outputs],\n",
    "                                    initializer=tf.keras.initializers.RandomNormal(stddev=self.bstd),\n",
    "                                   trainable=True)\n",
    "        \n",
    "        params = self.get_weights()\n",
    "    \n",
    "        for j in range(self.num_outputs):\n",
    "            r = 0\n",
    "            for i in range(self.num_inputs):\n",
    "                params[0][i][j] = params[0][i][j]*random.random()*3\n",
    "                r += (params[0][i][j]-0.5)**2\n",
    "            r = math.sqrt(r)\n",
    "            params[1][j] = r\n",
    "\n",
    "        self.set_weights(params )\n",
    "        \n",
    "    \n",
    "    # F2 method LS layer\n",
    "    def call(self, input):\n",
    "        \n",
    "        isp = input.shape\n",
    "        In1 = tf.transpose(input)\n",
    "        kernel_S, kernel_L  = tf.split(self.kernel,[ self.num_outputs_s + self.num_outputs_r, self.num_outputs_l ], axis = 1 )\n",
    "        bias_S, bias_L  = tf.split(self.bias,[ self.num_outputs_s +  self.num_outputs_r, self.num_outputs_l ], axis = 0 )\n",
    "        \n",
    "        # case spherical\n",
    "        \n",
    "        s_shape  = self.num_outputs_s + self.num_outputs_r\n",
    "        In2 = tf.stack([In1] * s_shape)\n",
    "        InD = tf.transpose(In2)\n",
    "        WD = tf.stack([kernel_S] * isp[0])\n",
    "        ddd = WD - InD\n",
    "        dd0 = tf.math.multiply(ddd, ddd)\n",
    "        dd1 = tf.math.reduce_sum(dd0, axis =1)\n",
    "        dd2 = tf.cast(dd1,tf.double)\n",
    "        dd3 = tf.sqrt(dd2)\n",
    "        d_r = tf.cast(dd3,tf.float32)\n",
    "        d_R = tf.abs(bias_S)\n",
    "        d_rR = tf.math.divide_no_nan(d_r,d_R)\n",
    "        d_x0 = tf.ones(d_rR.shape) - d_rR\n",
    "        result_S = tf.math.scalar_mul(6,d_x0)\n",
    "        result_S = sigmoid(result_S)\n",
    "        \n",
    "        # case linear\n",
    "\n",
    "        d_1 = tf.stack([bias_L] * isp[0])\n",
    "        result_L = tf.matmul(input, kernel_L) + d_1 \n",
    "        result_L = relu(result_L)\n",
    "\n",
    "        #case empty, merge\n",
    "        \n",
    "        '''\n",
    "        #print (self.num_outputs_r)\n",
    "        if self.num_outputs_r > 0:\n",
    "            r_S, _ = tf.split (result_S,[self.num_outputs_s, self.num_outputs_r],axis=1 )\n",
    "            r_1 = np.zeros((result_S.shape[0],self.num_outputs_r))\n",
    "            result_R = tf.cast(tf.constant(r_1),tf.float32)\n",
    "            result = tf.concat([r_S, result_R, result_L],axis=1)            \n",
    "            #print (self.num_outputs_s, self.num_outputs_r)\n",
    "            #print (\"result_S\", result_S)\n",
    "            #print (\"result_L\", result_L)\n",
    "            #print (\"result\", result)\n",
    "        else:\n",
    "            result = tf.concat([result_S, result_L],axis=1)        \n",
    "        '''\n",
    "        \n",
    "        result = tf.concat([result_S, result_L],axis=1)        \n",
    "        \n",
    "        return result\n",
    "    \n",
    "\n",
    "class NN_Model(Model):\n",
    "    \n",
    "    def __init__(self,c,hs,hr,hl):\n",
    "        super(NN_Model, self).__init__()\n",
    "        self.d1 = LSLayer(hs,hr,hl)\n",
    "        self.d2 = Dense(c)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        #print (\"call benn:\",x, tf.math.reduce_sum(x))\n",
    "        return self.d2(x)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "#@tf.function\n",
    "def train_step(datas, labels,modelk,loss_objectk,optimizerk,train_lossk,train_accuracyk):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = modelk(datas, training=True)\n",
    "        loss = loss_objectk(labels, predictions)\n",
    "    gradients = tape.gradient(loss, modelk.trainable_variables)\n",
    "    optimizerk.apply_gradients(zip(gradients, modelk.trainable_variables))\n",
    "\n",
    "    train_lossk(loss)\n",
    "    train_accuracyk(labels, predictions)\n",
    "\n",
    "#@tf.function\n",
    "def test_step(datas, labels,modelk,loss_objectk,test_lossk,test_accuracyk):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    \n",
    "    predictions = modelk(datas, training=False)\n",
    "    t_loss = loss_objectk(labels, predictions)\n",
    "\n",
    "    test_lossk(t_loss)\n",
    "    test_accuracyk(labels, predictions)\n",
    "    \n",
    "\n",
    "def analyse_NN (model, train_ds,optimizer):\n",
    "    \n",
    "    for datas, labels in train_ds:\n",
    "        dA = datas.numpy()\n",
    "        dL = labels.numpy()\n",
    "        for i in range(2):\n",
    "            tA1 = tf.constant(dA[i])\n",
    "            tA = tf.reshape(tA1,[1,dA.shape[1]])\n",
    "            predictions = model(tA, training=False)\n",
    "            \n",
    "            print (dA[i], predictions.numpy(),dL[i])\n",
    "            to1 = model.d1 (tA)\n",
    "            print (to1.numpy())\n",
    "            to2 = model.d2 (to1)\n",
    "            print (to2.numpy())\n",
    "            print (\"-----------\")\n",
    "\n",
    "    \n",
    "def update_NN_model (model, train_ds,optimizer):\n",
    "\n",
    "    rdb = 0\n",
    "    odb = 0\n",
    "    #N = min(5, model.d1.num_outputs_r)   # number of new SSN nodes\n",
    "    N = model.d1.num_outputs_r   # number of new SSN nodes\n",
    "    \n",
    "    if N <= 0:\n",
    "        return\n",
    "    \n",
    "    # k-means\n",
    "    \n",
    "    baditems = []\n",
    "    for datas, labels in train_ds:\n",
    "        predictions = model(datas, training=False)\n",
    "        for i in range(datas.shape[0]):\n",
    "            #print (datas.numpy()[i], predictions.numpy()[i], np.argmax(predictions.numpy()[i]), labels.numpy()[i],np.argmax(labels.numpy()[i]))\n",
    "            if np.argmax(predictions.numpy()[i]) != np.argmax(labels.numpy()[i]):\n",
    "                rdb = rdb + 1\n",
    "                baditems.append(datas.numpy()[i])\n",
    "            odb = odb + 1        \n",
    "    #print (\"pontossag:\",(odb-rdb)/odb, len(baditems))\n",
    "    N = min(N, len(baditems))\n",
    "    if N == 0:\n",
    "        return\n",
    "    inds = random.sample(range(len(baditems)), N)\n",
    "    \n",
    "    print (\"update layer\")\n",
    "    #print (baditems)\n",
    "    centers = KMeans(n_clusters=N).fit(baditems).cluster_centers_\n",
    "    #print (\"centers:\")\n",
    "    #print (centers)\n",
    "    neww = np.zeros((model.d1.num_inputs,N))\n",
    "    for i in range(N):\n",
    "        for j in range(model.d1.num_inputs):\n",
    "            neww[j,i] = centers[i][j]\n",
    "    #print (\"neww\")\n",
    "    #print (neww)\n",
    "            \n",
    "    xu = model.d1.get_weights()\n",
    "        \n",
    "    idx = random.sample(range(model.d1.num_outputs_s + model.d1.num_outputs_r),N)\n",
    "    for j1 in range(N):\n",
    "        j = idx[j1]\n",
    "        for i in range(xu[0].shape[0]):\n",
    "            xu[0][i][j] = neww[i,j1]\n",
    "           \n",
    "    for j1 in range(N):\n",
    "        j = idx[j1]\n",
    "        r = 0\n",
    "        for i in range(xu[0].shape[0]):\n",
    "            r += (xu[0][i][j]-0.5)**2\n",
    "        r = math.sqrt(r)\n",
    "        xu[1][j] = r\n",
    "            \n",
    "            \n",
    "    model.d1.set_weights(xu )\n",
    "    \n",
    "    #model.d1.num_outputs_s = model.d1.num_outputs_s + N\n",
    "    #model.d1.num_outputs_r = model.d1.num_outputs_r - N\n",
    "\n",
    "    #optimizer = tf.keras.optimizers.Adam\n",
    "    #for var in optimizer.variables():\n",
    "    #    var.assign(tf.zeros_like(var))\n",
    "    \n",
    "\n",
    "K = 10\n",
    "\n",
    "# (x_train,y_train,x_test,y_test) = gen_data_array_cv(K)\n",
    "\n",
    "M = x_train[0].shape[1]\n",
    "C = y_train[0].shape[1]\n",
    "\n",
    "EPOCHS = 80\n",
    "Eupd = [20]\n",
    "\n",
    "H = M * 5\n",
    "HS = M\n",
    "HR = M\n",
    "HL = 5*M\n",
    "B = 32\n",
    "\n",
    "model = []\n",
    "loss_object =  []\n",
    "optimizer = []\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "val_loss = []\n",
    "val_accuracy = []\n",
    "train_ds = []\n",
    "val_ds = []\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(B)\n",
    "\n",
    "\n",
    "\n",
    "best_v = 0\n",
    "best_t = 0\n",
    "\n",
    "\n",
    "\n",
    "for k in range(K):\n",
    "    print (\"k=\",k, \"------------------------------\")\n",
    "    # Create an instance of the model\n",
    "    model.append( NN_Model(C,HS,HR,HL))\n",
    "    #model.append( NN_Model(H,C,0,0))\n",
    "\n",
    "    loss_object.append(tf.keras.losses.CategoricalCrossentropy(from_logits=True))\n",
    "    #loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "    optimizer.append(tf.keras.optimizers.Adam())\n",
    "    train_loss.append(tf.keras.metrics.Mean(name='train_loss'))\n",
    "    train_accuracy.append(tf.keras.metrics.CategoricalAccuracy(name='train_accuracy'))\n",
    "\n",
    "    test_loss.append(tf.keras.metrics.Mean(name='test_loss'))\n",
    "    test_accuracy.append(tf.keras.metrics.CategoricalAccuracy(name='test_accuracy'))\n",
    "    \n",
    "    val_loss.append(tf.keras.metrics.Mean(name='val_loss'))\n",
    "    val_accuracy.append(tf.keras.metrics.CategoricalAccuracy(name='val_accuracy'))\n",
    "    \n",
    "\n",
    "    #print (x_train[:2])\n",
    "    train_ds.append( tf.data.Dataset.from_tensor_slices(\n",
    "        (x_train[k], y_train[k])).batch(B))\n",
    "    val_ds.append( tf.data.Dataset.from_tensor_slices(\n",
    "        (x_val[k], y_val[k])).batch(B))\n",
    "    #print (train_ds)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for epoch in range(EPOCHS):\n",
    "      # Reset the metrics at the start of the next epoch\n",
    "\n",
    "        train_loss[k].reset_states()\n",
    "        train_accuracy[k].reset_states()\n",
    "        test_loss[k].reset_states()\n",
    "        test_accuracy[k].reset_states()\n",
    "\n",
    "        for datas, labels in train_ds[k]:\n",
    "            train_step(datas, labels,model[k],loss_object[k],optimizer[k],train_loss[k],train_accuracy[k])\n",
    "\n",
    "\n",
    "        for test_datas, test_labels in test_ds:\n",
    "            #tpredictions = model[k](test_datas, training=False)\n",
    "            test_step(test_datas, test_labels,model[k],loss_object[k],test_loss[k],test_accuracy[k])\n",
    "\n",
    "        for val_datas, val_labels in val_ds[k]:\n",
    "            #vpredictions = model[k](val_datas, training=False)\n",
    "            test_step(val_datas, val_labels,model[k],loss_object[k],val_loss[k],val_accuracy[k])\n",
    "            \n",
    "            \n",
    "        if epoch == 5:\n",
    "            analyse_NN (model[k], train_ds[k], optimizer[k])\n",
    "    \n",
    "        if epoch in Eupd and k < K/2:\n",
    "            update_NN_model (model[k], train_ds[k], optimizer[k])\n",
    "\n",
    "\n",
    "        X.append(epoch)\n",
    "        Y.append(test_accuracy[k].result() * 100)\n",
    "        if epoch % 20 == 0:\n",
    "            #print(\n",
    "            #    f'Epoch {epoch + 1}, '\n",
    "            #    f'Loss: {train_loss[k].result()}, '\n",
    "            #    f'Val Accuracy: {val_accuracy[k].result() * 100}, '\n",
    "            #    f'Test Accuracy: {test_accuracy[k].result() * 100}'\n",
    "            #  )    \n",
    "            \n",
    "            if val_accuracy[k].result() > best_v:\n",
    "                best_v = val_accuracy[k].result().numpy()\n",
    "                best_t = test_accuracy[k].result().numpy()\n",
    "                print(\"XX\", best_v,best_t)\n",
    "            #print(model.d1.bias.numpy())\n",
    "\n",
    "\n",
    "    #print(\n",
    "    #    f'Epoch {epoch + 1}, '\n",
    "    #    f'Loss: {train_loss[k].result()}, '\n",
    "    #    f'Val Accuracy: {val_accuracy[k].result() * 100}, '\n",
    "    #    f'Test Accuracy: {test_accuracy[k].result() * 100}'\n",
    "    #  )    \n",
    "    #acclist.append(test_accuracy[k].result())\n",
    "    print (\"accuracy:\",best_t)\n",
    "    #plt.plot(X, Y,label=\"Accuracy curve\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read benchmark data\n",
    "from pandas import  read_csv\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def gen_data_array_cv(K):\n",
    "    \n",
    "    \n",
    "    dataT = read_csv (\"pima-indians-diabetes.csv\", header =None)\n",
    "    dataA = np.array(dataT)    \n",
    "        \n",
    "    N = dataA.shape[0]\n",
    "    M = dataA.shape[1]-1\n",
    "\n",
    "    Nte = int(0.1*N)\n",
    "    Ntr = N - Nte\n",
    "\n",
    "    cval = dict()\n",
    "    for i in range(N):\n",
    "        cval[dataA[i][-1]] = 1\n",
    "    C = len(cval.keys())\n",
    "    \n",
    "    minv = [0 for _ in range(M)]\n",
    "    maxv = [0 for _ in range(M)]\n",
    "    \n",
    "    for j in range(M):\n",
    "        minv[j] =  dataA[0,j]\n",
    "        maxv[j] =  dataA[0,j]\n",
    "        \n",
    "    for i in range(N):\n",
    "        for j in range(M):\n",
    "            minv[j] = min( dataA[i,j] , minv[j])\n",
    "            maxv[j] = max( dataA[i,j] , maxv[j])\n",
    "          \n",
    "    orders = random.sample(range(N), N)\n",
    "    \n",
    "    x2_test = np.zeros((Nte,M),dtype='float32')\n",
    "    y2_test = np.zeros((Nte,C))\n",
    "    \n",
    "    ite = 0\n",
    "    for n in range(Ntr, N):\n",
    "        i = orders[n]\n",
    "        for j in range(M):\n",
    "            x2_test[ite,j] = (dataA[i,j] - minv[j]) / (maxv[j] - minv[j])\n",
    "        y2_test[ite, int(dataA[i,M])] = 1\n",
    "        ite += 1    \n",
    "       \n",
    "    Ntrv = int(Ntr/K)\n",
    "    Ntrt = Ntr - Ntrv\n",
    "    \n",
    "    x2_train = []\n",
    "    y2_train = []\n",
    "    x2_val = []\n",
    "    y2_val = []\n",
    "    \n",
    "    for k in range(K):\n",
    "        x2_train.append(np.zeros((Ntrt,M),dtype='float32'))\n",
    "        y2_train.append(np.zeros((Ntrt,C)))\n",
    "        x2_val.append(np.zeros((Ntrv,M),dtype='float32'))\n",
    "        y2_val.append(np.zeros((Ntrv,C)))\n",
    "    \n",
    "        itr = 0\n",
    "        ite = 0\n",
    "        for n in range(Ntr):\n",
    "            i = orders[n]\n",
    "            if n >= k*Ntrv and n < (k+1)*Ntrv:\n",
    "                for j in range(M):\n",
    "                    x2_val[k][ite,j] = (dataA[i,j] - minv[j]) / (maxv[j] - minv[j])\n",
    "                y2_val[k][ite, int(dataA[i,M])] = 1\n",
    "                ite += 1\n",
    "            else:            \n",
    "                for j in range(M):\n",
    "                    x2_train[k][itr,j] = (dataA[i,j] - minv[j]) / (maxv[j] - minv[j])\n",
    "                y2_train[k][itr,int(dataA[i,M])] = 1\n",
    "                itr += 1\n",
    "                \n",
    "    return (x2_train,y2_train, x2_val, y2_val, x2_test, y2_test)\n",
    "\n",
    "K = 10\n",
    "(x_train,y_train,x_val,y_val, x_test,y_test) = gen_data_array_cv(K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
