{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "def gen_cluster_data_list(Cv, Lv, Nv, Mv):\n",
    "    Tr = []\n",
    "    Ts = []\n",
    "    C = Cv  # number of categories\n",
    "    L = Lv   # number of centers\n",
    "    N = Nv  # number of elements\n",
    "    M = Mv  # number of dimensions\n",
    "    X, y = make_blobs(n_samples=N, centers=L, n_features=M,cluster_std=.5, random_state=11)\n",
    "    cmap = []\n",
    "    for _ in range(L):\n",
    "        cmap.append(random.randint(0,C-1))\n",
    "    cols = []\n",
    "    for i in range(N):\n",
    "        cols.append(cmap[y[i]])\n",
    "\n",
    "    for i in range(int(0.9*N)):\n",
    "        row = [X[i,j] for j in range(M)]\n",
    "        row.append(cols[i])\n",
    "        Tr.append(row)\n",
    "    \n",
    "    for i in range(int(0.9*N)+1,N):\n",
    "        row = [X[i,j] for j in range(M)]\n",
    "        row.append(cols[i])\n",
    "        Ts.append(row)\n",
    "        \n",
    "    return (Tr, Ts)\n",
    "\n",
    "def normalize (train):\n",
    "    mx = []\n",
    "    mn = []\n",
    "    for i in range(len(train[0])-1):\n",
    "        mx.append(max([x[i] for x in train ]))\n",
    "        mn.append(min([x[i] for x in train ]))\n",
    "    for row in train:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i] - mn[i]) / (mx[i] - mn[i]) \n",
    "    return train\n",
    "\n",
    "\n",
    "def gen_data_array(Cv, Lv, Nv, Mv):\n",
    "    C = Cv  # number of categories\n",
    "    L = Lv   # number of centers\n",
    "    N = Nv  # number of elements\n",
    "    M = Mv  # number of dimensions\n",
    "    (T1,T2)  = gen_cluster_data_list(C, L, N, M)\n",
    "\n",
    "    T = normalize(T1)\n",
    "    N = len(T)\n",
    "    x2_train = np.zeros((N,M),dtype='float32')\n",
    "    y2_train = np.zeros((N,C))\n",
    "    for i in range(N):\n",
    "        row = T[i]\n",
    "        for j in range(M):\n",
    "            x2_train[i,j] = row[j]\n",
    "        y2_train[i,row[-1]] = 1\n",
    "\n",
    "    Ts = normalize(T2)\n",
    "    Ns = len(Ts)\n",
    "    x2_test = np.zeros((Ns,M),dtype='float32')\n",
    "    y2_test = np.zeros((Ns,C))\n",
    "    for i in range(Ns):\n",
    "        row = Ts[i]\n",
    "        for j in range(M):\n",
    "            x2_test[i,j] = row[j]\n",
    "        y2_test[i, row[-1]] = 1\n",
    "        \n",
    "    return (x2_train,y2_train, x2_test, y2_test)\n",
    "\n",
    "def gen_data_array_s(Cv, Lv, Nv, Mv):\n",
    "    C = Cv  # number of categories\n",
    "    L = Lv   # number of centers\n",
    "    N = Nv  # number of elements\n",
    "    M = Mv  # number of dimensions\n",
    "    (T1,T2)  = gen_cluster_data_list(C, L, N, M)\n",
    "\n",
    "    T = normalize(T1)\n",
    "    N = len(T)\n",
    "    x2_train = np.zeros((N,M),dtype='float32')\n",
    "    y2_train = np.zeros((N,1))\n",
    "    for i in range(N):\n",
    "        row = T[i]\n",
    "        for j in range(M):\n",
    "            x2_train[i,j] = row[j]\n",
    "        y2_train[i] = row[-1]\n",
    "\n",
    "    Ts = normalize(T2)\n",
    "    Ns = len(Ts)\n",
    "    x2_test = np.zeros((Ns,M),dtype='float32')\n",
    "    y2_test = np.zeros((Ns,1))\n",
    "    for i in range(Ns):\n",
    "        row = Ts[i]\n",
    "        for j in range(M):\n",
    "            x2_test[i,j] = row[j]\n",
    "        y2_test[i] = row[-1]\n",
    "        \n",
    "    return (x2_train,y2_train, x2_test, y2_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method SSLayer.call of <__main__.SSLayer object at 0x000001B32D3AE490>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method SSLayer.call of <__main__.SSLayer object at 0x000001B32D3AE490>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1, Loss: 0.6886810660362244, Accuracy: 57.95555877685547, Test Accuracy: 65.7314682006836\n",
      "Epoch 2, Loss: 0.6288647055625916, Accuracy: 62.977779388427734, Test Accuracy: 58.517032623291016\n",
      "Epoch 3, Loss: 0.5578601956367493, Accuracy: 66.17778015136719, Test Accuracy: 68.93787384033203\n",
      "Epoch 4, Loss: 0.48002418875694275, Accuracy: 83.13333129882812, Test Accuracy: 85.9719467163086\n",
      "Epoch 5, Loss: 0.37433570623397827, Accuracy: 94.64444732666016, Test Accuracy: 96.79358673095703\n",
      "Epoch 6, Loss: 0.27805718779563904, Accuracy: 99.06666564941406, Test Accuracy: 98.99800109863281\n",
      "Epoch 7, Loss: 0.20542272925376892, Accuracy: 99.80000305175781, Test Accuracy: 100.0\n",
      "Epoch 8, Loss: 0.1585310697555542, Accuracy: 99.93333435058594, Test Accuracy: 100.0\n",
      "Epoch 9, Loss: 0.12773849070072174, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 10, Loss: 0.10436469316482544, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 11, Loss: 0.08524300158023834, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 12, Loss: 0.07134083658456802, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 13, Loss: 0.06163402274250984, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 14, Loss: 0.054184988141059875, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 15, Loss: 0.04804312810301781, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 16, Loss: 0.04280194267630577, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 17, Loss: 0.03818585351109505, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 18, Loss: 0.033994875848293304, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 19, Loss: 0.030284689739346504, Accuracy: 100.0, Test Accuracy: 100.0\n",
      "Epoch 20, Loss: 0.02716376632452011, Accuracy: 100.0, Test Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "# P 05\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "\n",
    "class SSLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,  num_outputs,activation=sigmoid):\n",
    "        super(SSLayer, self).__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        self.activation = activation\n",
    "        \n",
    "    def build(self, input_shape):  \n",
    "        self.kernel = self.add_weight(\"kernel\",\n",
    "                                      shape=[int(input_shape[-1]),\n",
    "                                             self.num_outputs], \n",
    "                                      initializer=tf.keras.initializers.RandomNormal(stddev=.3))\n",
    "        #print (\"kernel \", self.kernel)\n",
    "        \n",
    "        self.bias = self.add_weight(\"bias\",\n",
    "                                      shape=[self.num_outputs],\n",
    "                                    initializer=tf.keras.initializers.RandomNormal(stddev=.5))\n",
    "        \n",
    "        #print (\"bias \", self.bias)\n",
    "    \n",
    "\n",
    "    def call(self, input):\n",
    "        #print (\"CALL------------------------------\")\n",
    "        isp = input.shape\n",
    "        #print (\"input shape:\", input.shape)\n",
    "        #print (tf.ones([self.batchsize, self.num_outputs]))\n",
    "        In1 = tf.transpose(input)\n",
    "        #print (\"In1 shape:\", In1.shape)\n",
    "        In2 = tf.stack([In1] * self.kernel.shape[1]) \n",
    "        #print (\"In2 shape:\", In2.shape)\n",
    "        InD = tf.transpose(In2)\n",
    "        #InD = tf.reshape(input,[isp[0],1,isp[1]])\n",
    "        #print (\"InD shape:\", InD.shape)\n",
    "        #print (\"kernel shape\", self.kernel.shape)\n",
    "        WD = tf.stack([self.kernel] * isp[0])\n",
    "        #print (\"WD shape\", WD.shape)\n",
    "        ddd = WD - InD\n",
    "        #print (\"ddd shape:\", ddd.shape)\n",
    "        #print (\"ddd:\", ddd)\n",
    "        dd0 = tf.math.multiply(ddd, ddd)\n",
    "        #print (\"dd0 shape\",dd0.shape)\n",
    "        dd1 = tf.math.reduce_sum(dd0, axis =1)\n",
    "        #print (\"dd1 shape\",dd1.shape)\n",
    "        dd2 = tf.cast(dd1,tf.double)\n",
    "        dd3 = tf.sqrt(dd2)\n",
    "        dd3 = tf.cast(dd3,tf.float32)\n",
    "        #print (\"dd3 shape\",dd3.shape)\n",
    "        dd4 = tf.abs(self.bias)\n",
    "        result = tf.math.divide_no_nan(dd3,dd4)\n",
    "        #print (\"result shape\",result.shape)\n",
    "        #print (\"before sigmoid 1\", result)\n",
    "        rr2 = tf.ones(result.shape) -result\n",
    "        #print (\"rr2 shape\",rr2.shape)\n",
    "        rr3 = tf.math.scalar_mul(6,rr2)\n",
    "        #print (\"rr3 shape\",rr3.shape)\n",
    "        #print (\"before sigmoid\", rr3)\n",
    "        result = self.activation(rr3)\n",
    "        #print (\"result shape\",result.shape)\n",
    "        #print (\"result\", result)\n",
    "        return result\n",
    "\n",
    "\n",
    "class NN_Model(Model):\n",
    "    \n",
    "  def __init__(self,c,l,n,m,h):\n",
    "    self.C=c\n",
    "    self.L=l\n",
    "    self.N=n\n",
    "    self.M=m\n",
    "    self.H = h\n",
    "    super(NN_Model, self).__init__()\n",
    "    self.d1 = SSLayer(self.H *self.M)\n",
    "    self.d2 = Dense(self.C)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.d1(x)\n",
    "    #print (\"call benn:\",x, tf.math.reduce_sum(x))\n",
    "    return self.d2(x)\n",
    "\n",
    "@tf.function\n",
    "def train_step(datas, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(datas, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step(datas, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    \n",
    "    predictions = model(datas, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "\n",
    "C = 2\n",
    "L = 25\n",
    "N = 5000\n",
    "M = 6\n",
    "H = 5\n",
    "\n",
    "# Create an instance of the model\n",
    "model = NN_Model(C,L,N,M,H)\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "#loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "(x_train,y_train,x_test,y_test) = gen_data_array(C, L, N, M)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).batch(32)\n",
    "#print (train_ds)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "    \n",
    "EPOCHS = 20\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Reset the metrics at the start of the next epoch\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_loss.reset_states()\n",
    "  test_accuracy.reset_states()\n",
    "\n",
    "  for datas, labels in train_ds:\n",
    "    train_step(datas, labels)\n",
    "\n",
    "  for test_datas, test_labels in test_ds:\n",
    "    #print (\"test_data_shape\", test_datas.shape)\n",
    "    predictions = model(test_datas, training=False)\n",
    "    #print (\"ttttttttttttttttttt\")\n",
    "    #for i in range(test_datas.shape[0]):\n",
    "        #print (predictions.numpy()[i], test_labels.numpy()[i])\n",
    "    test_step(test_datas, test_labels)\n",
    "    \n",
    "\n",
    "  print(\n",
    "    f'Epoch {epoch + 1}, '\n",
    "    f'Loss: {train_loss.result()}, '\n",
    "    f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "    f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "  )    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.2 5.750362307426086\n"
     ]
    }
   ],
   "source": [
    "import statistics \n",
    "\n",
    "x = [100,100,100,100,100]\n",
    "print (sum(x)/len(x), statistics.stdev(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kernel:0', 'bias:0']\n",
      "out:\n",
      "kernel <tf.Variable 'kernel:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[ 0.2523557 , -0.4805397 , -0.23506409,  0.3204259 ,  0.08248037],\n",
      "       [-0.04309471, -0.03324677, -0.0968312 , -0.27222425,  0.2576189 ],\n",
      "       [ 0.2890751 ,  0.2100241 , -0.10678596,  0.07982869, -0.51990956]],\n",
      "      dtype=float32)>\n",
      "bias <tf.Variable 'bias:0' shape=(5,) dtype=float32, numpy=\n",
      "array([-0.15722479,  0.3852226 ,  0.24986114,  0.3118788 ,  0.06286097],\n",
      "      dtype=float32)>\n",
      "ddi tf.Tensor(\n",
      "[[0.2 0.2 0.2 0.2 0.2]\n",
      " [0.3 0.3 0.3 0.3 0.3]\n",
      " [0.4 0.4 0.4 0.4 0.4]], shape=(3, 5), dtype=float32)\n",
      "ddd tf.Tensor(\n",
      "[[ 0.05235569 -0.6805397  -0.43506408  0.12042589 -0.11751963]\n",
      " [-0.3430947  -0.33324677 -0.3968312  -0.57222426 -0.04238111]\n",
      " [-0.1109249  -0.1899759  -0.506786   -0.32017133 -0.9199096 ]], shape=(3, 5), dtype=float32)\n",
      "dd0 tf.Tensor(\n",
      "[[0.00274112 0.46313432 0.18928075 0.0145024  0.01381086]\n",
      " [0.11771398 0.11105341 0.15747501 0.3274406  0.00179616]\n",
      " [0.01230433 0.03609084 0.25683203 0.10250968 0.84623367]], shape=(3, 5), dtype=float32)\n",
      "dd1 tf.Tensor([0.13275944 0.6102786  0.60358775 0.44445267 0.86184067], shape=(5,), dtype=float32)\n",
      "dd2 tf.Tensor([0.13275944 0.61027861 0.60358775 0.44445267 0.86184067], shape=(5,), dtype=float64)\n",
      "dd3 tf.Tensor([0.36436167 0.7812033  0.7769091  0.6666728  0.9283537 ], shape=(5,), dtype=float32)\n",
      "dd4 tf.Tensor([0.15722479 0.3852226  0.24986114 0.3118788  0.06286097], shape=(5,), dtype=float32)\n",
      "result tf.Tensor([ 2.317457   2.027927   3.1093636  2.1376023 14.768364 ], shape=(5,), dtype=float32)\n",
      "rr2 tf.Tensor([[ -1.317457   -1.0279269  -2.1093636  -1.1376023 -13.768364 ]], shape=(1, 5), dtype=float32)\n",
      "rr3 tf.Tensor([[ -7.904742   -6.1675615 -12.656181   -6.825614  -82.61018  ]], shape=(1, 5), dtype=float32)\n",
      "result tf.Tensor([[3.6883354e-04 2.0920038e-03 3.2484531e-06 1.0844171e-03 0.0000000e+00]], shape=(1, 5), dtype=float32)\n",
      "tf.Tensor([[3.6883354e-04 2.0920038e-03 3.2484531e-06 1.0844171e-03 0.0000000e+00]], shape=(1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# P 04\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "\n",
    "class SSLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_inputs, num_outputs,activation=sigmoid):\n",
    "        super(SSLayer, self).__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        self.kernel = self.add_weight(\"kernel\",\n",
    "                                      shape=[num_inputs,\n",
    "                                             self.num_outputs], \n",
    "                                      initializer=tf.keras.initializers.RandomNormal(stddev=.3))\n",
    "        \n",
    "        self.bias = self.add_weight(\"bias\",\n",
    "                                      shape=[self.num_outputs],\n",
    "                                    initializer=tf.keras.initializers.RandomNormal(stddev=.5))\n",
    "        \n",
    "        self.activation = activation\n",
    "\n",
    "    def call(self, input):\n",
    "        print (\"kernel\", self.kernel)\n",
    "        print (\"bias\", self.bias)\n",
    "        ddi = tf.multiply( tf.transpose(input), tf.ones([1, self.num_outputs]) )\n",
    "        print (\"ddi\", ddi)\n",
    "        ddd = self.kernel-ddi\n",
    "        print (\"ddd\", ddd)\n",
    "        dd0 = tf.math.multiply(ddd, ddd)\n",
    "        print (\"dd0\", dd0)\n",
    "        dd1 = tf.math.reduce_sum(dd0, axis =0)\n",
    "        print (\"dd1\", dd1)\n",
    "        dd2 = tf.cast(dd1,tf.double)\n",
    "        print (\"dd2\", dd2)\n",
    "        dd3 = tf.sqrt(dd2)\n",
    "        dd3 = tf.cast(dd3,tf.float32)\n",
    "        print (\"dd3\", dd3)\n",
    "        dd4 = tf.abs(self.bias)\n",
    "        print (\"dd4\", dd4)\n",
    "        result = tf.math.divide_no_nan(dd3,dd4)    \n",
    "        print (\"result\", result)\n",
    "        rr2 = tf.ones([1, self.num_outputs]) -result\n",
    "        print (\"rr2\", rr2)\n",
    "        rr3 = tf.math.scalar_mul(6,rr2)\n",
    "        print (\"rr3\", rr3)\n",
    "        result = self.activation(rr3)\n",
    "        print (\"result\", result)\n",
    "        return result\n",
    "\n",
    "M_input = 3\n",
    "M_ouput = 5\n",
    "#layer = SSLayer(M_input, M_ouput)\n",
    "layer = SSLayer(M_input, M_ouput,activation=sigmoid)\n",
    "\n",
    "print([var.name for var in layer.trainable_variables])\n",
    "\n",
    "a = np.array([[0.2, 0.3, 0.4]],dtype=\"float32\")\n",
    "print (\"out:\")\n",
    "print (layer(tf.constant(a)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "tf.Tensor([[ 10.911133    6.1613445 -17.000881   -8.46829   -11.545963 ]], shape=(1, 5), dtype=float32)\n",
      "['ss_layer_13/kernel:0', 'ss_layer_13/bias:0']\n",
      "<tf.Variable 'ss_layer_13/kernel:0' shape=(3, 5) dtype=float32, numpy=\n",
      "array([[ 6.808793  , -2.8096771 , -0.8926972 ,  1.2995508 ,  4.6321363 ],\n",
      "       [ 2.1581118 , -3.11415   ,  0.6261549 , -0.4685134 ,  0.15324706],\n",
      "       [-0.28710544,  5.3778553 , -5.8494463 , -2.8967533 , -5.510189  ]],\n",
      "      dtype=float32)> <tf.Variable 'ss_layer_13/bias:0' shape=(5,) dtype=float32, numpy=\n",
      "array([ 0.15592939, -0.21368217, -0.21329248, -0.38312   , -0.35173374],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "#P 03\n",
    "\n",
    "\n",
    "class SSLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_outputs):\n",
    "        super(SSLayer, self).__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\"kernel\",\n",
    "                                      shape=[int(input_shape[-1]),\n",
    "                                             self.num_outputs], \n",
    "                                      initializer=tf.keras.initializers.RandomNormal(stddev=4))\n",
    "        \n",
    "        self.bias = self.add_weight(\"bias\",\n",
    "                                      shape=[self.num_outputs])\n",
    "\n",
    "    def call(self, input):\n",
    "        return tf.matmul(input, self.kernel)\n",
    "\n",
    "M_input = 3\n",
    "M_ouput = 5\n",
    "layer = SSLayer(M_ouput)\n",
    "\n",
    "print([var.name for var in layer.trainable_variables])\n",
    "\n",
    "a = np.array([[1.0, 2.3, 3.0]],dtype=\"float32\")\n",
    "print (layer(tf.constant(a)))\n",
    "\n",
    "print([var.name for var in layer.trainable_variables])\n",
    "print (layer.kernel, layer.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 6), (None, 2)), types: (tf.float32, tf.float64)>\n",
      "Epoch 1, Loss: 0.632636547088623, Accuracy: 64.4888916015625, Test Accuracy: 82.36473083496094\n",
      "Epoch 2, Loss: 0.5692639946937561, Accuracy: 78.37777709960938, Test Accuracy: 77.95591735839844\n",
      "Epoch 3, Loss: 0.5161746144294739, Accuracy: 78.11111450195312, Test Accuracy: 78.55711364746094\n",
      "Epoch 4, Loss: 0.46758130192756653, Accuracy: 79.33333587646484, Test Accuracy: 83.567138671875\n",
      "Epoch 5, Loss: 0.42363086342811584, Accuracy: 81.06666564941406, Test Accuracy: 88.37675476074219\n",
      "Epoch 6, Loss: 0.38263243436813354, Accuracy: 83.68888854980469, Test Accuracy: 92.58516693115234\n",
      "Epoch 7, Loss: 0.34448081254959106, Accuracy: 87.06666564941406, Test Accuracy: 93.58717346191406\n",
      "Epoch 8, Loss: 0.30898571014404297, Accuracy: 90.13333129882812, Test Accuracy: 94.7895736694336\n",
      "Epoch 9, Loss: 0.2763032615184784, Accuracy: 91.5999984741211, Test Accuracy: 96.1923828125\n",
      "Epoch 10, Loss: 0.24705345928668976, Accuracy: 92.71110534667969, Test Accuracy: 97.19438934326172\n",
      "Epoch 11, Loss: 0.220830038189888, Accuracy: 94.53333282470703, Test Accuracy: 98.19639587402344\n",
      "Epoch 12, Loss: 0.19747944176197052, Accuracy: 96.22222137451172, Test Accuracy: 99.19839477539062\n",
      "Epoch 13, Loss: 0.17673082649707794, Accuracy: 97.97777557373047, Test Accuracy: 100.0\n",
      "Epoch 14, Loss: 0.1577235609292984, Accuracy: 98.86666870117188, Test Accuracy: 100.0\n",
      "Epoch 15, Loss: 0.1405237913131714, Accuracy: 99.4000015258789, Test Accuracy: 100.0\n",
      "Epoch 16, Loss: 0.1248205229640007, Accuracy: 99.73333740234375, Test Accuracy: 100.0\n",
      "Epoch 17, Loss: 0.11105285584926605, Accuracy: 99.86666107177734, Test Accuracy: 100.0\n",
      "Epoch 18, Loss: 0.09907091408967972, Accuracy: 99.95555114746094, Test Accuracy: 100.0\n",
      "Epoch 19, Loss: 0.08861517906188965, Accuracy: 99.977783203125, Test Accuracy: 100.0\n",
      "Epoch 20, Loss: 0.07947541028261185, Accuracy: 99.977783203125, Test Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "#P 02\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "class NN_Model(Model):\n",
    "    \n",
    "  def __init__(self,c,l,n,m,h):\n",
    "    self.C=c\n",
    "    self.L=l\n",
    "    self.N=n\n",
    "    self.M=m\n",
    "    self.H = h\n",
    "    super(NN_Model, self).__init__()\n",
    "    self.d1 = Dense(self.H*self.M, input_shape=(self.M,), activation='relu')\n",
    "    self.d2 = Dense(self.C)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.d1(x)\n",
    "    return self.d2(x)\n",
    "\n",
    "@tf.function\n",
    "def train_step(datas, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(datas, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step(datas, labels):\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  predictions = model(datas, training=False)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(labels, predictions)\n",
    "\n",
    "    \n",
    "# Create an instance of the model\n",
    "model = NN_Model(2,25,5000,6,5)\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "#loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "(x_train,y_train,x_test,y_test) = gen_data_array(C, L, N, M)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).batch(32)\n",
    "print (train_ds)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "    \n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Reset the metrics at the start of the next epoch\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_loss.reset_states()\n",
    "  test_accuracy.reset_states()\n",
    "\n",
    "  for datas, labels in train_ds:\n",
    "    train_step(datas, labels)\n",
    "\n",
    "  for test_datas, test_labels in test_ds:\n",
    "    test_step(test_datas, test_labels)\n",
    "\n",
    "  print(\n",
    "    f'Epoch {epoch + 1}, '\n",
    "    f'Loss: {train_loss.result()}, '\n",
    "    f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "    f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "  )    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.875 1.7268882005337975\n"
     ]
    }
   ],
   "source": [
    "import statistics \n",
    "\n",
    "x = [95,100,98,99,100,100,99,100]\n",
    "print (sum(x)/len(x), statistics.stdev(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6351555  0.79764634 0.5939905  0.91915816 0.41782913 0.74676895]] [[0. 1.]] [[-0.5523259  -0.40917388]]\n",
      "Epoch 1/20\n",
      "141/141 [==============================] - 0s 766us/step - loss: 0.5966 - accuracy: 0.6662\n",
      "Epoch 2/20\n",
      "141/141 [==============================] - 0s 744us/step - loss: 0.5471 - accuracy: 0.7004\n",
      "Epoch 3/20\n",
      "141/141 [==============================] - 0s 702us/step - loss: 0.5087 - accuracy: 0.7658\n",
      "Epoch 4/20\n",
      "141/141 [==============================] - 0s 730us/step - loss: 0.4666 - accuracy: 0.7889\n",
      "Epoch 5/20\n",
      "141/141 [==============================] - 0s 766us/step - loss: 0.4238 - accuracy: 0.8027\n",
      "Epoch 6/20\n",
      "141/141 [==============================] - 0s 731us/step - loss: 0.3838 - accuracy: 0.8262\n",
      "Epoch 7/20\n",
      "141/141 [==============================] - 0s 723us/step - loss: 0.3483 - accuracy: 0.8564\n",
      "Epoch 8/20\n",
      "141/141 [==============================] - 0s 738us/step - loss: 0.3147 - accuracy: 0.8873\n",
      "Epoch 9/20\n",
      "141/141 [==============================] - 0s 709us/step - loss: 0.2859 - accuracy: 0.9120\n",
      "Epoch 10/20\n",
      "141/141 [==============================] - 0s 738us/step - loss: 0.2599 - accuracy: 0.9371\n",
      "Epoch 11/20\n",
      "141/141 [==============================] - 0s 723us/step - loss: 0.2372 - accuracy: 0.9527\n",
      "Epoch 12/20\n",
      "141/141 [==============================] - 0s 709us/step - loss: 0.2171 - accuracy: 0.9662\n",
      "Epoch 13/20\n",
      "141/141 [==============================] - 0s 709us/step - loss: 0.1990 - accuracy: 0.9744\n",
      "Epoch 14/20\n",
      "141/141 [==============================] - 0s 744us/step - loss: 0.1822 - accuracy: 0.9829\n",
      "Epoch 15/20\n",
      "141/141 [==============================] - 0s 723us/step - loss: 0.1678 - accuracy: 0.9887\n",
      "Epoch 16/20\n",
      "141/141 [==============================] - 0s 738us/step - loss: 0.1542 - accuracy: 0.9900\n",
      "Epoch 17/20\n",
      "141/141 [==============================] - 0s 709us/step - loss: 0.1411 - accuracy: 0.9942\n",
      "Epoch 18/20\n",
      "141/141 [==============================] - 0s 724us/step - loss: 0.1298 - accuracy: 0.9960\n",
      "Epoch 19/20\n",
      "141/141 [==============================] - 0s 709us/step - loss: 0.1194 - accuracy: 0.9980\n",
      "Epoch 20/20\n",
      "141/141 [==============================] - 0s 745us/step - loss: 0.1095 - accuracy: 0.9993\n",
      "16/16 - 0s - loss: 0.0956 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09563780575990677, 1.0]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#P 01\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "C=2\n",
    "L=25\n",
    "N=5000\n",
    "M=6\n",
    "H = 5\n",
    "#(x_train,y_train,x_test,y_test) = gen_data_array_s(C, L, N, M)\n",
    "(x_train,y_train,x_test,y_test) = gen_data_array(C, L, N, M)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(H*M, input_shape=(M,), activation='relu'),\n",
    "  tf.keras.layers.Dense(C)\n",
    "])\n",
    "\n",
    "predictions = model(x_train[:1]).numpy()\n",
    "print(x_train[:1],y_train[:1], predictions)\n",
    "\n",
    "#loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=20)\n",
    "model.evaluate(x_test,  y_test, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.77777777777777 1.6414763002993509\n"
     ]
    }
   ],
   "source": [
    "import statistics \n",
    "\n",
    "x = [96,95,96,99,99,98,99,99,99]\n",
    "print (sum(x)/len(x), statistics.stdev(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.04517666, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "x = tf.constant(3.0)\n",
    "c1 = tf.constant(1.0)\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    #y = tf.math.sin(x * x)\n",
    "    y = tf.math.divide(c1, c1 + tf.math.exp(-x))\n",
    "dy_dx = g.gradient(y, x) \n",
    "print(dy_dx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor([2.82842712 1.41421356], shape=(2,), dtype=float64)\n",
      "tf.Tensor([1.41421356 0.70710678], shape=(2,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1, 2, 3], [1,4,1]])\n",
    "ta = tf.constant(a)\n",
    "b = np.array([[3, 2, 1],[1,3,2]])\n",
    "tb = tf.constant(b)\n",
    "tm = tf.math.multiply(ta-tb, ta-tb)\n",
    "dd = tf.math.reduce_sum(tm, axis =1)\n",
    "d2 = tf.cast(dd,tf.double)\n",
    "d3 = tf.sqrt(d2)\n",
    "dr = tf.fill([2], 2)\n",
    "dr2= tf.cast(dr,tf.double)\n",
    "print (dr)\n",
    "out = tf.math.divide_no_nan(d3,dr2)\n",
    "print (d3)\n",
    "print (out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.3 3.  5.2]\n",
      " [1.3 3.  5.2]\n",
      " [1.3 3.  5.2]\n",
      " [1.3 3.  5.2]], shape=(4, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print (tf.multiply( tf.ones([4,1]),tf.constant([1.3,3.0,5.2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1 2]], shape=(1, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[2]\n",
      " [1]], shape=(2, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[2 4]\n",
      " [1 2]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1, 2]])\n",
    "y = tf.constant([[2], [1]])\n",
    "print (x)\n",
    "print (y)\n",
    "z = tf.matmul(y,x)\n",
    "print (z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1. 1.]\n",
      "  [1. 1.]]\n",
      "\n",
      " [[1. 1.]\n",
      "  [1. 1.]]\n",
      "\n",
      " [[1. 1.]\n",
      "  [1. 1.]]], shape=(3, 2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[3. 2.]\n",
      " [4. 3.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[7. 5.]\n",
      "  [7. 5.]]\n",
      "\n",
      " [[7. 5.]\n",
      "  [7. 5.]]\n",
      "\n",
      " [[7. 5.]\n",
      "  [7. 5.]]], shape=(3, 2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones([3,2,2])\n",
    "y = tf.constant([  [3., 2.], [4.,3.]])\n",
    "print (x)\n",
    "print (y)\n",
    "z = tf.matmul(x,y)\n",
    "print (z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 3)\n",
      "(10, 3, 12)\n",
      "(10, 1, 12)\n"
     ]
    }
   ],
   "source": [
    "In = tf.constant([[0.8914187,  0.44162735, 0.2808851 ],\n",
    " [0.8515105,  0.46186763, 0.24088585],\n",
    " [0.67635417, 0.07768797, 0.1056074 ],\n",
    " [0.942432,   0.7850282,  0.04864224],\n",
    " [0.13471572, 0.22203955, 0.6862668 ],\n",
    " [0.6970358,  0.06602696, 0.04721393],\n",
    " [0.06776198, 0.5567025,  0.9401386 ],\n",
    " [0.92003983, 0.8369356,  0.01939357],\n",
    " [0.8942079,  0.41677353, 0.26042283],\n",
    " [0.95389813, 0.7809099,  0.05973615]])\n",
    "InD = tf.reshape(In,[10,1,3]) \n",
    "print (InD.shape)\n",
    "W = tf.constant([[ 0.64815485, -0.3559605 ,  0.20014353, -0.28112715,  0.09535905,\n",
    "        -0.35034025,  0.12817992, -0.5305331 ,  0.03162672, -0.08267911,\n",
    "        -0.02133827,  0.07015286],\n",
    "       [-0.03005282, -0.5115979 ,  0.08203132,  0.16849105,  0.4781897 ,\n",
    "         0.18680447, -0.02384542, -0.03511131,  0.14952978,  0.80725867,\n",
    "         0.11003426, -0.39055902],\n",
    "       [ 0.32035694, -0.15671906,  0.5563809 , -0.5291747 ,  0.15807222,\n",
    "        -0.5098055 ,  0.26924753,  0.06294397, -0.16664729, -0.16909388,\n",
    "         0.34415245,  0.04050118]])\n",
    "WD = tf.stack([W] * 10)\n",
    "print (WD.shape)\n",
    "R = tf.matmul(InD, WD)\n",
    "print (R.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 3]\n",
      " [4 5 6]], shape=(2, 3), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[1 2 3]\n",
      "  [4 5 6]]\n",
      "\n",
      " [[1 2 3]\n",
      "  [4 5 6]]\n",
      "\n",
      " [[1 2 3]\n",
      "  [4 5 6]]\n",
      "\n",
      " [[1 2 3]\n",
      "  [4 5 6]]], shape=(4, 2, 3), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 4  8 12]\n",
      " [16 20 24]], shape=(2, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "vec = tf.constant([[1,2,3],[4,5,6]])  # shape=(2,3)\n",
    "matrix = tf.stack([vec] * 4)  # shape=(4,2,3)\n",
    "\n",
    "\n",
    "print(vec)\n",
    "print(matrix)\n",
    "print (tf.math.reduce_sum(matrix,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.6 11.880890351970915\n"
     ]
    }
   ],
   "source": [
    "import statistics \n",
    "\n",
    "x = [76,72,69,82,70,60,89,90,100,78]\n",
    "print (sum(x)/len(x), statistics.stdev(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
