{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "special-slovakia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.90179956 0.9417195  0.37377474 0.07043936 0.53236914 0.7381097 ]\n",
      " [0.5166569  0.40206912 0.8782092  0.26794338 0.8631228  0.28210485]]\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method LSLayer.call of <__main__.LSLayer object at 0x0000019389C3D2E0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method LSLayer.call of <__main__.LSLayer object at 0x0000019389C3D2E0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1, Loss: 1.7645692825317383, Accuracy: 23.822221755981445, Test Accuracy: 32.46493148803711\n",
      "Epoch 21, Loss: 1.261717438697815, Accuracy: 50.666664123535156, Test Accuracy: 48.296592712402344\n",
      "Epoch 41, Loss: 1.132439374923706, Accuracy: 53.511112213134766, Test Accuracy: 52.705413818359375\n",
      "Epoch 61, Loss: 1.0625193119049072, Accuracy: 57.22222137451172, Test Accuracy: 55.91181945800781\n",
      "Epoch 81, Loss: 0.995327353477478, Accuracy: 63.82221984863281, Test Accuracy: 61.52304458618164\n",
      "Epoch 101, Loss: 0.9360077977180481, Accuracy: 66.73332977294922, Test Accuracy: 66.93386840820312\n",
      "Epoch 121, Loss: 0.8954169154167175, Accuracy: 67.5777816772461, Test Accuracy: 69.13827514648438\n",
      "Epoch 141, Loss: 0.8570539355278015, Accuracy: 71.9111099243164, Test Accuracy: 69.7394790649414\n",
      "Epoch 161, Loss: 0.7464721202850342, Accuracy: 78.51111602783203, Test Accuracy: 75.75150299072266\n",
      "Epoch 181, Loss: 0.6930209398269653, Accuracy: 81.06666564941406, Test Accuracy: 78.55711364746094\n",
      "Epoch 201, Loss: 0.6534764766693115, Accuracy: 82.4000015258789, Test Accuracy: 78.95791625976562\n",
      "Epoch 221, Loss: 0.6217501759529114, Accuracy: 82.64444732666016, Test Accuracy: 79.55912017822266\n",
      "Epoch 241, Loss: 0.596244215965271, Accuracy: 82.66666412353516, Test Accuracy: 78.95791625976562\n",
      "Epoch 261, Loss: 0.5753724575042725, Accuracy: 82.86666870117188, Test Accuracy: 79.15831756591797\n",
      "Epoch 281, Loss: 0.5578983426094055, Accuracy: 83.13333129882812, Test Accuracy: 79.15831756591797\n",
      "Epoch 301, Loss: 0.5429679751396179, Accuracy: 83.55555725097656, Test Accuracy: 79.15831756591797\n",
      "Epoch 321, Loss: 0.529992401599884, Accuracy: 84.02222442626953, Test Accuracy: 79.15831756591797\n",
      "Epoch 341, Loss: 0.5185598134994507, Accuracy: 84.64443969726562, Test Accuracy: 79.35871124267578\n",
      "Epoch 361, Loss: 0.508375346660614, Accuracy: 85.22222137451172, Test Accuracy: 79.759521484375\n",
      "Epoch 381, Loss: 0.4992237985134125, Accuracy: 85.55555725097656, Test Accuracy: 79.55912017822266\n",
      "Epoch 401, Loss: 0.4909440279006958, Accuracy: 85.93333435058594, Test Accuracy: 78.75751495361328\n",
      "Epoch 421, Loss: 0.4834107756614685, Accuracy: 86.13333129882812, Test Accuracy: 79.35871124267578\n",
      "Epoch 441, Loss: 0.47652462124824524, Accuracy: 86.28888702392578, Test Accuracy: 79.15831756591797\n",
      "Epoch 461, Loss: 0.4702041745185852, Accuracy: 86.4888916015625, Test Accuracy: 79.55912017822266\n",
      "Epoch 481, Loss: 0.46438100934028625, Accuracy: 86.44444274902344, Test Accuracy: 79.55912017822266\n",
      "Epoch 501, Loss: 0.458997905254364, Accuracy: 86.44444274902344, Test Accuracy: 79.35871124267578\n",
      "Epoch 521, Loss: 0.45400470495224, Accuracy: 86.42222595214844, Test Accuracy: 79.35871124267578\n",
      "Epoch 541, Loss: 0.4493591785430908, Accuracy: 86.31111145019531, Test Accuracy: 79.15831756591797\n",
      "Epoch 561, Loss: 0.44502365589141846, Accuracy: 86.28888702392578, Test Accuracy: 79.55912017822266\n",
      "Epoch 581, Loss: 0.4409659206867218, Accuracy: 86.24444580078125, Test Accuracy: 79.35871124267578\n",
      "Epoch 601, Loss: 0.437157541513443, Accuracy: 86.33333587646484, Test Accuracy: 79.35871124267578\n",
      "Epoch 621, Loss: 0.43357428908348083, Accuracy: 86.26667022705078, Test Accuracy: 79.55912017822266\n",
      "Epoch 641, Loss: 0.43019405007362366, Accuracy: 86.35555267333984, Test Accuracy: 80.16031646728516\n",
      "Epoch 661, Loss: 0.4269973933696747, Accuracy: 86.46666717529297, Test Accuracy: 80.16031646728516\n",
      "Epoch 681, Loss: 0.42396795749664307, Accuracy: 86.5111083984375, Test Accuracy: 80.16031646728516\n",
      "Epoch 701, Loss: 0.4210904836654663, Accuracy: 86.55555725097656, Test Accuracy: 80.36072540283203\n",
      "Epoch 721, Loss: 0.4183516800403595, Accuracy: 86.5999984741211, Test Accuracy: 80.36072540283203\n",
      "Epoch 741, Loss: 0.41573962569236755, Accuracy: 86.66666412353516, Test Accuracy: 80.36072540283203\n",
      "Epoch 761, Loss: 0.41324394941329956, Accuracy: 86.82221984863281, Test Accuracy: 80.36072540283203\n",
      "Epoch 781, Loss: 0.4108549654483795, Accuracy: 86.77777862548828, Test Accuracy: 80.36072540283203\n",
      "Epoch 801, Loss: 0.40856412053108215, Accuracy: 86.9111099243164, Test Accuracy: 80.36072540283203\n",
      "Epoch 821, Loss: 0.4063635766506195, Accuracy: 87.0, Test Accuracy: 80.16031646728516\n",
      "Epoch 841, Loss: 0.40424636006355286, Accuracy: 87.06666564941406, Test Accuracy: 80.36072540283203\n",
      "Epoch 861, Loss: 0.4022059738636017, Accuracy: 87.13333892822266, Test Accuracy: 80.56111907958984\n",
      "Epoch 881, Loss: 0.4002365469932556, Accuracy: 87.17778015136719, Test Accuracy: 80.56111907958984\n",
      "Epoch 901, Loss: 0.3983331322669983, Accuracy: 87.26666259765625, Test Accuracy: 80.56111907958984\n",
      "Epoch 921, Loss: 0.3964899480342865, Accuracy: 87.35555267333984, Test Accuracy: 80.96192169189453\n",
      "Epoch 941, Loss: 0.3947030305862427, Accuracy: 87.37777709960938, Test Accuracy: 80.96192169189453\n",
      "Epoch 961, Loss: 0.3929671347141266, Accuracy: 87.4888916015625, Test Accuracy: 80.96192169189453\n",
      "Epoch 981, Loss: 0.3912777900695801, Accuracy: 87.55555725097656, Test Accuracy: 80.96192169189453\n",
      "Epoch 1001, Loss: 0.3896309733390808, Accuracy: 87.5111083984375, Test Accuracy: 80.96192169189453\n",
      "Epoch 1021, Loss: 0.38802218437194824, Accuracy: 87.5777816772461, Test Accuracy: 80.96192169189453\n",
      "Epoch 1041, Loss: 0.3864462971687317, Accuracy: 87.55555725097656, Test Accuracy: 80.96192169189453\n",
      "Epoch 1061, Loss: 0.38489797711372375, Accuracy: 87.64444732666016, Test Accuracy: 80.96192169189453\n",
      "Epoch 1081, Loss: 0.3833710551261902, Accuracy: 87.75555419921875, Test Accuracy: 81.1623306274414\n",
      "Epoch 1101, Loss: 0.3818585276603699, Accuracy: 87.86666107177734, Test Accuracy: 81.36272430419922\n",
      "Epoch 1121, Loss: 0.3803507685661316, Accuracy: 87.9111099243164, Test Accuracy: 80.96192169189453\n",
      "Epoch 1141, Loss: 0.3788371980190277, Accuracy: 87.95555877685547, Test Accuracy: 81.1623306274414\n",
      "Epoch 1161, Loss: 0.37730175256729126, Accuracy: 87.97777557373047, Test Accuracy: 81.1623306274414\n",
      "Epoch 1181, Loss: 0.37572404742240906, Accuracy: 88.1111068725586, Test Accuracy: 81.1623306274414\n",
      "Epoch 1201, Loss: 0.3740760087966919, Accuracy: 88.04444885253906, Test Accuracy: 81.1623306274414\n",
      "Epoch 1221, Loss: 0.37232282757759094, Accuracy: 87.97777557373047, Test Accuracy: 80.96192169189453\n",
      "Epoch 1241, Loss: 0.3704304099082947, Accuracy: 87.8888931274414, Test Accuracy: 80.96192169189453\n",
      "Epoch 1261, Loss: 0.36838799715042114, Accuracy: 87.9111099243164, Test Accuracy: 80.76152038574219\n",
      "Epoch 1281, Loss: 0.3662322163581848, Accuracy: 88.022216796875, Test Accuracy: 81.36272430419922\n",
      "Epoch 1301, Loss: 0.364041268825531, Accuracy: 88.26666259765625, Test Accuracy: 81.7635269165039\n",
      "Epoch 1321, Loss: 0.3618946671485901, Accuracy: 88.26666259765625, Test Accuracy: 81.7635269165039\n",
      "Epoch 1341, Loss: 0.35984405875205994, Accuracy: 88.33333587646484, Test Accuracy: 82.1643295288086\n",
      "Epoch 1361, Loss: 0.3579079508781433, Accuracy: 88.37777709960938, Test Accuracy: 81.7635269165039\n",
      "Epoch 1381, Loss: 0.35608500242233276, Accuracy: 88.4888916015625, Test Accuracy: 81.7635269165039\n",
      "Epoch 1401, Loss: 0.3543635904788971, Accuracy: 88.57777404785156, Test Accuracy: 81.56312561035156\n",
      "Epoch 1421, Loss: 0.3527306318283081, Accuracy: 88.44444274902344, Test Accuracy: 81.56312561035156\n",
      "Epoch 1441, Loss: 0.35117337107658386, Accuracy: 88.5111083984375, Test Accuracy: 81.96392822265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1461, Loss: 0.34968113899230957, Accuracy: 88.5999984741211, Test Accuracy: 81.96392822265625\n",
      "Epoch 1481, Loss: 0.3482445478439331, Accuracy: 88.55555725097656, Test Accuracy: 81.7635269165039\n",
      "Epoch 1501, Loss: 0.3468565046787262, Accuracy: 88.53333282470703, Test Accuracy: 81.96392822265625\n",
      "Epoch 1521, Loss: 0.34551090002059937, Accuracy: 88.4888916015625, Test Accuracy: 81.96392822265625\n",
      "Epoch 1541, Loss: 0.34420260787010193, Accuracy: 88.57777404785156, Test Accuracy: 81.7635269165039\n",
      "Epoch 1561, Loss: 0.34292712807655334, Accuracy: 88.57777404785156, Test Accuracy: 81.96392822265625\n",
      "Epoch 1581, Loss: 0.3416807949542999, Accuracy: 88.57777404785156, Test Accuracy: 81.96392822265625\n",
      "Epoch 1601, Loss: 0.340459942817688, Accuracy: 88.64444732666016, Test Accuracy: 81.96392822265625\n",
      "Epoch 1621, Loss: 0.3392617404460907, Accuracy: 88.73332977294922, Test Accuracy: 81.96392822265625\n",
      "Epoch 1641, Loss: 0.338082879781723, Accuracy: 88.77777862548828, Test Accuracy: 81.7635269165039\n",
      "Epoch 1661, Loss: 0.3369208574295044, Accuracy: 88.86666870117188, Test Accuracy: 81.56312561035156\n",
      "Epoch 1681, Loss: 0.33577340841293335, Accuracy: 88.9111099243164, Test Accuracy: 81.56312561035156\n",
      "Epoch 1701, Loss: 0.3346373736858368, Accuracy: 88.9111099243164, Test Accuracy: 81.56312561035156\n",
      "Epoch 1721, Loss: 0.3335103392601013, Accuracy: 89.02222442626953, Test Accuracy: 81.56312561035156\n",
      "Epoch 1741, Loss: 0.33239036798477173, Accuracy: 89.0888900756836, Test Accuracy: 81.36272430419922\n",
      "Epoch 1761, Loss: 0.33127501606941223, Accuracy: 89.20000457763672, Test Accuracy: 81.7635269165039\n",
      "Epoch 1781, Loss: 0.3301621079444885, Accuracy: 89.26666259765625, Test Accuracy: 81.7635269165039\n",
      "Epoch 1801, Loss: 0.3290489614009857, Accuracy: 89.44444274902344, Test Accuracy: 81.56312561035156\n",
      "Epoch 1821, Loss: 0.3279336988925934, Accuracy: 89.5111083984375, Test Accuracy: 81.56312561035156\n",
      "Epoch 1841, Loss: 0.32681435346603394, Accuracy: 89.62222290039062, Test Accuracy: 81.56312561035156\n",
      "Epoch 1861, Loss: 0.32568925619125366, Accuracy: 89.62222290039062, Test Accuracy: 81.7635269165039\n",
      "Epoch 1881, Loss: 0.3245568573474884, Accuracy: 89.71111297607422, Test Accuracy: 81.7635269165039\n",
      "Epoch 1901, Loss: 0.323415607213974, Accuracy: 89.86666870117188, Test Accuracy: 81.56312561035156\n",
      "Epoch 1921, Loss: 0.3222646415233612, Accuracy: 89.97777557373047, Test Accuracy: 81.96392822265625\n",
      "Epoch 1941, Loss: 0.32110318541526794, Accuracy: 90.13333129882812, Test Accuracy: 81.96392822265625\n",
      "Epoch 1961, Loss: 0.3199304938316345, Accuracy: 90.15555572509766, Test Accuracy: 82.1643295288086\n",
      "Epoch 1981, Loss: 0.31874722242355347, Accuracy: 90.24444580078125, Test Accuracy: 82.1643295288086\n",
      "Epoch 2001, Loss: 0.3175535500049591, Accuracy: 90.26667022705078, Test Accuracy: 82.1643295288086\n",
      "Epoch 2021, Loss: 0.31634968519210815, Accuracy: 90.44444274902344, Test Accuracy: 82.1643295288086\n",
      "Epoch 2041, Loss: 0.31513723731040955, Accuracy: 90.46666717529297, Test Accuracy: 82.36473083496094\n",
      "Epoch 2061, Loss: 0.3139176666736603, Accuracy: 90.5999984741211, Test Accuracy: 82.36473083496094\n",
      "Epoch 2081, Loss: 0.3126920163631439, Accuracy: 90.71111297607422, Test Accuracy: 82.1643295288086\n",
      "Epoch 2101, Loss: 0.31146225333213806, Accuracy: 90.80000305175781, Test Accuracy: 82.36473083496094\n",
      "Epoch 2121, Loss: 0.31023097038269043, Accuracy: 90.93333435058594, Test Accuracy: 82.36473083496094\n",
      "Epoch 2141, Loss: 0.3089991807937622, Accuracy: 91.04444122314453, Test Accuracy: 82.56513214111328\n",
      "Epoch 2161, Loss: 0.30777034163475037, Accuracy: 91.06666564941406, Test Accuracy: 82.56513214111328\n",
      "Epoch 2181, Loss: 0.306546688079834, Accuracy: 91.11111450195312, Test Accuracy: 82.56513214111328\n",
      "Epoch 2201, Loss: 0.30533072352409363, Accuracy: 91.17778015136719, Test Accuracy: 82.76553344726562\n",
      "Epoch 2221, Loss: 0.3041244149208069, Accuracy: 91.22222137451172, Test Accuracy: 82.56513214111328\n",
      "Epoch 2241, Loss: 0.30293112993240356, Accuracy: 91.24444580078125, Test Accuracy: 82.96593475341797\n",
      "Epoch 2261, Loss: 0.3017529845237732, Accuracy: 91.26667022705078, Test Accuracy: 82.96593475341797\n",
      "Epoch 2281, Loss: 0.30059197545051575, Accuracy: 91.26667022705078, Test Accuracy: 83.36673736572266\n",
      "Epoch 2301, Loss: 0.2994500398635864, Accuracy: 91.28888702392578, Test Accuracy: 83.16632843017578\n",
      "Epoch 2321, Loss: 0.29832905530929565, Accuracy: 91.28888702392578, Test Accuracy: 83.16632843017578\n",
      "Epoch 2341, Loss: 0.29723021388053894, Accuracy: 91.31111145019531, Test Accuracy: 83.16632843017578\n",
      "Epoch 2361, Loss: 0.29615479707717896, Accuracy: 91.35555267333984, Test Accuracy: 82.96593475341797\n",
      "Epoch 2381, Loss: 0.2951025664806366, Accuracy: 91.37777709960938, Test Accuracy: 82.96593475341797\n",
      "Epoch 2401, Loss: 0.2940741181373596, Accuracy: 91.39999389648438, Test Accuracy: 82.96593475341797\n",
      "Epoch 2421, Loss: 0.293069064617157, Accuracy: 91.42222595214844, Test Accuracy: 82.96593475341797\n",
      "Epoch 2441, Loss: 0.29208633303642273, Accuracy: 91.4888916015625, Test Accuracy: 82.76553344726562\n",
      "Epoch 2461, Loss: 0.29112574458122253, Accuracy: 91.4888916015625, Test Accuracy: 82.96593475341797\n",
      "Epoch 2481, Loss: 0.29018697142601013, Accuracy: 91.55555725097656, Test Accuracy: 82.96593475341797\n",
      "Epoch 2501, Loss: 0.28926804661750793, Accuracy: 91.55555725097656, Test Accuracy: 82.96593475341797\n",
      "Epoch 2521, Loss: 0.28836825489997864, Accuracy: 91.55555725097656, Test Accuracy: 82.96593475341797\n",
      "Epoch 2541, Loss: 0.28748753666877747, Accuracy: 91.5999984741211, Test Accuracy: 82.76553344726562\n",
      "Epoch 2561, Loss: 0.28662392497062683, Accuracy: 91.62222290039062, Test Accuracy: 82.76553344726562\n",
      "Epoch 2581, Loss: 0.2857764959335327, Accuracy: 91.64443969726562, Test Accuracy: 82.76553344726562\n",
      "Epoch 2601, Loss: 0.2849448323249817, Accuracy: 91.68888854980469, Test Accuracy: 82.76553344726562\n",
      "Epoch 2621, Loss: 0.28412726521492004, Accuracy: 91.71111297607422, Test Accuracy: 82.56513214111328\n",
      "Epoch 2641, Loss: 0.28332340717315674, Accuracy: 91.75555419921875, Test Accuracy: 82.56513214111328\n",
      "Epoch 2661, Loss: 0.2825324237346649, Accuracy: 91.77777862548828, Test Accuracy: 82.56513214111328\n",
      "Epoch 2681, Loss: 0.28175321221351624, Accuracy: 91.82222747802734, Test Accuracy: 82.56513214111328\n",
      "Epoch 2701, Loss: 0.28098493814468384, Accuracy: 91.82222747802734, Test Accuracy: 82.36473083496094\n",
      "Epoch 2721, Loss: 0.2802262008190155, Accuracy: 91.86666870117188, Test Accuracy: 82.1643295288086\n",
      "Epoch 2741, Loss: 0.2794765830039978, Accuracy: 91.86666870117188, Test Accuracy: 82.1643295288086\n",
      "Epoch 2761, Loss: 0.27873495221138, Accuracy: 91.93333435058594, Test Accuracy: 82.1643295288086\n",
      "Epoch 2781, Loss: 0.2780006229877472, Accuracy: 91.93333435058594, Test Accuracy: 82.1643295288086\n",
      "Epoch 2801, Loss: 0.2772725224494934, Accuracy: 91.95555114746094, Test Accuracy: 82.1643295288086\n",
      "Epoch 2821, Loss: 0.27655020356178284, Accuracy: 91.95555114746094, Test Accuracy: 82.56513214111328\n",
      "Epoch 2841, Loss: 0.2758321166038513, Accuracy: 91.97777557373047, Test Accuracy: 82.56513214111328\n",
      "Epoch 2861, Loss: 0.27511754631996155, Accuracy: 92.0, Test Accuracy: 82.56513214111328\n",
      "Epoch 2881, Loss: 0.27440592646598816, Accuracy: 92.04444122314453, Test Accuracy: 82.56513214111328\n",
      "Epoch 2901, Loss: 0.273695707321167, Accuracy: 92.02222442626953, Test Accuracy: 82.76553344726562\n",
      "Epoch 2921, Loss: 0.27298691868782043, Accuracy: 92.0, Test Accuracy: 82.76553344726562\n",
      "Epoch 2941, Loss: 0.27227818965911865, Accuracy: 92.0, Test Accuracy: 82.76553344726562\n",
      "Epoch 2961, Loss: 0.27156883478164673, Accuracy: 92.02222442626953, Test Accuracy: 82.96593475341797\n",
      "Epoch 2981, Loss: 0.2708579897880554, Accuracy: 92.02222442626953, Test Accuracy: 82.96593475341797\n",
      "Epoch 3001, Loss: 0.27014589309692383, Accuracy: 91.97777557373047, Test Accuracy: 82.76553344726562\n",
      "Epoch 3021, Loss: 0.2694312632083893, Accuracy: 91.97777557373047, Test Accuracy: 82.76553344726562\n",
      "Epoch 3041, Loss: 0.26871368288993835, Accuracy: 92.04444122314453, Test Accuracy: 82.96593475341797\n",
      "Epoch 3061, Loss: 0.267993688583374, Accuracy: 92.06666564941406, Test Accuracy: 82.96593475341797\n",
      "Epoch 3081, Loss: 0.267270565032959, Accuracy: 92.15555572509766, Test Accuracy: 82.96593475341797\n",
      "Epoch 3101, Loss: 0.26654472947120667, Accuracy: 92.19999694824219, Test Accuracy: 82.96593475341797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3121, Loss: 0.26581647992134094, Accuracy: 92.19999694824219, Test Accuracy: 82.96593475341797\n",
      "Epoch 3141, Loss: 0.26508650183677673, Accuracy: 92.22222137451172, Test Accuracy: 82.96593475341797\n",
      "Epoch 3161, Loss: 0.264355331659317, Accuracy: 92.24444580078125, Test Accuracy: 82.96593475341797\n",
      "Epoch 3181, Loss: 0.2636236548423767, Accuracy: 92.28888702392578, Test Accuracy: 82.96593475341797\n",
      "Epoch 3201, Loss: 0.26289206743240356, Accuracy: 92.26667022705078, Test Accuracy: 82.76553344726562\n",
      "Epoch 3221, Loss: 0.2621612846851349, Accuracy: 92.24444580078125, Test Accuracy: 82.76553344726562\n",
      "Epoch 3241, Loss: 0.2614317536354065, Accuracy: 92.24444580078125, Test Accuracy: 82.76553344726562\n",
      "Epoch 3261, Loss: 0.2607046067714691, Accuracy: 92.26667022705078, Test Accuracy: 82.96593475341797\n",
      "Epoch 3281, Loss: 0.25998038053512573, Accuracy: 92.24444580078125, Test Accuracy: 82.96593475341797\n",
      "Epoch 3301, Loss: 0.2592597007751465, Accuracy: 92.24444580078125, Test Accuracy: 82.96593475341797\n",
      "Epoch 3321, Loss: 0.2585430145263672, Accuracy: 92.26667022705078, Test Accuracy: 82.96593475341797\n",
      "Epoch 3341, Loss: 0.257830947637558, Accuracy: 92.28888702392578, Test Accuracy: 82.96593475341797\n",
      "Epoch 3361, Loss: 0.2571234107017517, Accuracy: 92.28888702392578, Test Accuracy: 82.96593475341797\n",
      "Epoch 3381, Loss: 0.25642117857933044, Accuracy: 92.31111145019531, Test Accuracy: 83.16632843017578\n",
      "Epoch 3401, Loss: 0.2557242810726166, Accuracy: 92.28888702392578, Test Accuracy: 83.16632843017578\n",
      "Epoch 3421, Loss: 0.25503313541412354, Accuracy: 92.31111145019531, Test Accuracy: 83.16632843017578\n",
      "Epoch 3441, Loss: 0.25434765219688416, Accuracy: 92.4000015258789, Test Accuracy: 83.16632843017578\n",
      "Epoch 3461, Loss: 0.25366753339767456, Accuracy: 92.42222595214844, Test Accuracy: 83.16632843017578\n",
      "Epoch 3481, Loss: 0.2529936730861664, Accuracy: 92.42222595214844, Test Accuracy: 83.36673736572266\n",
      "Epoch 3501, Loss: 0.2523256540298462, Accuracy: 92.42222595214844, Test Accuracy: 83.36673736572266\n",
      "Epoch 3521, Loss: 0.2516637146472931, Accuracy: 92.44444274902344, Test Accuracy: 83.36673736572266\n",
      "Epoch 3541, Loss: 0.25100764632225037, Accuracy: 92.42222595214844, Test Accuracy: 83.36673736572266\n",
      "Epoch 3561, Loss: 0.25035759806632996, Accuracy: 92.4000015258789, Test Accuracy: 83.36673736572266\n",
      "Epoch 3581, Loss: 0.24971336126327515, Accuracy: 92.37777709960938, Test Accuracy: 83.16632843017578\n",
      "Epoch 3601, Loss: 0.24907518923282623, Accuracy: 92.37777709960938, Test Accuracy: 82.96593475341797\n",
      "Epoch 3621, Loss: 0.24844323098659515, Accuracy: 92.44444274902344, Test Accuracy: 83.16632843017578\n",
      "Epoch 3641, Loss: 0.2478175312280655, Accuracy: 92.35555267333984, Test Accuracy: 83.16632843017578\n",
      "Epoch 3661, Loss: 0.2471981942653656, Accuracy: 92.37777709960938, Test Accuracy: 83.16632843017578\n",
      "Epoch 3681, Loss: 0.24658522009849548, Accuracy: 92.37777709960938, Test Accuracy: 83.16632843017578\n",
      "Epoch 3701, Loss: 0.24597933888435364, Accuracy: 92.42222595214844, Test Accuracy: 83.16632843017578\n",
      "Epoch 3721, Loss: 0.24538040161132812, Accuracy: 92.44444274902344, Test Accuracy: 83.16632843017578\n",
      "Epoch 3741, Loss: 0.24478888511657715, Accuracy: 92.4000015258789, Test Accuracy: 82.96593475341797\n",
      "Epoch 3761, Loss: 0.24420583248138428, Accuracy: 92.44444274902344, Test Accuracy: 82.96593475341797\n",
      "Epoch 3781, Loss: 0.24363134801387787, Accuracy: 92.44444274902344, Test Accuracy: 82.96593475341797\n",
      "Epoch 3801, Loss: 0.24306620657444, Accuracy: 92.4888916015625, Test Accuracy: 82.96593475341797\n",
      "Epoch 3821, Loss: 0.24251091480255127, Accuracy: 92.5111083984375, Test Accuracy: 83.16632843017578\n",
      "Epoch 3841, Loss: 0.24196602404117584, Accuracy: 92.5111083984375, Test Accuracy: 83.16632843017578\n",
      "Epoch 3861, Loss: 0.24143218994140625, Accuracy: 92.44444274902344, Test Accuracy: 83.16632843017578\n",
      "Epoch 3881, Loss: 0.2409096509218216, Accuracy: 92.44444274902344, Test Accuracy: 82.96593475341797\n",
      "Epoch 3901, Loss: 0.24039873480796814, Accuracy: 92.42222595214844, Test Accuracy: 83.16632843017578\n",
      "Epoch 3921, Loss: 0.23989947140216827, Accuracy: 92.4000015258789, Test Accuracy: 83.16632843017578\n",
      "Epoch 3941, Loss: 0.23941239714622498, Accuracy: 92.44444274902344, Test Accuracy: 83.16632843017578\n",
      "Epoch 3961, Loss: 0.23893721401691437, Accuracy: 92.44444274902344, Test Accuracy: 83.16632843017578\n",
      "Epoch 3981, Loss: 0.2384743094444275, Accuracy: 92.44444274902344, Test Accuracy: 82.76553344726562\n",
      "Epoch 4000, Loss: 0.23804554343223572, Accuracy: 92.44444274902344, Test Accuracy: 82.76553344726562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1938c7fac10>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hybrid model L_S \n",
    "\n",
    "# P 05\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.activations import relu\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LSLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,  num_outputs_s, num_outputs_l, activation=sigmoid, wstd = 0.3, bstd = 0.5):\n",
    "        super(LSLayer, self).__init__()\n",
    "        self.num_outputs_l = num_outputs_l\n",
    "        self.num_outputs_s = num_outputs_s\n",
    "        self.num_outputs = num_outputs_l + num_outputs_s\n",
    "        self.activation = activation\n",
    "        self.wstd = wstd\n",
    "        self.bstd = bstd\n",
    "\n",
    "        \n",
    "    def build(self, input_shape):  \n",
    "        self.kernel = self.add_weight(\"kernel\",\n",
    "                                      shape=(int(input_shape[-1]),\n",
    "                                             self.num_outputs), \n",
    "                                      initializer=tf.keras.initializers.RandomNormal(stddev=self.wstd),\n",
    "                                     trainable=True)\n",
    "        \n",
    "        self.bias = self.add_weight(\"bias\",\n",
    "                                      shape=[self.num_outputs],\n",
    "                                    initializer=tf.keras.initializers.RandomNormal(stddev=self.bstd),\n",
    "                                   trainable=True)\n",
    "\n",
    "    \n",
    "    # F2 method LS layer\n",
    "    def call(self, input):\n",
    "        \n",
    "        isp = input.shape\n",
    "        In1 = tf.transpose(input)\n",
    "        kernel_S, kernel_L  = tf.split(self.kernel,[ self.num_outputs_s, self.num_outputs_l ], axis = 1 )\n",
    "        bias_S, bias_L  = tf.split(self.bias,[ self.num_outputs_s, self.num_outputs_l ], axis = 0 )\n",
    "        \n",
    "        # case spherical\n",
    "        \n",
    "        s_shape  = self.num_outputs_s\n",
    "        In2 = tf.stack([In1] * s_shape)\n",
    "        InD = tf.transpose(In2)\n",
    "        WD = tf.stack([kernel_S] * isp[0])\n",
    "        ddd = WD - InD\n",
    "        dd0 = tf.math.multiply(ddd, ddd)\n",
    "        dd1 = tf.math.reduce_sum(dd0, axis =1)\n",
    "        dd2 = tf.cast(dd1,tf.double)\n",
    "        dd3 = tf.sqrt(dd2)\n",
    "        d_r = tf.cast(dd3,tf.float32)\n",
    "        d_R = tf.abs(bias_S)\n",
    "        d_rR = tf.math.divide_no_nan(d_r,d_R)\n",
    "        d_x0 = tf.ones(d_rR.shape) - d_rR\n",
    "        result_S = tf.math.scalar_mul(6,d_x0)\n",
    "        result_S = sigmoid(result_S)\n",
    "        \n",
    "        # case linear\n",
    "        \n",
    "        d_1 = tf.stack([bias_L] * isp[0])\n",
    "        result_L = tf.matmul(input, kernel_L) + d_1 \n",
    "        result_L = relu(result_L)\n",
    "        \n",
    "        # merge\n",
    "        \n",
    "        result = tf.concat([result_S, result_L],axis=1)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "\n",
    "class NN_Model(Model):\n",
    "    \n",
    "    def __init__(self,c,l,n,m,hs,hl):\n",
    "        self.C=c\n",
    "        self.L=l\n",
    "        self.N=n\n",
    "        self.M=m\n",
    "        self.HS = hs\n",
    "        self.HL = hl\n",
    "        super(NN_Model, self).__init__()\n",
    "        self.d1 = LSLayer(self.HS,self.HL)\n",
    "        self.d2 = Dense(self.C)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        #print (\"call benn:\",x, tf.math.reduce_sum(x))\n",
    "        return self.d2(x)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(datas, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(datas, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step(datas, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    \n",
    "    predictions = model(datas, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "\n",
    "C= 6\n",
    "L= 50\n",
    "N= 5000\n",
    "M= 6\n",
    "HS = 10\n",
    "HL = 0\n",
    "EPOCHS = 4000\n",
    "\n",
    "# Create an instance of the model\n",
    "model = NN_Model(C,L,N,M,HS,HL)\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "#loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "(x_train,y_train,x_test,y_test) = gen_data_array(C, L, N, M)\n",
    "print (x_train[:2])\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).batch(32)\n",
    "#print (train_ds)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for epoch in range(EPOCHS):\n",
    "  # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    for datas, labels in train_ds:\n",
    "        train_step(datas, labels)\n",
    "        \n",
    "                \n",
    "    for test_datas, test_labels in test_ds:\n",
    "        #print (\"test_data_shape\", test_datas.shape)\n",
    "        predictions = model(test_datas, training=False)\n",
    "        #print (\"ttttttttttttttttttt\")\n",
    "        #for i in range(test_datas.shape[0]):\n",
    "        #    print (predictions.numpy()[i], test_labels.numpy()[i])\n",
    "        test_step(test_datas, test_labels)\n",
    "    \n",
    "    X.append(epoch)\n",
    "    Y.append(test_accuracy.result() * 100)\n",
    "    if epoch % 20 == 0:\n",
    "        print(\n",
    "            f'Epoch {epoch + 1}, '\n",
    "            f'Loss: {train_loss.result()}, '\n",
    "            f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "            f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "          )    \n",
    "print(\n",
    "    f'Epoch {epoch + 1}, '\n",
    "    f'Loss: {train_loss.result()}, '\n",
    "    f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "    f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "  )    \n",
    "\n",
    "plt.plot(X, Y,label=\"Accuracy curve\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "vanilla-furniture",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.66666666666667 9.092121131323903\n"
     ]
    }
   ],
   "source": [
    "import statistics \n",
    "\n",
    "x = [86,84,96,56,82]\n",
    "print (sum(x)/len(x), statistics.stdev(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "received-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "def gen_cluster_data_list(Cv, Lv, Nv, Mv):\n",
    "    Tr = []\n",
    "    Ts = []\n",
    "    C = Cv  # number of categories\n",
    "    L = Lv   # number of centers\n",
    "    N = Nv  # number of elements\n",
    "    M = Mv  # number of dimensions\n",
    "    X, y = make_blobs(n_samples=N, centers=L, n_features=M,cluster_std=.5, random_state=11)\n",
    "    cmap = []\n",
    "    for _ in range(L):\n",
    "        cmap.append(random.randint(0,C-1))\n",
    "    cols = []\n",
    "    for i in range(N):\n",
    "        cols.append(cmap[y[i]])\n",
    "\n",
    "    for i in range(int(0.9*N)):\n",
    "        row = [X[i,j] for j in range(M)]\n",
    "        row.append(cols[i])\n",
    "        Tr.append(row)\n",
    "    \n",
    "    for i in range(int(0.9*N)+1,N):\n",
    "        row = [X[i,j] for j in range(M)]\n",
    "        row.append(cols[i])\n",
    "        Ts.append(row)\n",
    "        \n",
    "    return (Tr, Ts)\n",
    "\n",
    "def normalize (train):\n",
    "    mx = []\n",
    "    mn = []\n",
    "    for i in range(len(train[0])-1):\n",
    "        mx.append(max([x[i] for x in train ]))\n",
    "        mn.append(min([x[i] for x in train ]))\n",
    "    for row in train:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i] - mn[i]) / (mx[i] - mn[i]) \n",
    "    return train\n",
    "\n",
    "\n",
    "def gen_data_array(Cv, Lv, Nv, Mv):\n",
    "    C = Cv  # number of categories\n",
    "    L = Lv   # number of centers\n",
    "    N = Nv  # number of elements\n",
    "    M = Mv  # number of dimensions\n",
    "    (T1,T2)  = gen_cluster_data_list(C, L, N, M)\n",
    "\n",
    "    T = normalize(T1)\n",
    "    N = len(T)\n",
    "    x2_train = np.zeros((N,M),dtype='float32')\n",
    "    y2_train = np.zeros((N,C))\n",
    "    for i in range(N):\n",
    "        row = T[i]\n",
    "        for j in range(M):\n",
    "            x2_train[i,j] = row[j]\n",
    "        y2_train[i,row[-1]] = 1\n",
    "\n",
    "    Ts = normalize(T2)\n",
    "    Ns = len(Ts)\n",
    "    x2_test = np.zeros((Ns,M),dtype='float32')\n",
    "    y2_test = np.zeros((Ns,C))\n",
    "    for i in range(Ns):\n",
    "        row = Ts[i]\n",
    "        for j in range(M):\n",
    "            x2_test[i,j] = row[j]\n",
    "        y2_test[i, row[-1]] = 1\n",
    "        \n",
    "    return (x2_train,y2_train, x2_test, y2_test)\n",
    "\n",
    "def gen_data_array_s(Cv, Lv, Nv, Mv):\n",
    "    C = Cv  # number of categories\n",
    "    L = Lv   # number of centers\n",
    "    N = Nv  # number of elements\n",
    "    M = Mv  # number of dimensions\n",
    "    (T1,T2)  = gen_cluster_data_list(C, L, N, M)\n",
    "\n",
    "    T = normalize(T1)\n",
    "    N = len(T)\n",
    "    x2_train = np.zeros((N,M),dtype='float32')\n",
    "    y2_train = np.zeros((N,1))\n",
    "    for i in range(N):\n",
    "        row = T[i]\n",
    "        for j in range(M):\n",
    "            x2_train[i,j] = row[j]\n",
    "        y2_train[i] = row[-1]\n",
    "\n",
    "    Ts = normalize(T2)\n",
    "    Ns = len(Ts)\n",
    "    x2_test = np.zeros((Ns,M),dtype='float32')\n",
    "    y2_test = np.zeros((Ns,1))\n",
    "    for i in range(Ns):\n",
    "        row = Ts[i]\n",
    "        for j in range(M):\n",
    "            x2_test[i,j] = row[j]\n",
    "        y2_test[i] = row[-1]\n",
    "        \n",
    "    return (x2_train,y2_train, x2_test, y2_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-prisoner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
