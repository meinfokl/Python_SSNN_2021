{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6886974573135376, Accuracy: 56.844444274902344, Test Accuracy: 66.13226318359375\n",
      "update layer\n",
      "Epoch 20, Loss: 0.06086183711886406, Accuracy: 100.0, Test Accuracy: 100.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x200075fee80>]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hybrid model L_S \n",
    "\n",
    "# P 05\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.activations import relu\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class LSLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self,  num_outputs_s, num_outputs_r, num_outputs_l, activation=sigmoid, wstd = 0.3, bstd = 0.5):\n",
    "        super(LSLayer, self).__init__()\n",
    "        self.num_outputs_l = num_outputs_l\n",
    "        self.num_outputs_s = num_outputs_s \n",
    "        self.num_outputs_r = num_outputs_r\n",
    "        self.num_outputs = num_outputs_l + num_outputs_s + num_outputs_r\n",
    "        self.activation = activation\n",
    "        self.wstd = wstd\n",
    "        self.bstd = bstd\n",
    "        \n",
    "    def build(self, input_shape):  \n",
    "        self.num_inputs = input_shape[-1]\n",
    "        self.kernel = self.add_weight(\"kernel\",\n",
    "                                      shape=(int(input_shape[-1]),\n",
    "                                             self.num_outputs), \n",
    "                                      initializer=tf.keras.initializers.RandomNormal(stddev=self.wstd),\n",
    "                                     trainable=True)\n",
    "\n",
    "        self.bias = self.add_weight(\"bias\",\n",
    "                                      shape=[self.num_outputs],\n",
    "                                    initializer=tf.keras.initializers.RandomNormal(stddev=self.bstd),\n",
    "                                   trainable=True)\n",
    "        \n",
    "    \n",
    "    # F2 method LS layer\n",
    "    def call(self, input):\n",
    "        \n",
    "        isp = input.shape\n",
    "        In1 = tf.transpose(input)\n",
    "        kernel_S, kernel_L  = tf.split(self.kernel,[ self.num_outputs_s + self.num_outputs_r, self.num_outputs_l ], axis = 1 )\n",
    "        bias_S, bias_L  = tf.split(self.bias,[ self.num_outputs_s +  self.num_outputs_r, self.num_outputs_l ], axis = 0 )\n",
    "        \n",
    "        # case spherical\n",
    "        \n",
    "        s_shape  = self.num_outputs_s + self.num_outputs_r\n",
    "        In2 = tf.stack([In1] * s_shape)\n",
    "        InD = tf.transpose(In2)\n",
    "        WD = tf.stack([kernel_S] * isp[0])\n",
    "        ddd = WD - InD\n",
    "        dd0 = tf.math.multiply(ddd, ddd)\n",
    "        dd1 = tf.math.reduce_sum(dd0, axis =1)\n",
    "        dd2 = tf.cast(dd1,tf.double)\n",
    "        dd3 = tf.sqrt(dd2)\n",
    "        d_r = tf.cast(dd3,tf.float32)\n",
    "        d_R = tf.abs(bias_S)\n",
    "        d_rR = tf.math.divide_no_nan(d_r,d_R)\n",
    "        d_x0 = tf.ones(d_rR.shape) - d_rR\n",
    "        result_S = tf.math.scalar_mul(6,d_x0)\n",
    "        result_S = sigmoid(result_S)\n",
    "        \n",
    "        # case linear\n",
    "\n",
    "        d_1 = tf.stack([bias_L] * isp[0])\n",
    "        result_L = tf.matmul(input, kernel_L) + d_1 \n",
    "        result_L = relu(result_L)\n",
    "\n",
    "        #case empty, merge\n",
    "        \n",
    "        '''\n",
    "        #print (self.num_outputs_r)\n",
    "        if self.num_outputs_r > 0:\n",
    "            r_S, _ = tf.split (result_S,[self.num_outputs_s, self.num_outputs_r],axis=1 )\n",
    "            r_1 = np.zeros((result_S.shape[0],self.num_outputs_r))\n",
    "            result_R = tf.cast(tf.constant(r_1),tf.float32)\n",
    "            result = tf.concat([r_S, result_R, result_L],axis=1)            \n",
    "            #print (self.num_outputs_s, self.num_outputs_r)\n",
    "            #print (\"result_S\", result_S)\n",
    "            #print (\"result_L\", result_L)\n",
    "            #print (\"result\", result)\n",
    "        else:\n",
    "            result = tf.concat([result_S, result_L],axis=1)        \n",
    "        '''\n",
    "        \n",
    "        result = tf.concat([result_S, result_L],axis=1)        \n",
    "        \n",
    "        return result\n",
    "    \n",
    "\n",
    "class NN_Model(Model):\n",
    "    \n",
    "    def __init__(self,c,hs,hr,hl):\n",
    "        super(NN_Model, self).__init__()\n",
    "        self.d1 = LSLayer(hs,hr,hl)\n",
    "        self.d2 = Dense(c)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        #print (\"call benn:\",x, tf.math.reduce_sum(x))\n",
    "        return self.d2(x)\n",
    "        \n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(datas, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(datas, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step(datas, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    \n",
    "    predictions = model(datas, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "    \n",
    "    \n",
    "def update_NN_model (model, train_ds,optimizer):\n",
    "\n",
    "    rdb = 0\n",
    "    odb = 0\n",
    "    #N = min(5, model.d1.num_outputs_r)   # number of new SSN nodes\n",
    "    N = model.d1.num_outputs_r   # number of new SSN nodes\n",
    "    \n",
    "    if N <= 0:\n",
    "        return\n",
    "    \n",
    "    # k-means\n",
    "    \n",
    "    baditems = []\n",
    "    for datas, labels in train_ds:\n",
    "        predictions = model(datas, training=False)\n",
    "        for i in range(datas.shape[0]):\n",
    "            #print (datas.numpy()[i], predictions.numpy()[i], np.argmax(predictions.numpy()[i]), labels.numpy()[i],np.argmax(labels.numpy()[i]))\n",
    "            if np.argmax(predictions.numpy()[i]) != np.argmax(labels.numpy()[i]):\n",
    "                rdb = rdb + 1\n",
    "                baditems.append(datas.numpy()[i])\n",
    "            odb = odb + 1        \n",
    "    #print (\"pontossag:\",(odb-rdb)/odb, len(baditems))\n",
    "    N = min(N, len(baditems))\n",
    "    if N == 0:\n",
    "        return\n",
    "    inds = random.sample(range(len(baditems)), N)\n",
    "    \n",
    "    print (\"update layer\")\n",
    "    #print (baditems)\n",
    "    centers = KMeans(n_clusters=N).fit(baditems).cluster_centers_\n",
    "    #print (\"centers:\")\n",
    "    #print (centers)\n",
    "    neww = np.zeros((model.d1.num_inputs,N))\n",
    "    for i in range(N):\n",
    "        for j in range(model.d1.num_inputs):\n",
    "            neww[j,i] = centers[i][j]\n",
    "    #print (\"neww\")\n",
    "    #print (neww)\n",
    "            \n",
    "    newb = np.zeros((N))\n",
    "    for i in range(N):\n",
    "        newb[i] = random.random()*model.d1.bstd\n",
    " \n",
    "    xu = model.d1.get_weights()\n",
    "    \n",
    "    for j in range(N):\n",
    "        for i in range(xu[0].shape[0]):\n",
    "            xu[0][i][model.d1.num_outputs_s + j] = neww[i,j]\n",
    "        xu[1][j] = newb[j]\n",
    "            \n",
    "    model.d1.set_weights(xu )\n",
    "    \n",
    "    model.d1.num_outputs_s = model.d1.num_outputs_s + N\n",
    "    model.d1.num_outputs_r = model.d1.num_outputs_r - N\n",
    "\n",
    "    #optimizer = tf.keras.optimizers.Adam\n",
    "    #for var in optimizer.variables():\n",
    "    #    var.assign(tf.zeros_like(var))\n",
    "    \n",
    "C= 6\n",
    "L= 25\n",
    "N= 5000\n",
    "M= 6\n",
    "HS = 20\n",
    "HR = 10\n",
    "HL = 0\n",
    "EPOCHS = 20\n",
    "Eupd = 6\n",
    "B = 32\n",
    "\n",
    "# Create an instance of the model\n",
    "model = NN_Model(C,HS,HR,HL)\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "#loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "(x_train,y_train,x_test,y_test) = gen_data_array(C, L, N, M)\n",
    "#print (x_train[:2])\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).batch(B)\n",
    "#print (train_ds)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(B)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for epoch in range(EPOCHS):\n",
    "  # Reset the metrics at the start of the next epoch\n",
    "\n",
    "    if epoch > 1:\n",
    "        xu2 = model.d1.get_weights()\n",
    "        #print (\"WW\",epoch, \"WW\", xu2[0])\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    for datas, labels in train_ds:\n",
    "        train_step(datas, labels)\n",
    "        \n",
    "                \n",
    "    for test_datas, test_labels in test_ds:\n",
    "        #print (\"test_data_shape\", test_datas.shape)\n",
    "        predictions = model(test_datas, training=False)\n",
    "        #print (\"ttttttttttttttttttt\")\n",
    "        #for i in range(test_datas.shape[0]):\n",
    "        #    print (predictions.numpy()[i], test_labels.numpy()[i])\n",
    "        test_step(test_datas, test_labels)\n",
    "        \n",
    "    if epoch == Eupd :\n",
    "        update_NN_model (model, train_ds, optimizer)\n",
    "        \n",
    "        \n",
    "    X.append(epoch)\n",
    "    Y.append(test_accuracy.result() * 100)\n",
    "    if epoch % 20 == 0:\n",
    "        print(\n",
    "            f'Epoch {epoch + 1}, '\n",
    "            f'Loss: {train_loss.result()}, '\n",
    "            f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "            f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "          )    \n",
    "        #print(model.d1.bias.numpy())\n",
    "\n",
    "\n",
    "print(\n",
    "    f'Epoch {epoch + 1}, '\n",
    "    f'Loss: {train_loss.result()}, '\n",
    "    f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "    f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "  )    \n",
    "\n",
    "    \n",
    "plt.plot(X, Y,label=\"Accuracy curve\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.5 2.6352313834736494\n"
     ]
    }
   ],
   "source": [
    "import statistics \n",
    "\n",
    "x = [100,100,100,100,100,100,100,100,100,100]\n",
    "print (sum(x)/len(x), statistics.stdev(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "def gen_cluster_data_list(Cv, Lv, Nv, Mv):\n",
    "    Tr = []\n",
    "    Ts = []\n",
    "    C = Cv  # number of categories\n",
    "    L = Lv   # number of centers\n",
    "    N = Nv  # number of elements\n",
    "    M = Mv  # number of dimensions\n",
    "    X, y = make_blobs(n_samples=N, centers=L, n_features=M,cluster_std=.5, random_state=11)\n",
    "    cmap = []\n",
    "    for _ in range(L):\n",
    "        cmap.append(random.randint(0,C-1))\n",
    "    cols = []\n",
    "    for i in range(N):\n",
    "        cols.append(cmap[y[i]])\n",
    "\n",
    "    for i in range(int(0.9*N)):\n",
    "        row = [X[i,j] for j in range(M)]\n",
    "        row.append(cols[i])\n",
    "        Tr.append(row)\n",
    "    \n",
    "    for i in range(int(0.9*N)+1,N):\n",
    "        row = [X[i,j] for j in range(M)]\n",
    "        row.append(cols[i])\n",
    "        Ts.append(row)\n",
    "        \n",
    "    return (Tr, Ts)\n",
    "\n",
    "def normalize (train):\n",
    "    mx = []\n",
    "    mn = []\n",
    "    for i in range(len(train[0])-1):\n",
    "        mx.append(max([x[i] for x in train ]))\n",
    "        mn.append(min([x[i] for x in train ]))\n",
    "    for row in train:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i] - mn[i]) / (mx[i] - mn[i]) \n",
    "    return train\n",
    "\n",
    "\n",
    "def gen_data_array(Cv, Lv, Nv, Mv):\n",
    "    C = Cv  # number of categories\n",
    "    L = Lv   # number of centers\n",
    "    N = Nv  # number of elements\n",
    "    M = Mv  # number of dimensions\n",
    "    (T1,T2)  = gen_cluster_data_list(C, L, N, M)\n",
    "\n",
    "    T = normalize(T1)\n",
    "    N = len(T)\n",
    "    x2_train = np.zeros((N,M),dtype='float32')\n",
    "    y2_train = np.zeros((N,C))\n",
    "    for i in range(N):\n",
    "        row = T[i]\n",
    "        for j in range(M):\n",
    "            x2_train[i,j] = row[j]\n",
    "        y2_train[i,row[-1]] = 1\n",
    "\n",
    "    Ts = normalize(T2)\n",
    "    Ns = len(Ts)\n",
    "    x2_test = np.zeros((Ns,M),dtype='float32')\n",
    "    y2_test = np.zeros((Ns,C))\n",
    "    for i in range(Ns):\n",
    "        row = Ts[i]\n",
    "        for j in range(M):\n",
    "            x2_test[i,j] = row[j]\n",
    "        y2_test[i, row[-1]] = 1\n",
    "        \n",
    "    return (x2_train,y2_train, x2_test, y2_test)\n",
    "\n",
    "def gen_data_array_s(Cv, Lv, Nv, Mv):\n",
    "    C = Cv  # number of categories\n",
    "    L = Lv   # number of centers\n",
    "    N = Nv  # number of elements\n",
    "    M = Mv  # number of dimensions\n",
    "    (T1,T2)  = gen_cluster_data_list(C, L, N, M)\n",
    "\n",
    "    T = normalize(T1)\n",
    "    N = len(T)\n",
    "    x2_train = np.zeros((N,M),dtype='float32')\n",
    "    y2_train = np.zeros((N,1))\n",
    "    for i in range(N):\n",
    "        row = T[i]\n",
    "        for j in range(M):\n",
    "            x2_train[i,j] = row[j]\n",
    "        y2_train[i] = row[-1]\n",
    "\n",
    "    Ts = normalize(T2)\n",
    "    Ns = len(Ts)\n",
    "    x2_test = np.zeros((Ns,M),dtype='float32')\n",
    "    y2_test = np.zeros((Ns,1))\n",
    "    for i in range(Ns):\n",
    "        row = Ts[i]\n",
    "        for j in range(M):\n",
    "            x2_test[i,j] = row[j]\n",
    "        y2_test[i] = row[-1]\n",
    "        \n",
    "    return (x2_train,y2_train, x2_test, y2_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "print (3**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
